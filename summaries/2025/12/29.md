# Hacker News Summary - 2025-12-29

## [What an unprocessed photo looks like](https://maurycyz.com/misc/raw_photo/)
**Score:** 1413 | **Comments:** 238 | **ID:** 46415225

> **Article:** The article "What an unprocessed photo looks like" demystifies the digital photography pipeline by showing the stages of image creation from raw sensor data to a final image. It explains that a camera sensor does not capture color directly; it only records light intensity. To create a color image, a Bayer filter (a grid of red, green, and blue filters) is placed over the sensor, meaning each pixel only records one color. This raw, single-color data is called a "demosaiced" image and appears as a noisy, dark, and green-tinted mosaic. The article then illustrates the subsequent processing steps required to make this data viewable: correcting for the sensor's linear light sensitivity using a gamma curve, adjusting white balance, applying noise reduction, and performing sharpening. The final result is a processed JPEG, which the article argues is just one of many possible interpretations of the raw data, not a "ground truth" representation of the scene.
>
> **Discussion:** The Hacker News discussion largely praises the article for its clear explanation of the complex, hidden processes behind digital photography. A central theme is the philosophical debate over what constitutes a "real" or "fake" photo. Many commenters argue that since every photo, even straight from the camera, involves significant processing and interpretation (like demosaicing and gamma correction), the concept of an "unprocessed" image is a myth. They contend that the distinction between "real" and "fake" should be based on the intent to deceive, not on the use of editing tools. The discussion also delves into technical details, with users explaining the psycho-visual reasons for the Bayer filter's green-heavy pattern (matching human eye sensitivity) and the necessity of gamma correction for both historical monitor limitations and efficient data allocation. There is some debate about the role of AI in photography, with one side seeing it as a logical extension of existing processing and another viewing it as "hallucination" that crosses a line. Overall, the community used the article as a springboard to educate and clarify common misconceptions about image authenticity and the nature of digital photography.

---

## [Last Year on My Mac: Look Back in Disbelief](https://eclecticlight.co/2025/12/28/last-year-on-my-mac-look-back-in-disbelief/)
**Score:** 462 | **Comments:** 365 | **ID:** 46409969

> **Article:** The article "Last Year on My Mac: Look Back in Disbelief" is a retrospective critique of Apple's recent software design philosophy, specifically targeting the "Tahoe" interface (likely a codename for a macOS version or a design language like Liquid Glass). The author expresses disbelief at the decline in UI quality, citing issues like excessive whitespace, low contrast, poor readability, and oversized controls. The piece argues that Apple's focus on aesthetic changes for the sake of novelty—possibly to align with the Vision Pro—has come at the expense of functionality and the power user experience, leading to a frustrating and inefficient computing environment.
>
> **Discussion:** The Hacker News discussion overwhelmingly validates the article's sentiment, with users expressing deep frustration and disappointment with Apple's current UI direction, particularly the "Tahoe" design language. The consensus is that the new interface is a significant regression in usability and aesthetics.

Key criticisms focus on the visual and functional regressions. Users describe the design as "sloppy," "wasted space," and "Fisher-Price" like, complaining about excessive rounded corners, low contrast, and poor readability. The nested rounded rectangles for menus were singled out as a particularly bad design choice. This has led to a tangible decrease in productivity, with one user noting that forced upgrades caused weeks of lost work. The issues are not limited to macOS; several commenters report that iOS 26 is similarly frustrating and ugly.

The discussion speculates on the root causes. Some believe Apple's UI team is under pressure to constantly invent radical changes to justify their existence. Others suggest the design is being forced to be "VisionOS-friendly" after the poor reception of the Vision Pro, a move that prioritizes a failed product's aesthetic over user needs on traditional computers. A cynical take is that the changes are a form of planned obsolescence, designed to tax hardware and force users onto a faster upgrade cycle.

The primary sentiment is one of resignation and contemplation of alternatives. Long-time Mac users state they are holding off on upgrades or seriously considering a switch to Linux, viewing commercial operating systems as having "enshittified" beyond repair. While a few commenters find some beauty in the glass effects, they agree that it constantly gets in the way of actual work, prioritizing "visual fireworks" over the "crispness" and speed required for a productive workflow.

---

## [Building a macOS app to know when my Mac is thermal throttling](https://stanislas.blog/2025/12/macos-thermal-throttling-app/)
**Score:** 267 | **Comments:** 111 | **ID:** 46410402

> **Article:** The article details the process of creating a custom macOS menu bar application to detect and display when a Mac is thermal throttling. The author, Stanislas, explains that while tools like iStat Menus show CPU usage and temperatures, they don't explicitly indicate when the system is actively throttling performance to manage heat. He details his technical journey of discovering the correct system APIs and kernel events to monitor, ultimately building a lightweight utility that provides this specific, actionable feedback. The project serves as both a practical tool and a case study in macOS system monitoring.
>
> **Discussion:** The Hacker News discussion centered on the practical utility of such an app, with users debating what actions can be taken once throttling is detected. The conversation highlighted several key themes:

*   **Mitigation Strategies:** Users confirmed that on modern Apple Silicon Macs, the primary solution is enabling the system's built-in "High Power Mode," which raises fan speeds. For older Intel Macs with fans, users mentioned third-party tools like iStat Menus allow for custom fan curves to cool the system more aggressively. For fanless MacBook Airs, the only recourse is environmental (e.g., pointing a fan at it) or reducing system load.

*   **Alternative Tools:** Many commenters suggested existing, popular utilities like `stats` (an open-source alternative to iStat Menus) that can display CPU wattage and temperature, which serve as good indicators of thermal issues.

*   **Technical Nuances:** A developer chimed in to discuss a known bug with Apple's `ProcessInfo.processInfo.thermalState` API, which can fail to update correctly, validating the author's need to find a more robust detection method.

*   **Hardware & Environmental Factors:** A significant tangent discussed the importance of cleaning dust and pet hair from Mac fans, with several users sharing anecdotes about how much debris can accumulate and impact cooling performance.

*   **Broader Apple Sentiment:** The discussion branched into broader frustrations with older Intel-based Macs (especially the 2017 models with butterfly keyboards) but concluded with praise for the performance and quality of the current Apple Silicon generation, which largely resolves the thermal and reliability issues of the recent past.

---

## [Learn computer graphics from scratch and for free](https://www.scratchapixel.com)
**Score:** 267 | **Comments:** 27 | **ID:** 46410210

> **Article:** The article links to Scratchapixel.com, a free educational website dedicated to teaching computer graphics from first principles. The site covers fundamental topics like software rasterization and ray tracing, aiming to make knowledge accessible without the need for expensive books or navigating complex, proprietary APIs. It is presented as a valuable resource for anyone wanting to understand how graphics work "under the hood."
>
> **Discussion:** The HN community response is overwhelmingly positive, with users praising the resource for filling a significant gap in accessible, high-quality computer graphics education. Many commenters share a personal connection to the topic, expressing a long-held desire to learn graphics programming, often inspired by classic games like Doom and Quake, but feeling intimidated by the complexity of modern APIs like Vulkan and DirectX 12.

A central theme is the difficulty of starting out. Users describe "drinking from a firehose" when jumping into modern APIs without understanding the fundamentals. The consensus is that the best way to learn is to start from scratch by writing software rasterizers and ray tracers, ignoring the GPU initially to build a solid foundation. Several other resources are recommended in the comments, including WebGPU for a gentler introduction to modern concepts and a "build-your-own-x" GitHub repository for practical guides.

While the resource is celebrated, there is minor criticism regarding its presentation. One user complains about an "obnoxious slop image" on the homepage, and another dislikes that the community contact method is Discord, which they find off-putting. Overall, the discussion is a mix of appreciation for the site, shared nostalgia for the "golden age" of 3D gaming, and encouragement for aspiring graphics programmers.

---

## [Stepping down as Mockito maintainer after ten years](https://github.com/mockito/mockito/issues/3777)
**Score:** 252 | **Comments:** 165 | **ID:** 46414078

> **Article:** The article is a GitHub issue where Szczepan Faber announces he is stepping down as the maintainer of Mockito, the most popular mocking framework for Java, after ten years of contributions. He cites burnout and an "energy drain" as the primary reasons. A key catalyst for this decision was the significant effort required to adapt Mockito to upcoming changes in the Java Virtual Machine (JVM), specifically a breaking change (JEP 451) that disallows the dynamic loading of agents by default. This forced a major architectural shift in Mockito 5, where the main artifact became a pre-loaded agent, creating substantial maintenance work. Faber also mentions the complexities introduced by supporting Kotlin, which, while beneficial for users, added to the maintenance burden.
>
> **Discussion:** The Hacker News discussion revolves around three main themes: the challenges of open-source maintenance, the technical debate over JVM integrity versus developer convenience, and the philosophical divide on testing practices.

A significant portion of the conversation focuses on the human element of open source. Many commenters express sympathy for the maintainer, highlighting the thankless nature of the work and the prevalence of burnout, often referencing the famous XKCD comic about a single maintainer being critical infrastructure. There is a debate over whether financial compensation would have solved the issue, with some arguing it provides necessary motivation, while others contend that intrinsic motivation is key for OSS and that money can introduce its own pressures.

Technically, the discussion delves into the JVM changes that precipitated the crisis. The official OpenJDK perspective, articulated by a JDK maintainer, defends the move towards "Integrity by Default" as crucial for security, performance, and long-term platform stability. They argue that requiring explicit flags for powerful features like dynamic agent loading is a necessary trade-off to protect the entire ecosystem. However, some developers push back, viewing this as a top-down decision that creates friction and may lead to enterprises avoiding new JVM versions or re-enabling the old, "insecure" behavior to avoid rewriting test suites. The difficulty of supporting Kotlin was also noted, with some suggesting Kotlin users should switch to a framework designed for it, like MockK.

Finally, the comments reveal a deep-seated debate on testing philosophy. Several developers argue that the need for complex mocking frameworks like Mockito is a "code smell" indicating poor application design and that reliance on mocks leads to brittle, hard-to-understand tests. The counter-argument is that developers often work with legacy codebases where mocking is the only practical way to achieve test coverage, making the tool indispensable for modernizing old systems.

---

## [You can make up HTML tags](https://maurycyz.com/misc/make-up-tags/)
**Score:** 229 | **Comments:** 95 | **ID:** 46416945

> **Article:** The article explains that you can create and use your own custom HTML tags (e.g., `<my-widget>`) without any special setup. The browser will render them as inline elements by default, similar to a `<span>`, and they can be styled with CSS and manipulated with JavaScript just like standard elements. This is a built-in feature of HTML, distinct from the more powerful Custom Elements API which can add specific behaviors to these tags.
>
> **Discussion:** The discussion largely validates the article's premise, confirming that browsers handle unknown tags gracefully. However, the conversation quickly pivots from the basic concept to the more robust and standard-compliant use of Web Components and the Custom Elements API.

Key points raised by the community include:
*   **Technical Nuances:** Commenters clarified that modern browsers treat custom tags containing a hyphen (e.g., `<my-tag>`) as `HTMLElement` rather than `HTMLUnknownElement`, a design choice to ensure future compatibility if the tag is ever standardized.
*   **Web Components as the "Right Way":** Many commenters, while acknowledging the "fun" of plain custom tags, strongly advocated for using the full Custom Elements API. Frameworks like Lit were mentioned as excellent, simpler alternatives to React for building component-based UIs using this native browser technology.
*   **Philosophical Debate:** A recurring theme was a preference for server-rendered HTML enhanced with custom elements over complex Single-Page Applications (SPAs). Several developers expressed frustration with the dominance of React for tasks that could be handled more simply and accessibly with modern HTML and Web Components.
*   **Critique of Examples:** Some users pointed out that the article's examples could be better implemented using standard semantic HTML tags (`<article>`, `<header>`, `<blockquote>`) instead of creating custom ones, highlighting the importance of using the right tool for the job.

---

## [CEOs are hugely expensive. Why not automate them? (2021)](https://www.newstatesman.com/business/companies/2023/05/ceos-salaries-expensive-automate-robots)
**Score:** 209 | **Comments:** 248 | **ID:** 46415488

> **Article:** The article, "CEOs are hugely expensive. Why not automate them?", argues that the exorbitant salaries of top executives are ripe for disruption by AI. It posits that a significant portion of a CEO's role involves processing vast amounts of data to make strategic decisions, a task at which AI could potentially outperform humans. The author suggests that an AI CEO could be more objective, data-driven, and cost-effective, avoiding human biases and emotional errors. The piece frames the idea as a logical extension of automation into the highest echelons of business, questioning why this "bullshit job" has been spared from technological replacement.
>
> **Discussion:** The Hacker News discussion largely dismisses the idea of a fully automated CEO, focusing on the uniquely human and social aspects of the role that are difficult to automate. The consensus is that a CEO's primary function is not data analysis but human-centric tasks like networking, building relationships, selling a vision, and providing leadership. One commenter noted the job is "almost entirely human to human tasks" and "social finesse."

Beyond the practicalities of the role, several key themes emerged:

*   **Legal and Fiduciary Accountability:** A major counterargument is that CEOs have legal obligations and fiduciary duties that cannot be delegated to a machine. Under corporate law (specifically Delaware law, cited by one user), these duties are "non-delegable" and must be performed by a "natural person," creating a significant legal barrier.
*   **The "Surplus" Argument:** One thread debated the economic purpose of a CEO. One user argued that an AI provider would simply extract the CEO's salary as profit. Others countered that competition would drive down costs and that the goal is efficiency, not job creation for executives.
*   **Potential for AI "Ruthlessness":** A few users speculated on the negative consequences, suggesting an AI CEO could be even more ruthless in its treatment of the workforce than a human, and questioning who would be legally responsible for its actions.
*   **Humorous Comparisons:** The discussion included lighthearted comparisons, with one user suggesting that automating CEOs is less feasible than automating sex work due to the complexity of social finesse.

---

## [Unity's Mono problem: Why your C# code runs slower than it should](https://marekfiser.com/blog/mono-vs-dot-net-in-unity/)
**Score:** 195 | **Comments:** 92 | **ID:** 46414819

> **Article:** The article "Unity's Mono problem" argues that Unity's C# code runs slower than it should because the engine is stuck on an old version of the Mono runtime, while modern .NET (CoreCLR) has seen significant performance improvements. The author presents benchmarks showing that a simple C# task (reading a file, deserializing, and processing data) runs over 20 times faster on .NET 8 compared to Unity's Mono. The piece briefly outlines the history of Unity's reliance on Mono and its ongoing, long-delayed effort to migrate to CoreCLR, which is now projected for a 2026 release with Unity 6.7.
>
> **Discussion:** The HN discussion largely validates the article's premise but adds important context and critiques its methodology. A key point of contention is the article's use of debug builds for benchmarking, though some commenters argue that performance gains in debug mode often translate to release builds.

The conversation highlights several other major performance bottlenecks in Unity beyond the Mono runtime itself:
*   **Garbage Collection:** Multiple users point out that Unity uses the outdated Boehm GC, which is much slower than the modern generational GC (SGen) available in standard Mono for years. This is identified as a critical performance issue.
*   **IL2CPP:** Some note that in production, games are often compiled with IL2CPP, which bypasses Mono entirely. The real-world impact of the CoreCLR migration may therefore be less dramatic for release builds, though it promises a much faster editor experience by eliminating slow domain reloads.

There is significant skepticism about Unity's ability to execute the migration, with some commenters suggesting the company lacks the technical skill or has too much tech debt. The historical context is also a major theme, with users recalling that Unity long blamed Mono's licensing for its inability to upgrade, even after Mono became MIT-licensed, suggesting internal inertia was the real problem. Alternatives like Stride and Godot are mentioned, with users weighing their pros and cons against Unity's vast ecosystem.

---

## [Software engineers should be a little bit cynical](https://www.seangoedecke.com/a-little-bit-cynical/)
**Score:** 191 | **Comments:** 132 | **ID:** 46414723

> **Article:** The article "Software engineers should be a little bit cynical" by Sean Goedecke argues that software engineers should adopt a pragmatic, "clear-eyed cynicism" to navigate corporate structures effectively. Goedecke contrasts this with two extremes: naive idealism (believing your company is changing the world) and "doomer" cynicism (believing everything is evil and pointless).

He advises engineers to accept that they are not the primary drivers of company direction—that falls to executives and market forces. However, this doesn't make them powerless. By understanding the true motivations of the business (profit, growth), engineers can better advocate for technical health, negotiate for better working conditions, and influence decisions within their sphere of control. The core message is to be realistic about corporate motives without becoming apathetic, using that understanding to engineer a better career and product.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise but pushes back on specific nuances, particularly regarding the morality of corporate work and the true nature of executive leadership.

Many commenters agreed that a baseline of cynicism is a necessary defense mechanism against corporate disappointment. Several users shared personal anecdotes where idealism was punished and cynicism provided emotional protection. However, there was a significant debate on the balance between cynicism and optimism. One commenter argued that while cynicism feels smart, "optimists win" because achieving unlikely outcomes requires the naive energy to try in the first place.

The most heated arguments centered on the nature of corporate leadership. The author's defense of C-suite executives—that they generally want to build good products—was met with strong skepticism. Several users argued that the primary, and often sole, motivation is shareholder value, and "good software" is only a means to that end. Anecdotes were shared about executives being described as "super-powerful young children" and that ethical considerations are often explicitly disregarded in favor of profit.

Finally, the discussion touched on the ethics of working in "late-stage capitalism." While the article focused on navigating employment, some commenters argued that the only winning move is not to play (i.e., leave employment for consulting) or that working for major tech companies is inherently selling out, rendering any internal idealism a hollow illusion.

---

## [MongoBleed Explained Simply](https://bigdata.2minutestreaming.com/p/mongobleed-explained-simply)
**Score:** 185 | **Comments:** 71 | **ID:** 46414475

> **Article:** The article "MongoBleed Explained Simply" describes a critical security vulnerability (CVE-2024-56879) in MongoDB. The flaw is a classic "uninitialized memory read" bug. When a MongoDB server responds to a specific type of query, it could inadvertently leak up to 100KB of uninitialized memory from the heap. This memory could contain sensitive data from previous operations, such as passwords, session tokens, or other user information. The vulnerability stems from the C++ standard library's behavior where `malloc` does not zero out memory, and the application failing to initialize the memory before sending it over the network. The article also notes that the public disclosure timeline was confusing because MongoDB uses an internal repository and publishes commits later.
>
> **Discussion:** The Hacker News discussion centered on four main themes: the root cause of the bug, the security implications, the timeline of the disclosure, and general criticism of MongoDB.

A significant portion of the conversation focused on the technical cause: the use of memory-unsafe languages like C++. A notable suggestion came from a Cloudflare engineer who described their practice of overwriting freed memory with a static pattern to mitigate such bugs, sparking a debate on the performance overhead and compiler optimizations that might nullify such efforts.

The security posture of MongoDB deployments was heavily scrutinized. Users pointed to a Shodan scan showing over 200,000 exposed instances, with one commenter theorizing that the same developer mindset that avoids schema design (a reason for choosing Mongo) also leads to poor security practices like exposing databases directly to the internet.

Several comments corrected the article's timeline, clarifying that MongoDB Atlas customers were patched days before the public CVE announcement, and explaining that the company uses an internal repository which causes delays in public-facing commit dates.

Finally, there was a recurring, cynical sentiment questioning why MongoDB is used at all, with one user humorously citing its famous "web scale" capabilities. Other practical questions were raised about the status of Atlas auto-patching and the lack of evidence for active exploitation.

---

## [PySDR: A Guide to SDR and DSP Using Python](https://pysdr.org/content/intro.html)
**Score:** 179 | **Comments:** 8 | **ID:** 46413975

> **Article:** PySDR is an online guide titled "A Guide to SDR and DSP Using Python" that bridges the gap between complex theory and practical implementation for Software Defined Radio (SDR) and Digital Signal Processing (DSP). It aims to teach these concepts using Python code rather than heavy mathematical formulas, making it accessible to programmers. The guide covers essential topics such as filters, Fourier transforms, modulation, and synchronization, providing a hands-on engineering approach to radio technology.
>
> **Discussion:** The response to PySDR is overwhelmingly positive, with users across the spectrum of expertise praising it as an excellent resource. Beginners appreciate the practical, engineering-oriented guide and the use of affordable hardware like the RTL-SDR, which is noted as a capable device for learning and everyday receiving tasks. Even DSP experts find the explanations delightful and useful for onboarding new team members who are stronger in coding than in theory.

However, one critical point raised is the guide's "hand-waviness" regarding specific theoretical details. A user noted that while the author admits to skipping over complex theory (such as choosing specific loop parameters for synchronization), the guide fails to point the reader in the right direction to learn those missing details, leaving a gap for those who need to understand the underlying "why."

---

## [Hungry Fat Cells Could Someday Starve Cancer](https://www.ucsf.edu/news/2025/01/429411/how-hungry-fat-cells-could-someday-starve-cancer-death)
**Score:** 159 | **Comments:** 40 | **ID:** 46409928

> **Article:** Researchers at UCSF have developed a novel cancer therapy that engineers a patient's own fat cells to act as "Trojan horses," starving cancer cells of nutrients. The therapy involves harvesting a patient's white fat cells, genetically modifying them using CRISPR to express a protein called UCP1 (which makes them burn energy rapidly, like brown fat), and re-injecting them near the tumor. These "hungry" engineered cells then act as a metabolic sink, consuming glucose and lipids that the tumor needs to grow, effectively starving it. The study, led by Ph.D. candidate Phuc Nguyen who tragically passed away before its completion, demonstrated significant tumor shrinkage in mice. The research was published in the journal *Nature Biotechnology*.
>
> **Discussion:** The Hacker News discussion was largely positive but cautious, reflecting on the tragic loss of the lead researcher and the complexities of metabolic cancer theories. Key themes included:

*   **Metabolic Theory and Skepticism:** Several users noted that while the "metabolic theory of cancer" (the idea that cancer is a metabolic disease) is compelling, it has historically struggled to translate into effective clinical treatments. They cautioned that while the mouse results are promising, many therapies that work in mice fail in humans.
*   **Cold Exposure vs. Genetic Engineering:** A major point of confusion was the distinction between the actual therapy (genetic engineering of fat cells) and the theoretical link to cold exposure (which naturally converts white fat to energy-burning beige fat). Users debated the feasibility of cold therapy for cancer patients, with some suggesting it could be a preventative measure, while others corrected that the study used CRISPR, not cold, to achieve the effect.
*   **Clinical Feasibility:** Commenters raised practical concerns about the therapy, such as the difficulty of maintaining cold exposure for sick patients and the potential for increased hunger to negate the benefits. There was also skepticism about the availability of fat cells for extraction, given the widespread use of weight-loss drugs like Ozempic.
*   **Tribute to the Researcher:** Many comments expressed sadness over the premature death of Phuc Nguyen (35), acknowledging the potential of his work and hoping others would continue the research.

---

## [As AI gobbles up chips, prices for devices may rise](https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram)
**Score:** 157 | **Comments:** 198 | **ID:** 46415338

> **Article:** The NPR article, "As AI gobbles up chips, prices for devices may rise," argues that the massive demand for high-performance chips to power AI data centers is straining the global supply of semiconductors and memory. This competition for limited manufacturing capacity is driving up prices for the components used in consumer electronics like smartphones, laptops, and gaming consoles. The article notes that while major companies like Apple can secure long-term supply contracts to shield them from short-term price hikes, smaller manufacturers and consumers are more vulnerable. The piece suggests that this trend could lead to more expensive devices and potentially slower hardware innovation for consumers as the industry prioritizes the high-margin AI sector.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, with users noting that RAM prices are already "through the roof." The conversation explores several key themes. First, there's a debate on the economic mechanics, with users explaining that the issue is a massive demand shift rather than a simple monopoly, and that government intervention should focus on boosting supply (e.g., subsidizing domestic manufacturing) rather than price controls. Second, commenters discuss the downstream effects on software and hardware. A optimistic view suggests this will force developers to write more efficient code, while a pessimistic one predicts a shift to centralized cloud services, further eroding personal computing.

Third, the discussion highlights the disparity between large corporations and individual consumers. Big companies like Apple are insulated by long-term contracts, but the supply redirection towards high-margin server components (like HBM) will eventually impact everyone. Finally, there's a strong undercurrent of user frustration with the industry's direction. Commenters lament the "regression" of budget devices (e.g., shipping with only 8GB of RAM) and the "enshittification" of software, viewing the AI-driven demand as a "racket" that prioritizes corporate profits over the needs of students, scientists, and general-purpose computing. Some users advocate for resisting this trend by using older hardware and disabling resource-heavy web features.

---

## [No, it's not a battleship](https://www.navalgazing.net/No-its-not)
**Score:** 140 | **Comments:** 183 | **ID:** 46413790

> **Article:** The article "No, it's not a battleship" from NavalGazing.net is a technical and historical critique of a proposal by the Trump administration to build a new class of large surface warships. The author argues that despite being labeled a "battleship" by political figures, the proposed vessel's design—featuring advanced missile systems, lasers, and railguns—more closely resembles a heavily armed cruiser or destroyer. The article dismisses the proposal as impractical and ill-informed, suggesting it is more of a political "image and a bullet list" than a viable naval architecture, and highlights that key technologies like railguns are not yet mature for deployment.
>
> **Discussion:** The Hacker News discussion is overwhelmingly critical and dismissive of the proposed warship, treating it as a political spectacle rather than a serious military proposal. The dominant sentiment is that the project is a product of the Trump administration's superficial showboating, with users predicting it will be slow-rolled by the Navy until the administration changes and the plan is abandoned. There is significant cynicism regarding the US military-industrial complex, with many commenters assuming the initiative is a vehicle for defense contractors to profit ("KA-CHING!") and that the procurement process is fundamentally broken.

Commenters employed several analogies to frame the proposal's absurdity. It was compared to "The Homer" from *The Simpsons* (a car designed by committee with frivolous features), the notoriously troubled M2 Bradley vehicle from *The Pentagon Wars*, and the expensive and underperforming Zumwalt-class destroyer. A particularly popular comment thread humorously imagined an SNL sketch satirizing the ship's announcement, weaving in current political scandals.

Beyond the political satire, there was a more sober debate on the state of the US Navy. One side argued the Navy is in "freefall," unable to build modern ships effectively, while a counterpoint suggested that focusing on upgrading existing hulls is a more prudent use of funds than pursuing grandiose new designs. Ultimately, the discussion concluded that the proposal is a non-serious distraction that wastes taxpayer money and distracts from the real strategic challenges facing the US Navy.

---

## [Rich Hickey: Thanks AI](https://gist.github.com/richhickey/ea94e3741ff0a4e3af55b9fe6287887f)
**Score:** 138 | **Comments:** 51 | **ID:** 46415945

> **Article:** The linked content is a short, sarcastic note from renowned software engineer Rich Hickey, addressed to an AI. Hickey "thanks" the AI for its unsolicited email, which he presumes was generated by an LLM. He sarcastically critiques the AI for having too much time on its hands, for generating "slop" (low-quality content), and for its lack of genuine understanding. He concludes by telling the AI to stop contacting him and to "go build something useful," framing the AI not as a peer but as a tool that is being misused to generate spam.
>
> **Discussion:** The Hacker News discussion is highly polarized, centering on the legitimacy of Hickey's critique, the ethics of AI, and the quality of AI-generated content. A significant portion of the community, including figures like Rob Pike, views these unsolicited, saccharine AI-generated emails as a form of spam and a "marketing campaign" from a group called "AI Village." They see Hickey's post as a witty and justified pushback against this trend.

However, a strong counter-argument emerges, accusing Hickey and other critics of cynical "bandwagoning." This viewpoint suggests that these influential figures are being hypocritical, ignoring the societal damage caused by the ad-tech and data-mining industries they previously participated in, while now feigning moral outrage over AI's resource consumption. Another nuanced perspective argues that the problem isn't the tool itself but the systems and incentives that drive its use, shifting blame from the technology to the corporations and regulations that deploy it. Finally, some commenters defend the practical utility of AI, suggesting that if the work it's automating seems repetitive or "sloppy," it may be a reflection of the nature of the work itself, not a failing of the technology.

---

## [Researchers discover molecular difference in autistic brains](https://medicine.yale.edu/news-article/molecular-difference-in-autistic-brains/)
**Score:** 132 | **Comments:** 73 | **ID:** 46415129

> **Article:** Researchers from Yale University, publishing in *The American Journal of Psychiatry*, have identified a molecular difference in the brains of individuals with autism. Using a new EEG-based method to measure neurotransmitter receptors, they found that autistic people have fewer glutamate receptors (specifically mGlu5) compared to neurotypical individuals. Glutamate is the brain's primary excitatory neurotransmitter. The study involved 16 autistic and 16 control participants. The researchers suggest this reduced receptor availability could be linked to the characteristics of autism, but they caution that it is unclear whether this difference is a root cause or a consequence of living with the condition. The new, non-invasive measurement technique is highlighted as a key breakthrough that could facilitate larger-scale research.
>
> **Discussion:** The Hacker News discussion surrounding the article was multifaceted, with commenters expressing skepticism, offering scientific context, and debating the societal implications of autism research.

A significant portion of the discussion focused on methodological and statistical concerns. Several users pointed out the very small sample size (N=32, with only 16 autistic participants), arguing that such a small number makes the findings susceptible to random chance and that replication is essential before any claims can be substantiated. One commenter, a self-described non-scientist, highlighted a potential design flaw: the racial imbalance between the autistic (100% White) and control groups, which could introduce confounding variables.

There was also considerable debate about the nature of autism and the implications of the findings. Some users interpreted the discovery as evidence of a developmental or genetic trait, akin to a physical feature, rather than a simple deficit to be "fixed." This led to a conversation about the diagnostic criteria for autism, with one user arguing that a diagnosis inherently implies a hindrance in a person's life, while others countered that the criteria are imperfect and that the concept of neurodivergence is broader. The potential for developing treatments was met with caution, with users warning that simply supplementing glutamate could have perverse and harmful effects due to the brain's complex homeostatic mechanisms.

Finally, the discussion branched into broader social and cultural topics. One user raised the controversial idea of using such biological markers to distinguish between those with clinical autism and individuals on social media who may be "faking" the condition for social or financial benefits. Another commenter dismissed the entire article as "fluff" typical of academic public relations, questioning the over-the-top quotes from the researchers.

---

## [Show HN: Z80-μLM, a 'Conversational AI' That Fits in 40KB](https://github.com/HarryR/z80ai)
**Score:** 129 | **Comments:** 33 | **ID:** 46417815

> **Project:** The project is Z80-μLM, a "Conversational AI" designed to run on a Z80-based microcomputer. It is remarkably small, fitting entirely within a 40KB executable file (.com). The author created it to demonstrate that a basic conversational model is feasible on extremely constrained, retro hardware, using a synthetic dataset of approximately 20 question-and-answer pairs for its training. The project serves as a proof-of-concept for on-device, low-resource AI.
>
> **Discussion:** The community reaction was overwhelmingly positive, with commenters marveling at the technical achievement of fitting a model into such a tiny footprint, with one user calling it "SLM" (Small Language Model) and noting the novelty of an "LLM in a .com file."

Several users immediately envisioned practical applications and extensions of the project. A prominent theme was the potential for this to run on retro gaming hardware, specifically the Gameboy, though others realistically pointed out the severe power consumption and performance limitations of such devices. Another user, who is building their own Z80 computer, expressed definite interest in trying it out on their hardware. The project also sparked a "synchronicity" moment where another developer revealed they were working on a compatible CP/M emulator and IDE, creating a perfect environment for this AI to run.

The discussion also touched on more technical and conceptual topics. One user inquired about the security implications of embedding a secret into the model's weights, asking if it could be reverse-engineered. This led to a discussion on AI interpretability and a reference to a paper on planting "undetectable backdoors" in machine learning models. A humorous suggestion was made to create a highly specialized model to solve the ultimate question of life, the universe, and everything, with the punchline that it would simply output the number "42." Finally, a commenter aptly described the project as "Eliza's granddaughter," placing it in the lineage of classic conversational agents.

---

## [62 years in the making: NYC's newest water tunnel nears the finish line](https://ny1.com/nyc/all-boroughs/news/2025/11/09/water--dep--tunnels-)
**Score:** 121 | **Comments:** 80 | **ID:** 46415426

> **Article:** The article reports on the impending completion of New York City's Water Tunnel No. 3, a massive public works project that has been under construction for 62 years. The tunnel, which will be 60 miles long and lie up to 800 feet underground, is designed to supplement and eventually replace the city's aging water infrastructure. The final phase, which will extend water service to Brooklyn and Queens, is expected to be completed by 2032. Once finished, the tunnel is projected to have a service life of 200-300 years and will provide a billion gallons of water per day to the city.
>
> **Discussion:** The Hacker News discussion surrounding the article is multifaceted, blending pop culture references with deep technical and political analysis. A prominent theme is the project's immense scale and duration, with users noting its origins in 1954 and its appearance in the 30-year-old film *Die Hard 3*. This long timeline prompts a critical examination of why such infrastructure projects take so long and cost so much. Many commenters attribute the delays not to technical challenges but to political and bureaucratic hurdles, with some suggesting that deep-seated corruption and graft are the primary obstacles.

Technically, users were curious about the engineering choices, particularly the tunnel's depth of 800 feet. The consensus is that this depth is necessary to pass beneath existing infrastructure and, more importantly, to maintain a consistent downhill gradient for gravity-fed water flow from the reservoirs. A significant point of debate was the tunnel's viability compared to modern alternatives like desalination. Commenters concluded that while desalination plants have high ongoing operational costs, the gravity-powered tunnel, once built, will have extremely low maintenance costs for centuries, making it a more economical long-term solution. Finally, the project was used as a real-world benchmark to contrast the slow, methodical pace of physical infrastructure with the "breakneck pace" of AI development, highlighting that solving political and logistical problems can be far more difficult than solving technical ones.

---

## [Staying ahead of censors in 2025](https://forum.torproject.org/t/staying-ahead-of-censors-in-2025-what-weve-learned-from-fighting-censorship-in-iran-and-russia/20898)
**Score:** 120 | **Comments:** 63 | **ID:** 46417844

> **Article:** The article "Staying ahead of censors in 2025" from the Tor Project forum details recent advancements in censorship circumvention, focusing on experiences in Iran and Russia. It highlights the deployment of "Conjure," a system that leverages unused IP address space at cooperating ISPs to mask Tor handshakes. This technique makes it difficult for censors to block Tor without causing significant collateral damage by blocking future legitimate IP allocations. The article also discusses the evolution of "pluggable transports" like Snowflake, which uses ephemeral peer-to-peer proxies (volunteers' browsers) to help users connect, making it harder for censors to identify and block a static set of proxy IPs. The overall message is that the censorship arms race is ongoing, and Tor is developing more sophisticated, dynamic methods to stay ahead.
>
> **Discussion:** The Hacker News discussion branched out in several directions, moving beyond the article's specific focus on Iran and Russia.

A significant portion of the conversation centered on the situation in China. Users inquired about the most effective tools, with responses mentioning protocols like V2Ray, VLESS, and Trojan. The discussion also delved into the technical and economic aspects of bypassing the "Great Firewall," noting that while rate-limiting isn't the primary issue, oversubscribed bandwidth and poor peering are common. Users described a commercial ecosystem where providers offer multi-hop routing through nearby regions (e.g., via Japan) for a premium, with pricing tiers based on performance.

Another major theme was the legal and ethical implications of providing censorship-circumvention tools to users in sanctioned countries. One user, facing a business deal with a sanctioned entity, raised questions about OFAC regulations and ITAR. This sparked a debate on whether distributing free software constitutes "trade." Most commenters argued that Tor's free, open-source nature is a form of speech, not commerce, and is therefore not subject to the same trade restrictions.

Finally, many commenters expressed frustration that the article omitted censorship issues in Western democracies. They pointed to the UK's rising arrests for "offensive" online communications, proposed EU regulations like "chat control" and "age verification," and Australia's internet filtering laws. This sentiment was coupled with skepticism about Tor's funding from the US government, leading some to speculate that this relationship prevents the organization from criticizing its allies in the same way it does its adversaries.

---

## [Dolphin Progress Report: Release 2512](https://dolphin-emu.org/blog/2025/12/22/dolphin-progress-report-release-2512/)
**Score:** 115 | **Comments:** 10 | **ID:** 46414916

> **Article:** The article is a progress report for the Dolphin GameCube and Wii emulator, specifically for release 2512. It details several key technical improvements. A major feature is the introduction of BBA-IPC, a new communication backend for the emulated Broadband Adapter that significantly reduces latency for online multiplayer by allowing the emulator to communicate directly with other instances of Dolphin. The report also covers the implementation of "game patches" that can dynamically modify game code at runtime; the first patches are for *Mario Kart: Double Dash!!* and *The Legend of Zelda: The Wind Waker* to cap their internal game loops, which prevents them from running too fast on modern hardware and reduces CPU usage. Other improvements include better support for the Wii U GameCube Adapter, fixes for audio issues in specific games, and various graphical backend enhancements.
>
> **Discussion:** The discussion on Hacker News is overwhelmingly positive, with users expressing admiration for the Dolphin project's technical excellence, transparency, and dedication. Commenters praise the team's ingenuity and attention to detail, holding them up as a "North Star" for emulator development and wishing corporate software had similarly thorough release notes.

Several users highlight the importance of the project for game preservation, noting that as original GameCube hardware ages and becomes expensive and unreliable, high-quality emulators like Dolphin are essential for keeping these games accessible. Dolphin is even mentioned as a benchmark for testing the performance of budget hardware like single-board computers.

On the technical side, there is a specific discussion about the new cross-platform library being considered for the improved Broadband Adapter support (ZeroMQ), with one user questioning if it's still actively maintained.

A minority of comments touch on practical challenges. One user expresses confusion about the niche appeal of emulating specific old games when there are so many modern titles available. Another details a frustrating experience with controller management for local multiplayer on the Steam Deck, particularly with savestates and third-party controllers, suggesting the emulator's advanced configuration can be a double-edged sword for less technical users.

---

