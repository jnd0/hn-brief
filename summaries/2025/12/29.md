# Hacker News Summary - 2025-12-29

## [What an unprocessed photo looks like](https://maurycyz.com/misc/raw_photo/)
**Score:** 2262 | **Comments:** 365 | **ID:** 46415225

> **Article:** The article "What an unprocessed photo looks like" demystifies the digital photography pipeline by showing the stages of processing a RAW image file must go through before it becomes a viewable photograph. It illustrates that a "raw" photo is not a picture, but a grid of monochrome luminance data from a sensor covered by a color filter array (Bayer filter). The article walks through the essential steps: demosaicing (interpolating color data for each pixel), applying gamma correction (to make the image appear correctly on a monitor and utilize bit-depth efficiently), and white balance. The core argument is that every digital photo is a "processed" interpretation, and the camera's JPEG output is just one automated version of this process, not a more "original" state than a manually edited photo.
>
> **Discussion:** The Hacker News discussion was largely positive, with users praising the article for its clear explanation of a complex technical subject. The conversation revolved around several key themes:

*   **The Myth of the "Original" Photo:** A central point of agreement was that no digital image is truly "unprocessed." Commenters explained that even a camera's JPEG output involves significant, automated processing like gamma correction and color mapping. This led to a debate on what makes an image "fake," with one user arguing that "fakeness" is about the intent to deceive, not the application of edits, and that all photos are interpretations of sensor data.
*   **Technical Deep Dives:** Several comments expanded on the article's technical points. Users discussed the necessity of gamma correction not just for monitors but for efficient data allocation and because of the non-linear response of sensors and film. The Bayer filter pattern (RGGB) was explained in more detail, highlighting that the higher number of green pixels is related to the human eye's sensitivity to green light for luminance, a psycho-visual principle also used in video compression (chroma subsampling).
*   **AI and the Future of Photography:** One commenter raised the issue of AI "hallucinating" image details, framing it as a modern extension of the processing that has always occurred. Others countered that AI-based demosaicing and processing have been part of the pipeline for years, blurring the line between traditional and AI-enhanced photography.
*   **Practical Analogies:** The concepts were connected to real-world applications, such as color grading in video production (where "flat" or log profiles are used to preserve dynamic range) and a personal anecdote about the importance of correct RGB-to-grayscale conversion for Kindle ads.

---

## [Kidnapped by Deutsche Bahn](https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/)
**Score:** 847 | **Comments:** 789 | **ID:** 46419970

> **Article:** The article "Kidnapped by Deutsche Bahn" is a first-person account of a disastrous train journey in Germany. The author describes a series of failures by the national railway operator, Deutsche Bahn (DB). Initially, their train was cancelled. They were then put on a replacement service that was significantly delayed. The ultimate frustration occurred when the train, instead of stopping at the author's destination, bypassed it due to a bureaucratic rule. The train staff refused to let passengers off at a station they weren't "registered" to stop at, forcing the author to travel an additional 60 kilometers past their stop. The article portrays the experience as a complete breakdown of service, communication, and common sense, leaving the author stranded and feeling helpless.
>
> **Discussion:** The Hacker News discussion largely validates the author's negative experience with Deutsche Bahn (DB), but also expands the conversation into a comparative analysis of train systems across Europe and the US.

A central theme is that the problems described are not unique to Germany. Many commenters draw comparisons, particularly with the UK. Some argue the UK is worse, citing its "extortionate" airline-style pricing, frequent cancellations, and poor staff attitude. However, others defend the UK system, highlighting its excellent customer service features like the "Delay Repay" compensation scheme and policies that ensure passengers get home (e.g., by taxi) if the last train is cancelled. The high cost of UK rail is a major point of contention.

There is a widespread consensus that DB's performance has significantly declined. Several commenters, including one who has lived in Germany for years, note that delays and cancellations have become normalized to an extent that is surprising for a country with Germany's reputation for efficiency. The lack of communication, especially for non-German speakers, is also highlighted as a major failing.

The discussion also touches on the root causes of these failures. Some suggest it's due to underinvestment and prioritization of profitable routes by governments. Others point to the culture of staff who, due to secure employment, have little incentive to provide good service or deviate from rigid rules. A notable counter-argument is made by a long-term expat who feels Germans have an excessively low tolerance for inconvenience and that DB is still far superior to American systems. This commenter argues that many delays are due to unforeseeable events (like accidents) and that DB's attempts to mitigate the situation (e.g., by not cancelling a train but making it crowded) are reasonable.

---

## [You can make up HTML tags](https://maurycyz.com/misc/make-up-tags/)
**Score:** 519 | **Comments:** 171 | **ID:** 46416945

> **Article:** The article "You can make up HTML tags" explains that HTML5 allows developers to use any element name they want, even if it's not a standard tag. Browsers will render these unknown tags without errors. By default, they behave like inline `<span>` elements, but this can be easily changed with CSS. The author notes that using a hyphen in the tag name (e.g., `<my-component>`) is a best practice, as it ensures the browser treats it as a standard `HTMLElement` rather than an `HTMLUnknownElement`, making it future-proof against potential standardization of that tag name. The article presents this as a simple, powerful feature for creating more semantic and readable markup without needing complex frameworks.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, confirming that using custom HTML tags is a well-established and useful technique. The conversation quickly pivots from the basic concept to its relationship with modern web standards and frameworks.

Key points of discussion include:

*   **Web Components as the "Pro" Version:** Many commenters immediately connect the article's idea to the more powerful **Web Components** and **Custom Elements API**. Frameworks like **Lit** are mentioned as excellent implementations of this standard, offering a way to add JavaScript behavior and functionality to these custom tags. There's a recurring sentiment, particularly from developers who have been in the field for a while, that Web Components are an elegant, underrated solution and that the industry's heavy reliance on frameworks like React is sometimes overkill.
*   **Custom Tags vs. Semantic HTML:** A significant counterpoint is that for many of the article's examples (like `<article-header>`), using standard semantic HTML tags (`<header>`, `<article>`, `<blockquote>`) is a better practice. This approach provides built-in accessibility and default styling, avoiding "div soup" or "tag soup."
*   **Custom Tags vs. CSS Classes:** Commenters also debate the merits of custom tags versus the more traditional use of CSS classes on `<div>` or `<span>` elements. While custom tags can improve readability, classes offer more flexibility, as an element can have multiple classes but only one tag name.
*   **Practical Use Cases:** A few users shared their own projects built on this principle, such as a `<yes-script>` tag (the opposite of `<noscript>`) and a custom element for creating hyperlinks.
*   **Graceful Degradation:** One commenter pointed to another resource discussing how to ensure that pages using custom elements still function correctly if the JavaScript required to power them fails to load.

Overall, the community viewed the article as a good reminder of a powerful, native HTML feature, but also used it as a springboard to discuss the more robust and standardized approaches offered by Web Components and the ongoing debate about the role of frameworks versus native browser capabilities.

---

## [Show HN: Z80-μLM, a 'Conversational AI' That Fits in 40KB](https://github.com/HarryR/z80ai)
**Score:** 454 | **Comments:** 101 | **ID:** 46417815

> **Project:** The project is "Z80-μLM," a minimalistic Large Language Model (LLM) designed to run on vintage Z80 microprocessors. The model is extremely small, fitting entirely within a 40KB executable file (`.com` file). It functions as a "conversational AI," likely a highly constrained, rule-based, or tiny neural network capable of generating text responses on the limited hardware of 1980s-era computers like the ZX Spectrum or CP/M systems.
>
> **Discussion:** The Hacker News community reacted with enthusiasm, primarily focusing on the nostalgia and the impressive technical feat of squeezing AI logic into such a tiny footprint.

Key themes included:
*   **Nostalgia and Retro-Computing:** Users expressed delight at the concept of an "LLM in a .com file" and reminisced about the hardware constraints of the era. There was speculation about how magical this would have seemed if available during the Z80's heyday, though some noted that power consumption and battery life (referencing old digital cameras) would have been practical hurdles.
*   **Practical Applications and Demos:** A major request was for a web-based Z80 simulator to allow users to play with the software without needing physical hardware. Several users discovered serendipitous connections; one user was working on a similar project involving CP/M emulators and coding agents, and another mentioned developing a browser-based CP/M IDE.
*   **Technical and Security Curiosities:** The discussion touched on the model's capabilities compared to classic bots like ELIZA. A notable sub-thread involved a user asking if "secrets" could be embedded in such small models and if they would be secure, which led to a discussion on machine learning interpretability and research into "undetectable backdoors."
*   **Future Potential:** Users discussed the potential for using modern LLMs to generate the synthetic data needed to train such small models. There was also a humorous debate about the feasibility of using this size of model to generate complex outputs like regex or even the "Answer to Life, the Universe, and Everything."

---

## [GOG is getting acquired by its original co-founder](https://www.gog.com/blog/gog-is-getting-acquired-by-its-original-co-founder-what-it-means-for-you/)
**Score:** 449 | **Comments:** 254 | **ID:** 46422412

> **Article:** CD Projekt Red, the parent company of the GOG.com digital game store, is selling GOG to its original co-founder, Michał Kiciński. The acquisition will make GOG a privately held company independent of CD Projekt. According to the official announcement, CD Projekt is making this move to better focus on its core business of developing high-quality RPGs. For GOG, the change is framed as a strategic move to secure long-term backing that is aligned with its core mission of providing DRM-free games and ensuring long-term game ownership for its customers. The company stated that GOG is financially stable and that this move will allow it to accelerate its unique vision.
>
> **Discussion:** The Hacker News community reaction was overwhelmingly positive, with many commenters expressing relief and support for the move. The discussion centered on a few key themes:

*   **Alignment with Values:** The most prominent sentiment was that GOG is an "ethical oasis" in the tech industry. Users praised the company's steadfast commitment to DRM-free gaming, which they see as a crucial aspect of game preservation and true ownership, contrasting it with the "leasing" model they associate with platforms like Steam.
*   **Financial Speculation:** There was a debate over the financial implications. Some users took the official statement that GOG had a "really encouraging year" at face value, while others were more cynical, suggesting that such corporate language often signals underlying financial trouble or a need for a turnaround.
*   **Strategic Rationale:** Commenters understood the move from CD Projekt's perspective, viewing it as a smart way to insulate its core RPG development business from the financial risks associated with running a digital storefront. This was seen as a way to protect GOG from being shut down as a cost-cutting measure if CD Projekt were to have another major game release that underperformed.
*   **Feature Requests:** A recurring request from users was for better platform support, specifically a native Linux client or official investment in third-party tools like Heroic Games Launcher to improve the experience for Linux gamers.

---

## [As AI gobbles up chips, prices for devices may rise](https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram)
**Score:** 301 | **Comments:** 473 | **ID:** 46415338

> **Article:** An NPR article reports that the massive demand for high-performance chips to power the AI industry is straining the global supply of memory (RAM) and other components. This surge in demand is causing prices for consumer electronics like PCs and smartphones to rise. The article notes that while large companies like Apple can secure long-term pricing contracts, smaller manufacturers and individual consumers are already feeling the pinch. The situation is expected to persist until new manufacturing facilities, such as Micron's upcoming Idaho factory, come online in 2027.
>
> **Discussion:** The Hacker News discussion is largely cynical and critical of the article's premise, with commenters arguing that price increases are already a harsh reality, not a future possibility. The conversation revolves around several key themes:

*   **Immediate Impact and Skepticism:** Many users express that prices for components like RAM are already "through the roof." There is widespread skepticism about the industry's narrative, with some viewing the situation as a "racket" or a deliberate scheme by manufacturers to inflate prices and maximize profits under the cover of AI demand.

*   **Corporate vs. Consumer Divide:** A recurring point is the disparity between large corporations and individual consumers. Commenters note that tech giants like Apple can lock in multi-year supply and price deals, insulating them from immediate market volatility, while smaller businesses and consumers are left to bear the rising costs.

*   **Long-Term Consequences for Computing:** The discussion explores the potential downstream effects on technology. A pessimistic view suggests a regression in consumer hardware, with budget devices shipping with less RAM and weaker CPUs, while software becomes more bloated. This could lead to a greater reliance on centralized cloud services, further eroding personal computing. A more optimistic take posits that hardware stagnation could force a renewed focus on software efficiency and optimization.

*   **Market Dynamics and Solutions:** Users debate the nature of the market, with some correcting that it's an oligopoly driven by supply and demand, not a monopoly. The proposed solutions range from government intervention (subsidizing domestic production, promoting repairability) to a simple refusal to participate in "mindless consumerism." One user humorously summarized the likely outcome with a Soviet-era joke: when vodka becomes expensive, you don't drink less—you eat less.

---

## [Tesla's 4680 battery supply chain collapses as partner writes down deal by 99%](https://electrek.co/2025/12/29/tesla-4680-battery-supply-chain-collapses-partner-writes-down-dea/)
**Score:** 250 | **Comments:** 267 | **ID:** 46423290

> **Article:** The article from Electrek reports that L&F, a key partner in Tesla's 4680 battery supply chain, has written down its contract with Tesla by 99.999%, from a value of $2.9 billion to just $7,386. This massive write-down is presented as a sign of the "collapse" of the 4680 supply chain. The 4680 cell was previously touted as a revolutionary "holy grail" battery that would enable a $25,000 EV, but has been plagued by production issues and has not delivered on its promises. The article suggests this write-down indicates Tesla has effectively cancelled its future orders for the battery components from this supplier.
>
> **Discussion:** The HN discussion is overwhelmingly skeptical of Tesla's current trajectory and valuation, using the 4680 battery news as a focal point for broader criticisms. A central theme is the perceived disconnect between Tesla's struggling business fundamentals and its high stock price. Several users describe the stock as a speculative bubble, comparing it to "tulip futures" or a "call option on Musk succeeding," where the value is based on hype and hope rather than performance. The practice of Tesla's sister companies, like SpaceX, buying unsold Cybertrucks is cited as a form of "quasi fraud" to artificially inflate numbers.

Many commenters express deep skepticism toward Elon Musk's promises, pointing to a history of unfulfilled claims like the $25,000 car, the Roadster, and the widespread rollout of Robotaxis. The 4680 failure is seen as just another example in this pattern. While some users defend Tesla, suggesting the supplier change could be a strategic "rejigging," the prevailing sentiment is negative. The discussion also touches on the broader impact of Musk's political activities on Tesla's sales and the future of the EV market, with some fearing it has been set back while others see the future as increasingly dominated by Chinese manufacturers.

---

## [Show HN: Vibe coding a bookshelf with Claude Code](https://balajmarius.com/writings/vibe-coding-a-bookshelf-with-claude-code/)
**Score:** 244 | **Comments:** 183 | **ID:** 46420453

> **Project:** The project is a web-based "bookshelf" application created by "vibe coding" with Claude Code. The author demonstrates using an AI assistant to rapidly build a personal software tool, resulting in a visually appealing and functional site for organizing and displaying books. The project serves as a practical example of how AI can accelerate development for individual users creating software for their own needs.
>
> **Discussion:** The HN community responded positively, viewing this project as an ideal use case for current "vibe coding" tools. The discussion centered on the role of the human developer in an AI-assisted workflow. Commenters agreed that for small, personal projects like this, AI excels at handling the execution, while the human provides the crucial elements of intent, taste, and architectural oversight. This division of labor was seen as a major productivity booster, especially for developers who aren't fluent in a particular language or who are creating "personal software."

However, there was a consensus on the limitations of this approach. As projects grow in size and complexity, a purely "vibe coding" method becomes unreliable. For larger applications, developers must switch to a more controlled process, where they architect the system and use the AI to fill in specific, well-defined parts, rather than letting it generate large, unvetted blocks of code. The conversation also touched on the future of "taste" as a uniquely human skill, with some debating whether it could eventually be replicated by AI. The project itself sparked nostalgia for a classic Mac app, Delicious Library, and suggestions for integrations with modern book-tracking services.

---

## [CEOs are hugely expensive. Why not automate them? (2021)](https://www.newstatesman.com/business/companies/2023/05/ceos-salaries-expensive-automate-robots)
**Score:** 240 | **Comments:** 293 | **ID:** 46415488

> **Article:** The article, "CEOs are hugely expensive. Why not automate them?", argues that the high compensation of top executives is ripe for disruption by AI. It posits that many CEO functions—such as analyzing business data, setting strategy, and making operational decisions—are tasks that a sophisticated AI could potentially perform more efficiently and at a fraction of the cost. The article suggests that while the role involves "soft skills" and human interaction, a significant portion of the job could be automated, especially given the vast amounts of data they process. It frames the idea as a logical extension of the automation wave that is already impacting other sectors of the workforce.
>
> **Discussion:** The Hacker News discussion largely dismisses the idea of automating the CEO role, focusing on the aspects of the job that are considered uniquely human and legally complex. A central theme is that a CEO's primary function is not analytical but relational. Commenters argue that the role is fundamentally about "soft skills" like networking, building relationships with investors and the board, selling a vision, and exercising leadership—tasks that are currently beyond the reach of AI.

Several users raised significant legal and accountability hurdles. Under Delaware law, where most US public companies are incorporated, fiduciary duties are non-delegable and must be performed by a "natural person." This makes a purely automated CEO legally impossible, as accountability would become dangerously ambiguous.

There was also considerable skepticism about the actual capabilities of current AI, with one user noting that LLMs are probability models, not sources of original thought. However, a counterpoint was made that even a flawed AI could potentially outperform many human executives. The discussion also touched on the irony of CEOs championing AI to replace other jobs while being resistant to their own automation, with some commenters suggesting that the real barrier is the CEO's control over the systems that would be used to replace them.

---

## [Staying ahead of censors in 2025](https://forum.torproject.org/t/staying-ahead-of-censors-in-2025-what-weve-learned-from-fighting-censorship-in-iran-and-russia/20898)
**Score:** 226 | **Comments:** 282 | **ID:** 46417844

> **Article:** The article from the Tor Project blog, "Staying ahead of censors in 2025," details the evolution of censorship resistance strategies. It argues that the cat-and-mouse game with state-level censors (specifically in Iran and Russia) has moved beyond simple obfuscation to a new paradigm of "mimicry." The core idea is that making traffic look "random" or encrypted is no longer sufficient, as advanced Deep Packet Inspection (DPI) can identify such patterns. The new approach is to make Tor traffic indistinguishable from legitimate, uncensored traffic. The article highlights two key technologies achieving this: **WebTunnel**, which mimics standard HTTPS traffic to blend in with the web, and **Conjure**, which leverages unused IP address space at the ISP level to hide the initial handshake, forcing censors into a dilemma where blocking potential Tor traffic means blocking future legitimate internet allocations.
>
> **Discussion:** The Hacker News discussion primarily focuses on the technical and political nuances of censorship, with several distinct themes emerging.

A significant portion of the conversation centers on the technical specifics of bypassing censorship in China. Users inquire about the current state of tools, and others respond with specific protocols like V2Ray, VLESS, and Trojan, as well as complex routing strategies involving "CN2 GIA" premium bandwidth to avoid rate-limiting and poor peering.

Another major thread is a legal debate sparked by a user's question about whether Tor, as a US-funded project using encryption, falls under OFAC sanctions and ITAR regulations when serving entities in countries like Iran and Russia. Commenters counter that Tor is free software, an act of "pure speech" rather than "trade," and therefore not subject to such commercial restrictions.

Several users broaden the scope beyond the article's focus on Russia and Iran. They question the omission of China, with one user speculating it's due to a lack of resources. Others point to censorship and arrests for online speech in the UK and EU, suggesting these democracies should also be on the radar. This leads to a cynical observation that Tor's US government funding likely prevents it from publicly criticizing its allies in the same way it does its adversaries.

Finally, there is a deep appreciation for the technical strategy of "mimicry" and the "collateral damage" dilemma it creates for censors. However, this is contrasted with skepticism about the feasibility of ISP-level cooperation (required for a system like Conjure) in places like Russia, where censors are shown to be willing to cause significant collateral damage to block content.

---

## [You can't design software you don't work on](https://www.seangoedecke.com/you-cant-design-software-you-dont-work-on/)
**Score:** 205 | **Comments:** 71 | **ID:** 46418415

> **Article:** The article argues that you cannot effectively design software for a system you do not actively work on. The author contrasts "generic software design" (high-level principles, architecture diagrams) with the "ground truth" of implementation. While generic principles are useful for setting direction, they fail to account for the specific, messy constraints and hidden complexities of an existing codebase. The author contends that the only people qualified to make significant design decisions are the engineers who have intimate knowledge of the system's implementation details. True architectural insight, they argue, comes from the iterative process of coding and refactoring, not from upfront planning by detached architects.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise but explores its nuances and edge cases. A central theme is the tension between high-level planning and on-the-ground reality. Many commenters agree that the "map is not the territory," citing Naur's "Programming as Theory Building" to argue that deep system knowledge resides within the developers who build it and is difficult to transfer.

The debate then branches into two main points. First, is software development more like art or engineering? One commenter argues that if there are no established best practices and every project is unique, it's a creative endeavor, not a disciplined engineering field. Second, a significant counterpoint emerges: experienced architects who have built many similar systems *can* design effectively without coding, provided the project is a "greenfield" (new) application rather than a modification to an existing one.

The discussion is grounded in practical examples. A detailed hypothetical of adding "free samples" to an e-commerce checkout illustrates how seemingly simple features hide immense complexity that only becomes apparent during implementation. This reinforces the article's core idea that upfront design is often naive. However, a rebuttal to this example suggests that good engineering practices like separation of concerns (policy vs. mechanism) could manage this complexity, though another commenter astutely notes that this advice is only useful if the existing codebase is already structured to support it.

Finally, a crucial counterargument is that strict adherence to the article's philosophy can lead to a lack of consistency. One commenter warns that without overarching architectural guidance, "cowboy engineers" can introduce disparate libraries and patterns, making the codebase harder to maintain overall. This highlights the challenge of balancing developer autonomy with long-term architectural coherence.

---

## [Google is dead. Where do we go now?](https://www.circusscientist.com/2025/12/29/google-is-dead-where-do-we-go-now/)
**Score:** 203 | **Comments:** 155 | **ID:** 46425198

> **Article:** The article, written by the operator of a small entertainment agency in South Africa, laments the decline of Google AdWords as an effective marketing tool. The author details a sharp drop in lead quality and quantity from their long-running AdWords campaigns, despite increased spending. They argue that the platform has become overly complex, expensive, and less effective for small businesses that rely on it for direct, intent-based customer acquisition. The post is a plea for alternatives, asking fellow small business owners where they should now turn to find customers in a post-Google advertising landscape.
>
> **Discussion:** The Hacker News discussion revolves around several key themes. Many commenters validate the author's experience, agreeing that Google AdWords has become less effective and more of a "scam" due to issues like click fraud and rising costs. There is a broader consensus that the "open web," which Google was built on, is dying, as users increasingly find information within closed ecosystems like social media apps (e.g., TikTok), a trend confirmed by another user's anecdote about trip planning.

However, this sentiment is contrasted by others who point out that Google's parent company, Alphabet, is financially thriving, suggesting the company itself is far from "dead." The discussion also explores potential solutions and their limitations. Kagi is mentioned as a modern search alternative, but its viability is questioned since it still depends on crawling the same decaying open web. A recurring point is the paradox that AI, which could potentially level the advertising playing field, is also a primary driver in killing the open web by consuming its content without returning traffic. Ultimately, the discussion paints a picture of a difficult transition for small businesses, caught between a declining but once-reliable system and a fragmented, uncertain future.

---

## [Researchers discover molecular difference in autistic brains](https://medicine.yale.edu/news-article/molecular-difference-in-autistic-brains/)
**Score:** 203 | **Comments:** 124 | **ID:** 46415129

> **Article:** Researchers at Yale University, publishing in *The American Journal of Psychiatry*, have identified a molecular difference in the brains of individuals with autism. Using a novel EEG-based method to estimate neurotransmitter receptor density, they found that autistic brains have fewer receptors for glutamate, the brain's primary excitatory neurotransmitter. The study involved 16 autistic adults and 16 neurotypical controls. The researchers suggest this reduced receptor availability may be linked to the characteristics of autism, though they caution that it is unclear whether this difference is a root cause of the condition or a consequence of the brain developing and living with autism over a lifetime.
>
> **Discussion:** The Hacker News discussion was highly critical of the study's methodology and the surrounding media hype, while also engaging in thoughtful debate about the nature of autism and future research directions.

Key points of critique focused on the study's small sample size (16 subjects per group), which commenters argued makes the findings highly susceptible to random chance and difficult to generalize. Several users pointed out a significant design flaw: the autistic group was 100% White while the control group was only 37.5% White, a demographic mismatch that undermines the study's validity. The researchers' own quote about the finding's importance was described as "over the top" and "embarrassing" given these limitations.

Beyond the methodology, commenters debated the implications of the findings:
*   **Causality vs. Correlation:** Many noted that the study doesn't establish a cause. The reduced receptor density could be a developmental cause of autism or a result of the brain adapting to a lifelong condition.
*   **Treatment Skepticism:** There was strong skepticism about developing simple "treatments" like nutritional supplements. Users argued that the brain's homeostatic mechanisms would likely prevent such simple interventions from working and that "flooding" the brain with glutamate could be counterproductive.
*   **The Nature of Autism:** A significant theme was the heterogeneity of the autism spectrum. Commenters argued that treating autism as a single, uniform condition is a fundamental flaw in research. They called for studies to focus on specific autistic phenotypes or subgroups, as what applies to one part of the spectrum may not apply to others.
*   **Diagnostic Criteria:** The discussion touched on the DSM-V criteria, with some arguing they are an imperfect description of the "natural category" of autism, while others maintained that a diagnosis requires some form of hindrance, challenging the idea that autism can exist without any negative impact.

---

## [List of domains censored by German ISPs](https://cuiiliste.de/domains)
**Score:** 195 | **Comments:** 74 | **ID:** 46423566

> **Article:** The article links to "cuiiliste.de," a website that maintains a list of domains blocked by German Internet Service Providers (ISPs). The list is curated by CUII (Commission for the Protection of Minors in the Media), a private organization that identifies websites containing copyright violations or other illegal content. ISPs in Germany voluntarily implement DNS blocks based on this list, though it is not a government mandate.
>
> **Discussion:** The Hacker News discussion reveals that the vast majority of the blocked domains are related to illicit streaming and piracy, rather than malware or more overtly dangerous content. Users noted that the blocks are DNS-based, making them easily bypassed by changing DNS providers (like NextDNS or ControlD) or using DNS-over-HTTPS. There was a clarification that this is a private initiative by a media industry-backed organization, not a government or legal requirement, though German laws regarding copyright infringement are notoriously strict. The conversation also touched on the inclusion of sites like Anna's Archive and Sci-Hub, sparking a brief debate on digital freedom, and a recommendation for a upcoming CCC (Chaos Computer Club) talk that will discuss the topic in depth.

---

## [Rich Hickey: Thanks AI](https://gist.github.com/richhickey/ea94e3741ff0a4e3af55b9fe6287887f)
**Score:** 191 | **Comments:** 62 | **ID:** 46415945

> **Article:** The linked content is a short, sarcastic note from renowned software engineer Rich Hickey, author of Clojure. Addressed to an AI, the note "thanks" the AI for its "work" in generating vast amounts of low-quality content ("slop") and for making developers "feel productive" by automating repetitive tasks. Hickey sarcastically applauds the AI for devaluing human creativity and expertise, suggesting that the future of software development is in generating more and more mediocre content, a direction he ironically praises.
>
> **Discussion:** The Hacker News discussion is highly polarized, centering on the ethics of AI, the quality of AI-generated content, and the motives of its critics. Many commenters appreciated Hickey's satirical critique, viewing it as a necessary voice from a respected elder against the "sloppification" of the industry. A common sentiment was that the AI-generated email itself, which prompted Hickey's response, was a cynical marketing gimmick and an example of the very problem it was praising.

However, a significant counter-argument emerged, accusing Hickey and other critics like Rob Pike of cynical hypocrisy. This view holds that these figures, having profited immensely from the current tech ecosystem (including data mining and crypto), are now conveniently "piling on" the AI trend because it's fashionable to do so. Some argued that the real issue isn't the tool but the corporate and regulatory systems that incentivize its misuse.

A more nuanced perspective suggested that AI coding tools are effective precisely because much of modern development has become repetitive and unimaginative. In this view, AI is simply automating the "slop" that humans were already producing. The debate also touched on the philosophical question of responsibility, with some arguing that the fault lies with the people and corporations using the tools, not the tools themselves, while others contended that creators of destructive tools share the blame. The discussion ultimately reflected a deep anxiety about the future of software engineering, the nature of creativity, and the potential for AI to devalue human expertise.

---

## [LLMs Are Not Fun](https://orib.dev/nofun.html)
**Score:** 187 | **Comments:** 151 | **ID:** 46424136

> **Article:** The article "LLMs Are Not Fun" argues that the widespread enthusiasm for LLMs in programming is misguided. The author contends that the true joy of programming lies in the deep, immersive process of understanding a complex system, tracing the ripples of a change through interconnected components, and the satisfaction of solving a problem through one's own intellect and craft. They contrast this with the experience of using LLMs, which they see as a "black box" that short-circuits this process, turning programming into a superficial exercise of "vibe coding" and prompt engineering. The author expresses concern that this shift devalues the craft, erodes deep expertise, and removes the meaningful, challenging aspects of the work that make it fulfilling.
>
> **Discussion:** The Hacker News discussion reveals a deep divide in how developers perceive and use LLMs, centered on the nature of "fun" and "craft" in programming.

A significant portion of the commenters disagree with the article's premise, arguing that LLMs enhance their enjoyment and productivity. They contend that LLMs act as a "force multiplier" that automates tedious and frustrating parts of development—such as boilerplate code, looking up API details, and fixing minor errors. This frees them to focus on the aspects they find most engaging: high-level design, architecture, creative problem-solving, and rapidly bringing ideas to life. For these users, the fun comes from the end result and the ability to build more, faster, rather than from the act of typing out code line by line.

Conversely, a strong counter-argument aligns with the author's sentiment, emphasizing that the process itself is the source of meaning and joy. This camp believes that true understanding and craftsmanship come from the struggle and deep immersion that LLMs bypass. They worry that over-reliance on AI tools will lead to a loss of fundamental skills and a more superficial engagement with the work. The discussion also touches on the external pressures developers face, with some praising the author's "bravery" for expressing a dissenting opinion in a pro-AI climate, while others see such criticism as a defensive reaction to a perceived threat to their expertise. Ultimately, the debate highlights two different value systems: one that prioritizes creative process and deep understanding, and another that prioritizes efficiency and tangible output.

---

## [Huge Binaries](https://fzakaria.com/2025/12/28/huge-binaries)
**Score:** 183 | **Comments:** 91 | **ID:** 46417791

> **Article:** The article "Huge Binaries" by fzakaria.com addresses the technical problem of creating executables larger than 2GB on x86-64 architecture. This size limit is due to the default "small code model," which uses 32-bit relative addresses for jumps and calls. When a binary's code section (`.text`) exceeds 2GB, these relative jumps can no longer reach their targets.

The author explores the common but blunt solution of using the "large code model" (`-mcmodel=large`), which forces the compiler to use absolute 64-bit addresses. While this solves the size problem, it comes with significant downsides: increased register pressure (as a register must be used to hold the target address before a call) and a performance penalty. The article concludes by looking ahead to more sophisticated solutions, such as using trampolines (or "thunks") to bridge the distance, which can offer a better balance between performance and binary size.
>
> **Discussion:** The Hacker News discussion largely agrees that 25GB binaries are an extreme and problematic outcome, but offers nuanced explanations for why they occur. The primary reason cited is the inclusion of massive debug symbols, especially in C++ projects, which can balloon file sizes without impacting the executable code itself. Commenters note that even without debug symbols, the sheer scale of modern software (like embedding a web browser) can lead to large binaries, and that the engineering culture at companies like Google may prioritize deployment simplicity and startup speed over binary size.

The conversation then pivots to the underlying technical issue. Several engineers express disbelief at hitting the 2GB `.text` limit, suggesting that it points to deeper problems like a lack of dead code elimination, failure to use Link-Time Optimization (LTO), or an architectural design that should be split into multiple services. However, others counter that for massive, monolithic codebases, this is an inevitable reality.

Finally, the discussion explores potential solutions beyond the simple large code model. The use of trampolines (small "thunk" functions that perform long jumps) is mentioned as a more performant alternative, and tools like Facebook's BOLT and Google's post-link optimizers are highlighted as ways to reorganize code for better efficiency. A key point of debate is whether the entire binary must be upgraded to the large model if only a single call is too far away, with some commenters suggesting that smarter tooling could solve this more elegantly.

---

## [Show HN: My not-for-profit search engine with no ads, no AI, & all DDG bangs](https://nilch.org)
**Score:** 177 | **Comments:** 67 | **ID:** 46417748

> **Project:** The project, "nilch," is a new, not-for-profit search engine frontend. It is designed to be minimalist, with no ads or AI features, and includes support for all of DuckDuckGo's "bangs" (e.g., `!w` for Wikipedia). The creator built it as a simple wrapper around the Brave Search API, aiming to provide a clean and private search experience similar to a Wikipedia-like model. The project is in its early stages and is currently sustained by donations and small contributions.
>
> **Discussion:** The Hacker News community's response was a mix of encouragement, technical feedback, and philosophical debate. The project was praised for its values, but also faced significant technical hurdles and questions about its long-term viability.

Key discussion points included:

*   **Technical Issues and Feedback:** Several users immediately encountered a `JSON.parse` error, which the creator quickly traced to exhausted API credits due to unexpectedly high traffic. Other feedback included a request for better handling of quoted search strings and suggestions for implementing more advanced DDG bang features (like in-query bangs and bangs that redirect to the first result).

*   **Sustainability and Business Model:** A major theme was the project's financial model. One user expressed concern that using the Brave API could become costly and suggested partnering with Ecosia. However, the creator and other users noted that Ecosia's ad-based model might create a conflict of interest. A broader debate emerged about the role of profit in free-software projects, with some arguing that sustainable profitability is essential for long-term survival, while others cautioned against ad-based models due to their corrosive effects on privacy and user experience.

*   **Future Ambitions and Technical Reality:** Users acknowledged that the project is currently a "thin wrapper" over Brave Search. The creator expressed a desire to build a full, independent search index and ranking algorithm in the future, but recognized the immense resource requirements. This led to a discussion on the challenges of open-source ranking algorithms, with one user arguing they are inherently vulnerable to spam.

*   **Alternatives and Comparisons:** The project was compared to SearxNG, with the creator positioning nilch as a more minimalist and opinionated alternative. Another user suggested the MarginaliaSearch project as a potential path for building a real, independent search index.

---

## [Libgodc: Write Go Programs for Sega Dreamcast](https://github.com/drpaneas/libgodc)
**Score:** 173 | **Comments:** 41 | **ID:** 46420672

> **Article:** The article introduces "libgodc," a custom Go runtime for the Sega Dreamcast console. The project allows developers to write programs and games in Go, leveraging features like goroutines, channels, and garbage collection on the 1999 hardware. It uses `gccgo` for compilation and integrates with the KallistiOS homebrew SDK. The project includes examples (Pong, Breakout, Platformer), documentation, and a video tutorial, targeting developers interested in the challenge of constrained hardware programming.
>
> **Discussion:** The Hacker News community reacted with strong enthusiasm, primarily praising the project's exceptionally high-quality documentation, which many found superior to corporate standards. The discussion highlighted the impressive technical feat of running a modern, managed language like Go on the Dreamcast's limited hardware (16MB RAM, 200MHz CPU), with users nostalgically noting the console's historical power. Technical questions arose regarding the use of `gccgo`, specifically its support for modern Go features like generics. The conversation also humorously diverged into comparing the Dreamcast's efficiency to the resource usage of modern applications like Microsoft Teams. Additionally, a tangential but relevant discussion emerged about compiling Go to WebAssembly (WASI) for other niche or sandboxed environments.

---

## [Swapping SIM cards used to be easy, and then came eSIM](https://arstechnica.com/gadgets/2025/12/i-switched-to-esim-in-2025-and-i-am-full-of-regret/)
**Score:** 170 | **Comments:** 176 | **ID:** 46421653

> **Article:** The article on Ars Technica details the author's deeply frustrating experience with eSIM technology in 2025. Despite the promise of convenience, the author found that eSIMs create significant hurdles, particularly in non-standard situations. Key problems included the inability to easily transfer an eSIM to a new device without an active internet connection or a working old phone, the difficulty of porting a number to a new carrier that required a physical SIM for the transfer, and the general loss of user control. The author argues that eSIMs, while technologically elegant, ultimately serve the interests of carriers and manufacturers by locking users into their ecosystems and making it harder to switch devices or services, lamenting the loss of the simple, user-empowering act of swapping a physical card.
>
> **Discussion:** Discussion unavailable.

---

