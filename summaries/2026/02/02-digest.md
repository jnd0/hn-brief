# HN Daily Digest - 2026-02-02

Notepad++ got pwned—not by script kiddies, but by what the maintainer calls state-sponsored actors who hijacked its update mechanism to push malicious payloads, primarily targeting users in Asia. The root cause? A self-signed certificate with its private key sitting in plaintext in the project’s public GitHub repo. Let that sink in: one of the most widely used text editors on Windows was left open to supply chain compromise because someone thought it was fine to check a signing key into version control. The dev claims it’s fixed now with proper code signing and infrastructure hardening, but the damage is done. This isn’t just a failure of crypto hygiene; it’s a symptom of a broader crisis in open-source sustainability. We rely on tiny teams, often solo devs, to maintain critical infrastructure with zero budget, no security audits, and no ops support—then act shocked when it breaks. And yet, the HN thread went full culture war, with half the comments debating whether Notepad++ should’ve embedded support for Taiwan and Ukraine in its update prompts. Newsflash: when your tool becomes a geopolitical billboard, it becomes a target. Whether you think that’s justified or not, the security model didn’t account for it—and now users are paying the price.

That breach ties into a deeper theme bubbling up today: trust in software is fraying at every level. The Moltbot RCE exploit—where a single click can give an attacker full access to your data and API keys—wasn’t some obscure edge case. It’s the logical conclusion of building AI agents with god-mode privileges by default, no sandboxing, and zero principle of least privilege. It’s “move fast and break things” reborn as “move fast and backdoor your entire stack.” And yes, some hobbyist will spin up a bot that scrapes their Gmail, connects to their GitHub, and then visits a malicious link that turns it into a data exfiltration drone. The scary part isn’t that it’s possible—it’s that the ecosystem is encouraging it. Tools like NanoClaw, a 500-line TypeScript “Clawdbot” implementation using Claude and Apple’s container isolation, are being celebrated as clever hacks, but they’re really just demos waiting to become attack vectors. The creator openly admits using AI to write the code and docs, which sparked its own mini-backlash about authenticity and craftsmanship. But the real question isn’t whether the README was written by a human—it’s whether we’re sleepwalking into a world where every AI agent is a potential trojan horse, and the only defense is “don’t run sketchy code.” Good luck selling that to your CISO.

Meanwhile, Apple’s own stack is looking increasingly rickety. One dev documented how their brand-new iPhone 16 Pro Max produces garbage when running MLX-based LLMs, while other devices handle the same models flawlessly. The issue persists across both third-party and Apple’s own AI frameworks, suggesting a hardware-level defect in the neural engine or memory subsystem. Either Apple shipped a silicon flaw, or the QA process for on-device AI is nonexistent. Neither option inspires confidence. And then there’s Time Machine—again—broken in macOS Tahoe for network backups over SMB. Not “needs tweaking,” not “edge case,” but outright nonfunctional for many NAS setups. The pattern is clear: Apple’s deprecating features not by announcement, but by neglect. Local Time Machine backups still work, sure, but the moment you add a network layer, it’s a minefield of config tweaks and undocumented changes. This isn’t an accident. It’s a push toward iCloud subscriptions, quietly eroding self-hosted and local solutions. Combine that with the App Store’s 30%–36% cuts, App Review delays, and DSA compliance chaos in Europe, and you’ve got a platform increasingly hostile to developers and power users alike. The 1976 Apple I ad—$666.66, free software, open hardware—reads like satire now. The company once sold liberation; today it sells walled gardens with premium maintenance fees.

On the flip side, there’s a countercurrent of people trying to claw back control, often through retro or niche tools. Adventure Game Studio, an open-source engine from the late ’90s, is still alive, still used, and still beloved for letting people make point-and-click adventures without needing a Unity license or a DevOps team. But even there, the frustration is palpable: no native macOS editor, reliance on Wine, outdated tooling. It’s maintained, but barely. Same with Amiga Unix—Amix—a forgotten SVR4 port that failed because it ignored the Amiga’s unique hardware and cost a fortune. It’s a museum piece now, but the discussion reveals a longing for systems that were open, extensible, and not locked down by corporate policy. FOSDEM 2026’s Day 1 recap tapped into that spirit: a celebration of libre software, self-hosted AI, and analog computing. But even there, the debate turned political—because it always does. Can open-source ever be “neutral”? Or is the act of sharing code without gatekeepers inherently a political stance? Many argued that FOSS used to be a refuge from identity politics, a place where your code spoke for itself. But others pointed out that neutrality often means silence—and silence benefits those already in power. The truth is, these spaces were never apolitical. They just excluded certain voices until they couldn’t anymore.

Then there are the hacks that feel like resistance. Xikipedia.org turns Wikipedia into a doomscroll feed, client-side, no tracking, all privacy preserved. It’s TikTok for nerds, and yes, it loads 40MB upfront and crashes on mobile—but the idea is brilliant. Knowledge as infinite scroll, driven by curiosity, not engagement metrics. Contrast that with English professors forcing printed readings to fight AI summarization. It’s a well-intentioned gesture, but it’s also futile. Students will OCR the pages or screenshot them into GPT. The real issue isn’t the medium—it’s that education hasn’t adapted to a world where information synthesis is trivial. The value isn’t in regurgitation; it’s in critique, in creation, in knowing what questions to ask. One CS professor got it right: use AI as a tutor, but test understanding with handwritten code on paper. That’s the balance—augmentation without abdication.

And in the middle of all this, a shoelace site. Ian’s Shoelace Site. 160 points for teaching people how to tie their shoes properly. The irony isn’t lost: we’re building AI agents and cracking 40-year-old dongles, but half of us are still tying granny knots that come undone by lunch. The Ian Knot—two loops, one pull, done—is faster and more secure. But people resist it. Too different. Too hard to learn. Sound familiar? We cling to broken systems because the alternative requires relearning. That’s the story of modern tech: we’re all using legacy mental models, duct-taped workflows, and insecure defaults because the path of least resistance is paved with technical debt.

The 40-year-old dongle crack was a nostalgia trip—constant return values over parallel ports—but also a warning. DRM never worked. It just punished legitimate users. And yet, we’re seeing a resurgence in licensing paranoia, especially in B2B software, where dongles persist because customers want “perpetual” licenses. But perpetual doesn’t mean free—it means no recurring revenue. So vendors lock the software to hardware, only to face the logistical nightmare of lost keys and dead USB ports. The cycle repeats: control begets friction, friction begets workarounds, workarounds beget obsolescence.

At the dark end of the spectrum, the Wired exposé on scam compounds in Laos is a gut punch. Thousands enslaved, forced to run romance scams and crypto fraud under threat of violence. It’s modern slavery with a tech stack—CRM-like dashboards, performance metrics, even HR departments. And it thrives because the incentives are misaligned: victims are trafficked across borders, profits laundered through shell companies, and enforcement is patchy at best. Some commenters tried to spin it as a “failure of communism,” but the real failure is global capitalism’s blind spot for human cost. This isn’t some aberration—it’s the logical extreme of treating labor as disposable.

And then, a sliver of hope: exercise can make your brain look younger. MRI scans show a nearly one-year reduction in brain age after 12 months of consistent aerobic activity. It’s not flashy, not viral, but it’s real. The irony? The same people optimizing their dotfiles and tweaking their AI agents won’t go for a 30-minute walk. We’ll build tools to automate everything except our own well-being.

Worth watching: the backlash against AI-generated code and docs isn’t just about quality—it’s about accountability. When a system fails, who do you blame? The developer? The model? The training data? We’re entering an era where software has no clear author, and that’s going to break incident response, liability, and trust in ways we’re not ready for.

---

*This digest summarizes the top 20 stories from Hacker News.*