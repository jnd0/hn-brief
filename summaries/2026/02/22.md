# Hacker News Summary - 2026-02-22

## [How I use Claude Code: Separation of planning and execution](https://boristane.com/blog/how-i-use-claude-code/)
**Score:** 716 | **Comments:** 454 | **ID:** 47106686

> **Article:** Thearticle details the author's workflow using Claude Code to separate planning and execution, emphasizing the need for detailed prompts to prevent the model from skimming and to ensure surface-level reading is unacceptable. A key claim is that structuring prompts with specific instructions forces the model to engage deeply with code, as demonstrated by the author's method of writing extensive plans and specifications before implementation. The author positions this approach as a structured way to leverage AI tools effectively, contrasting it with more ad-hoc methods.
>
> **Discussion:** The discussion centers on the effectiveness and practicality of the author's detailed prompting approach for Claude Code. Key themes include the debate over whether this method represents a revolutionary insight or simply an evolved practice among experienced developers, with some users (like chaboud and keyle) arguing it's a natural skill, while others (like FuckButtons) dismiss it as superstition. Technical insights emerge around the attention mechanism and the model's reliance on its vast training data, with nostrademons explaining how specific phrasing weights relevant corpus examples. Disagreements arise regarding efficiency: jamesmcq and streetfighter64 question the time savings for experienced developers, citing the complexity of large codebases, while shepherdjerred and keyle counter with personal experiences showing significant time reductions. The community also debates planning strategies, with mvkel advocating for batch planning to avoid starting over, and bonoboTP and Bishonen88 sharing contrasting experiences with smaller, iterative approaches versus larger one-shot plans. The discussion highlights a divide between those finding AI-assisted planning transformative and those skeptical of its reliability and efficiency gains.

---

## [What not to write on your security clearance form (1988)](https://milk.com/wall-o-shame/security_clearance.html)
**Score:** 465 | **Comments:** 207 | **ID:** 47102576

> **Article:** The 1988 article “What not to write on your security clearance form” recounts a notorious incident where a security officer tore up a submitted SF86 after the applicant disclosed a childhood joke about being a Japanese spy, then handed back a blank form with a warning not to mention it again. It argues that the official guidance often encourages applicants to omit or falsify information, effectively making lying a de‑facto requirement despite the felony risk. The piece also lists common pitfalls such as admitting past drug use, financial troubles, or foreign contacts, suggesting that many candidates strategically “pick the closest bin” to avoid disqualification. Overall, it paints the clearance process as a bureaucratic maze where the form’s limited categories force applicants into selective honesty.
>
> **Discussion:** Commenters reacted with disbelief that security officials would explicitly tell applicants to lie, noting the legal and ethical contradictions. Several users shared personal experiences, describing how investigators sometimes tolerate past missteps if they are disclosed and no longer ongoing, while others emphasized that even minor suspicions of marijuana use trigger intense scrutiny compared to larger financial issues. The thread also explored broader themes such as Goodhart’s Law, the prevalence of functional alcoholism in the military, and the paradox of a system that incentivizes concealment, thereby creating new blackmail risks. Side remarks about the milk.com domain and its quirky server header added a light‑hearted diversion, but the consensus remained that the clearance process is both inefficient and prone to encouraging dishonest reporting.

---

## [Why is Claude an Electron app?](https://www.dbreunig.com/2026/02/21/why-is-claude-an-electron-app.html)
**Score:** 392 | **Comments:** 402 | **ID:** 47104973

> **Article:** The article explains that Claude uses Electron because some engineers preferred it for non-native building, allowing code sharing between web and desktop interfaces. Boris from the Claude Code team notes this was a trade-off, as Electron enables consistent features across platforms despite potential performance drawbacks. The post highlights engineering trade-offs involved in the choice, suggesting the stack might change in the future.
>
> **Discussion:** The discussion centers on Claude's use of Electron, with users complaining about UI jank and CPU usage during output streaming, while defenders argue these issues could be resolved through performance engineering rather than switching stacks. Debates emerge about AI's role in coding, with some users like yodsanklai arguing AI doesn't eliminate bugs or reduce system complexity, while others like Dig1t counter that diligent use with code reviews maintains control. Technical arguments compare Electron's resource usage to other web-based apps like Gmail and VSCode, with some users defending Electron's utility for cross-platform development despite performance criticisms, while others highlight specific integration challenges with macOS and resource constraints on low-end machines. The thread also touches on broader themes of AI's effectiveness in coding and the trade-offs between development speed and system understanding.

---

## [Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU](https://github.com/xaskasdf/ntransformer)
**Score:** 300 | **Comments:** 80 | **ID:** 47104667

> **Project:** The project demonstrates running Llama 3.1 70B on a single RTX 3090 by bypassing the CPU to directly transfer data from NVMe storage to the GPU using DMA, reducing latency. The author highlights achieving ~3000 tokens per second on a PS2 with classic transformers, leveraging its 32-bit addressable VRAM, and notes that the approach avoids CPU bottlenecks by streaming layers directly to the GPU. The project emphasizes hardware-level optimizations for large language models.
>
> **Discussion:** The discussion centers on the project's technical feasibility and practicality. Users debate whether 0.2 tokens per second is sufficient for interactive use or better suited for batch processing, with some noting that DDR4/DDR5 memory bandwidth and PCIe data rates limit performance. The idea of using NVMe as extended VRAM via DMA is praised for its innovation, though concerns about energy costs and cost-effectiveness arise. Technical insights include comparisons to Apple's unified memory, MoE model optimizations, and the potential for multi-tier MoE architectures. Some users question the project's real-world utility, while others express interest in its implications for low-resource environments. The thread also touches on hardware modifications, such as patched NVIDIA drivers, and the trade-offs between local inference and cloud-based solutions.

---

## [How Taalas “prints” LLM onto a chip?](https://www.anuragk.com/blog/posts/Taalas.html)
**Score:** 270 | **Comments:** 132 | **ID:** 47103661

> **Article:** The article analyzes Taalas's approach to embedding large language models directly onto specialized chips. It explores how Taalas claims to achieve remarkable efficiency by hardcoding model weights into hardware, potentially offering significant speed and power advantages over traditional GPU inference. The piece examines the technical feasibility and implications of this approach, including questions about model updates and the trade-offs between hardware specialization and flexibility.
>
> **Discussion:** The Hacker News discussion explores the implications of Taalas's approach to hardware-accelerated LLMs, with participants debating whether this represents the future of AI deployment. Some commenters draw parallels to historical approaches like CD-ROM cartridges and Nintendo DS cartridges, suggesting a model where users physically swap chips for different models. Others question the business viability given how quickly AI models evolve, with one commenter noting that new models emerge every two weeks while custom chips take months to produce. Technical details spark debate, including speculation about how Taalas achieves single-transistor multiplication and whether this involves analog or digital techniques. The discussion also touches on potential applications like local AI on phones and laptops, with some arguing that once AI output quality is "good enough," other factors like privacy and cost become more important than having the latest model. There's significant interest in whether this approach could work for specialized applications like TTS, speech recognition, or robotics where latency constraints make general-purpose GPUs less suitable.

---

## [Personal Statement of a CIA Analyst](https://antipolygraph.org/statements/statement-038.shtml)
**Score:** 226 | **Comments:** 156 | **ID:** 47102975

> **Article:** The articledetails a CIA analyst's negative experiences with polygraph testing during the hiring process, describing it as psychological torture rather than a scientific assessment. The author argues the polygraph serves three purposes: weeding out undesirable candidates, exerting power over applicants, and identifying potential sociopaths. Specific details include the use of an extremely tight blood pressure cuff left on for 8 hours, a calibration test involving lying about a chosen number, and interrogation tactics designed to force confessions about embarrassing past actions. The author also references the anti-polygraph website antipolygraph.org and its owner George Maschke, who faced issues with the CIA.
>
> **Discussion:** The discussion revolves around the purpose, effectiveness, and ethics of CIA polygraph testing. Commenters debate whether the polygraph is a necessary security tool or a form of psychological torture. Technical insights emerge about the test's mechanics, including the use of a tight blood pressure cuff and calibration tests designed to establish a "baseline of truth" through lies. Personal anecdotes from users like Arainach (NSA experience) and snickerbockers (CIA examiner knowledge) add weight to the critique. Themes of power dynamics, the targeting of "uncontrollable" honest individuals, and the potential for blackmail are prominent. While some agree with the author's harsh assessment, others defend the process as a necessary evil for national security, citing the need to identify vulnerabilities. Humor and sarcasm appear in comments about crying during tests and the comparison to children's fantasy literature, contrasting with more serious reflections on the system's brokenness and the "battered housewife syndrome" of government employees.

---

## [Parse, Don't Validate and Type-Driven Design in Rust](https://www.harudagondi.space/blog/parse-dont-validate-and-type-driven-design-in-rust/)
**Score:** 220 | **Comments:** 59 | **ID:** 47103931

> **Article:** The article argues that programs should parse untrusted input into types that are correct by construction rather than merely validating data after the fact, using Rust’s type system to encode invariants such as non‑empty collections or range‑restricted integers. It contrasts this “parse‑don’t‑validate” approach with traditional validation, citing examples like wrapping a nonzero integer in a newtype and the non‑empty Vec pattern (T, Vec<T>), and references Alexis King’s follow‑up piece “Names are not type safety.” The author notes that while dependent types can provide full correctness guarantees, practical implementations often rely on lightweight wrappers or pattern‑based constraints. The piece also acknowledges limitations, such as the difficulty of encoding constraints that involve relationships between multiple values.
>
> **Discussion:** Commenters debate the trade‑off between encoding invariants in types versus keeping validation logic in functions, with some praising the clarity of newtype wrappers while others warn about the practicality of full dependent typing. Several participants highlight the tension between the Perlis maxim of many functions on one data structure and the desire to avoid numerous similar types, noting that Rust’s pattern types or macro‑based encodings offer a middle ground. The conversation touches on real‑world concerns like parsing from stdin, where runtime checks must be performed before a type can be constructed, and whether the approach truly prevents errors or just shifts them. Some users compare the idea to object‑oriented encapsulation and question its novelty, while others point out that certain examples, such as handling a negative discriminant, expose the limits of the parse‑only strategy. Overall, the thread reflects enthusiasm for the concept alongside skepticism about its universal applicability and calls for more concrete language support.

---

## [EDuke32 – Duke Nukem 3D (Open-Source)](https://www.eduke32.com/)
**Score:** 196 | **Comments:** 71 | **ID:** 47104185

> **Article:** EDuke32 is an open-source port of Duke Nukem 3D, allowing players to experience the classic first-person shooter on modern systems. The port includes enhancements like improved rendering, support for high resolutions, and compatibility with custom mods. The project has an active community that continues to develop and maintain the engine, ensuring the game remains playable decades after its original release.
>
> **Discussion:** The discussion centered around nostalgia for classic FPS games, with many users sharing fond memories of Duke Nukem 3D's multiplayer and modding capabilities. Several commenters praised the game's level design and weapon variety, particularly the creative ways to defeat enemies. The conversation also touched on accessibility, with one blind user sharing how they used AI to make Duke Nukem 3D playable through audio cues. Other users recommended various mods and source ports for Duke Nukem 3D and similar classic games, highlighting the enduring popularity of these titles in the gaming community. The thread revealed a strong appreciation for the tactile feel and level of detail in 90s shooters compared to modern games.

---

## [Cloudflare outage on February 20, 2026](https://blog.cloudflare.com/cloudflare-outage-february-20-2026/)
**Score:** 180 | **Comments:** 118 | **ID:** 47103649

> **Article:** The article discusses a recent Cloudflare outage and the community's reaction to it. Users express concern over increasing network disruptions and criticize leadership changes, highlighting issues like poor API design and a shift in focus toward AI at the expense of reliability. Technical debates center on API behavior, data handling, and the need for better testing and communication. Several contributors warn of erosion of trust and emphasize the importance of robust system design and transparency.
>
> **Discussion:** Multiple voices weighed in on the outage, with some calling for immediate fixes and others questioning the company's direction. Concerns were raised about leadership decisions, API changes, and whether the company is prioritizing user needs over internal pressures. Technical insights focused on API reliability, data management, and the risks of overloading existing endpoints. The conversation underscores a broader frustration with transparency and the challenges of maintaining stability in a complex platform.

---

## [Evidence of the bouba-kiki effect in naïve baby chicks](https://www.science.org/doi/10.1126/science.adq7188)
**Score:** 164 | **Comments:** 53 | **ID:** 47105198

> **Article:** The study analyzed 42 chicks, revealing a link between shape and auditory properties while highlighting ongoing debates.
>
> **Discussion:** Debates persist over whether the findings reflect innate linguistic structures or alternative explanations. Technical insights suggest complex interactions between perception and cognition. Diverse perspectives highlight both agreement and skepticism about the study's conclusions. Technical nuances and methodological concerns remain central to understanding the results. Community reactions range from support to caution regarding implications for language theory.

---

## [Toyota’s hydrogen-powered Mirai has experienced rapid depreciation](https://carbuzz.com/toyota-mirai-massive-depreciation-one-year/)
**Score:** 161 | **Comments:** 397 | **ID:** 47103136

> **Article:** The article notes that the Toyota Mirai, the only hydrogen fuel‑cell sedan sold in the United States, has lost roughly half its value within its first year on the market, dropping from an MSRP of about $58,000 to an average trade‑in price of $28,000 according to CarBuzz data. It attributes the steep depreciation to low consumer demand, the scarcity of hydrogen refueling stations, and the high cost of hydrogen fuel relative to gasoline. The piece also highlights Toyota’s continued investment in the Mirai despite the market signals, suggesting that the company is betting on future infrastructure growth to justify the vehicle’s price.
>
> **Discussion:** Commenters dissected the Mirai’s rapid depreciation as a symptom of broader hydrogen market challenges, with many arguing that the well‑to‑wheel inefficiency of producing hydrogen—especially via steam‑methane reforming—makes it an unattractive consumer fuel. Others countered that hydrogen’s high energy density and light weight could still be valuable for aviation, where batteries remain too heavy, and for long‑distance renewable energy transport, noting that liquid fuels currently offer ten times the energy density of batteries. The thread also revealed a split over infrastructure: skeptics pointed out the logistical nightmare of building a hydrogen distribution network, citing leakage, embrittlement, and the need for specialized tanks and pumps, while proponents suggested that oil majors might use existing natural‑gas pipelines to pivot to hydrogen production. Community sentiment ranged from dismissive of hydrogen as a dead end to cautious optimism about niche applications, with several users emphasizing that any hydrogen rollout would divert resources from the already‑proven electric‑vehicle ecosystem. The debate underscored a recurring theme that hydrogen’s viability hinges on solving production, storage, and distribution problems before it can compete with battery‑electric cars.

---

## [Japanese Woodblock Print Search](https://ukiyo-e.org/)
**Score:** 151 | **Comments:** 24 | **ID:** 47107781

> **Article:** Ukiyo-e.org is a comprehensive online database for Japanese woodblock prints, created by John Resig. The site uses computer vision analysis to cluster related prints across various museums and universities. Resig mentions he's working on a new site focused on prints for sale from dealers and auctions, which will be more technically complex and require continuous updates.
>
> **Discussion:** The Hacker News community responded enthusiastically to the Ukiyo-e.org site, with many users praising Resig's work and sharing their personal connections to Japanese woodblock prints. Several commenters discussed their favorite artists, with Kawase Hasui, Shiro Kasamatsu, and Tom Killion mentioned as standouts. The thread also featured recommendations for additional resources, including a Twitch stream of the printmaking process, Dave Bull's YouTube channel, and a museum in Kurashiki. Some users shared personal stories about owning prints or secret boxes featuring woodblock art. Resig engaged with the community, confirming his identity and providing updates on his upcoming auction site, including screenshots posted on Reddit. One user expressed frustration with the site's user experience, calling it a "shitty way to present art."

---

## [Attention Media ≠ Social Networks](https://susam.net/attention-media-vs-social-networks.html)
**Score:** 150 | **Comments:** 59 | **ID:** 47110515

> **Article:** The article argues that platforms like Facebook and Instagram have fundamentally shifted from being "social networks"—tools for connecting with friends—to "attention media," which are designed to maximize user engagement through algorithmic feeds. It claims this transition is driven by advertising-based business models that prioritize keeping users scrolling by injecting viral content and ads, often at the expense of genuine social interaction. A key detail is the replacement of chronological feeds with algorithmic ones that surface content from strangers rather than followed accounts. The author emphasizes that recognizing this distinction is essential for addressing the platforms' negative societal and psychological impacts.
>
> **Discussion:** Users overwhelmingly lament the degradation of platforms like Facebook and Instagram, where algorithmic feeds have replaced content from friends with random strangers, influencers, and ads, making the services feel unusable for their original purpose. A central disagreement emerges over responsibility: some commenters assert that users themselves initially sought fame and validation, prompting platforms to cater to those behaviors, while others counter that companies deliberately exploit psychological vulnerabilities with sophisticated, addictive design. Alternatives such as Mastodon are examined but criticized for inheriting harmful habits and facing inherent competitive disadvantages, as addictive formats naturally outcompete healthier models. Several participants analogize social media to alcohol, arguing its core design is harmful and that preserving "good parts" is akin to an alcoholic justifying continued drinking. The discussion also highlights technical frustrations, such as the inability to opt out of algorithmic recommendations for others, and the pervasive issue of "explore" sections that turn any platform into a time sink.

---

## [A Botnet Accidentally Destroyed I2P](https://www.sambent.com/a-botnet-accidentally-destroyed-i2p-the-full-story/)
**Score:** 134 | **Comments:** 82 | **ID:** 47106985

> **Article:** An investigation into a botnet attack that inadvertently damaged I2P highlights concerns over botnet evasion tactics and the resilience of anonymity networks. The incident raised questions about the difficulty of banning botnet owners on services like Discord and the broader implications for cybersecurity infrastructure. Experts debated whether the attack exploited protocol weaknesses or reflected intentional evasion strategies, while some noted the stress test it placed on I2P’s decentralized design. The conversation underscored the challenges of balancing security, privacy, and law enforcement oversight in digital communications.
>
> **Discussion:** The debate centered on the botnet’s ability to bypass bans and the protocol’s capacity to handle large-scale attacks, with users questioning the effectiveness of current cybersecurity measures. Discussions highlighted the complexity of distinguishing malicious actors from legitimate users and the potential for state-sponsored operations. Technical concerns about protocol resilience and the need for better defenses against such threats dominated the conversation, reflecting broader anxieties about digital infrastructure security. Community members emphasized the importance of understanding both the technical and policy dimensions of these incidents.

---

## [Back to FreeBSD: Part 1](https://hypha.pub/back-to-freebsd-part-1)
**Score:** 125 | **Comments:** 50 | **ID:** 47108989

> **Article:** The article "Back to FreeBSD: Part 1" discusses the author's return to FreeBSD after using Linux containers, comparing FreeBSD jails to Docker containers. The article appears to examine why Docker became dominant despite FreeBSD jails having technically solid isolation capabilities years earlier. It likely explores FreeBSD's strengths in stability, security, and coherent system design, while acknowledging the ecosystem advantages that made Docker popular for containerization.
>
> **Discussion:** The discussion centered on why Docker won the containerization war despite FreeBSD jails having technically superior isolation technology earlier. Many argued Docker's success came from its ecosystem—Dockerfiles, public registry, and compose tools—rather than the underlying technology. Some FreeBSD users praised its stability and found bugs that Linux missed, while others acknowledged Linux's market dominance and investment make its warts tolerable. Technical debates emerged about whether containers are fundamentally different from jails, with some noting containers are just an illusion created through Linux kernel primitives like namespaces and cgroups. There was also discussion about whether FreeBSD needs equivalent tools to Docker's ecosystem to remain competitive, despite having only 0.1% market share.

---

## [Canvas_ity: A tiny, single-header <canvas>-like 2D rasterizer for C++](https://github.com/a-e-k/canvas_ity)
**Score:** 112 | **Comments:** 38 | **ID:** 47103506

> **Article:** Canvas_ity is a tiny, single-header C++ library that provides a canvas-like 2D rasterizer. The project includes an extensive list of recommended reading materials that the author consulted during development, covering topics from vector mathematics and font specifications to image processing and color spaces. The library implements features like arc drawing, stroke expansion, Bezier curve handling, and various compositing operations, with a focus on high-quality rendering techniques.
>
> **Discussion:** The discussion centered on the technical aspects of the Canvas_ity library and C++ development practices. One user highlighted the perp dot product concept, explaining its mathematical properties and relation to cross products. A debate emerged about the header-only implementation approach, with some developers defending it as a common C++ pattern due to the lack of standard library packaging, while others suggested it reflects an unwillingness to learn proper build systems. The author clarified that no AI-generated code was used in the library, though security hardening hasn't been a priority. Throughout the conversation, participants shared insights about mathematical foundations, rendering techniques, and the practical challenges of C++ library development.

---

## [Inputlag.science – Repository of knowledge about input lag in gaming](https://inputlag.science)
**Score:** 99 | **Comments:** 32 | **ID:** 47103945

> **Article:** Inputlag.science is a new repository dedicated to understanding and documenting input lag in gaming. The site covers the entire input chain from controller to display, including often-overlooked factors like game engine processing. It provides detailed technical information about how different components contribute to overall latency, with specific focus on the engine's role in adding latency beyond just frame rate.
>
> **Discussion:** The Hacker News community shared numerous personal experiences with input lag, highlighting how subtle changes in hardware or software can dramatically impact gaming performance. Several users described switching between different refresh rates or mice and noticing significant differences in responsiveness, with one user winning nearly every round of Fortnite after correcting their refresh rate setting. The discussion also delved into technical aspects like triple buffering implementations, with contributors explaining how different buffering modes affect latency in various ways. Users shared practical measurement techniques, including using smartphone slow-motion cameras to measure latency down to 4ms granularity, and discussed how compositors and operating systems can introduce additional latency. The conversation revealed a gap between academic research on input latency and practical gaming knowledge, with some users noting that modern high-polling-rate devices have made input lag less of an issue for most gamers.

---

## [Password managers less secure than promised](https://ethz.ch/en/news-and-events/eth-news/news/2026/02/password-managers-less-secure-than-promised.html)
**Score:** 76 | **Comments:** 73 | **ID:** 47105052

> **Article:** Password managers like KeePass, 1Password, and Bitwarden are under scrutiny for security flaws that contradict their marketing promises. Experts highlight concerns around data exposure, recovery processes, and the risks of storing sensitive information offline or online. While some users advocate for offline-only solutions or layered backups, others stress the importance of robust encryption and awareness of potential attack vectors. The community is divided on whether current safeguards are sufficient or if deeper vulnerabilities exist, especially regarding key management and organizational trust models.
>
> **Discussion:** Security experts are questioning the claims made by password managers about their safety, pointing to potential gaps in their encryption and recovery methods. Concerns center on how data is protected during transfers, stored offline, and managed in case of disasters. There is a clear divide between those who favor offline vaults or specialized hardware like fireproof safes and those who argue for stronger transparency and improved technical defenses. The conversation underscores the need for better user education and clearer accountability from service providers.

---

## [Palantir's secret weapon isn't AI – it's Ontology. An open-source deep dive](https://github.com/Leading-AI-IO/palantir-ontology-strategy)
**Score:** 76 | **Comments:** 47 | **ID:** 47107512

> **Article:** The linked GitHub article argues that Palantir's competitive advantage stems from its sophisticated use of ontology—a structured framework for defining entities and relationships—rather than AI alone. Commenters largely dispute this, with the top comment asserting Palantir's backend technology is unremarkable ("mid," 3/10) and its true differentiators are a user-friendly front-end for non-technical government users, dedicated forward-deployed engineers, and a willingness to work on ethically fraught government contracts. Another commenter notes their "flexible" ontological model acts as a quick-adaptation glue for diverse government contracts, deepening their moat through integration. The article itself is criticized by some as AI-generated hype that misrepresents standard database concepts like views and user-defined functions as revolutionary ontology.
>
> **Discussion:** The discussion pivots on a fundamental disagreement about Palantir's actual technology and value. A primary theme is the technical assessment: many engineers dismiss the "ontology" claim as merely repackaging established database concepts (views, UDFs, graph relationships) with fancy terminology, arguing their stack is not architecturally innovative. A competing view, however, credits Palantir's success to non-technical strengths: an exceptionally accessible UI for tech-averse users (like police), intense customer support via embedded engineers, and deep, lucrative relationships with the DoD that allow rapid, "glued-in" deployments. This technical debate is inseparable from intense ethical scrutiny; several commenters frame Palantir as a powerful, legitimized tool for government surveillance and data manipulation, comparing it to Cambridge Analytica at scale and warning of an asymmetric warfare where the public is ill-equipped. The conversation also includes a factual correction regarding an unsupported claim about police IQ tests and a tangential debate on whether individual technological self-defense or legislative action (like GDPR) is the viable recourse against such systems.

---

## [DialUp95 – A 90s inspired nostalgia hit](https://dialup95.com/)
**Score:** 69 | **Comments:** 45 | **ID:** 47102763

> **Article:** ...
>
> **Discussion:** Discussion unavailable.

---

