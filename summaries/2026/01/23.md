# Hacker News Summary - 2026-01-23

## [We will ban you and ridicule you in public if you waste our time on crap reports](https://curl.se/.well-known/security.txt)
**Score:** 898 | **Comments:** 583 | **ID:** 46717556

> **Article:** The article links to cURL's security.txt file, which contains a blunt and public policy statement warning that individuals who submit low-quality or AI-generated "slop" bug reports will be banned and ridiculed. This policy is a response to the project being flooded with AI-generated error reports, which prompted the decision to remove monetary bug bounties to reduce the financial incentive for such submissions.
>
> **Discussion:** The Hacker News discussion largely validates cURL's frustration, acknowledging that the rise of AI has exacerbated a pre-existing problem of low-effort contributions to open-source projects. Many commenters shared personal anecdotes of maintainers being overwhelmed by nonsensical issues and pull requests, often generated by students or individuals seeking to game systems like Hacktoberfest or academic requirements.

The conversation split into two main themes regarding solutions. One side supported cURL's aggressive stance, viewing it as a necessary, if harsh, deterrent. The other side argued that rudeness is ineffective because AI operators do not feel personal shame and simply feed the responses back into their models. Instead, they proposed structural changes to reduce noise, such as moving discussions to niche platforms or requiring a "discussion first" gatekeeping step before issues can be opened. Ultimately, the consensus was that the ease of generating code with LLMs has created a crisis where the cost of review scales sublinearly compared to the infinite volume of AI slop, forcing maintainers to adopt stricter, sometimes abrasive, measures to protect their time and attention.

---

## [Show HN: isometric.nyc – giant isometric pixel art map of NYC](https://cannoneyed.com/isometric-nyc/)
**Score:** 859 | **Comments:** 173 | **ID:** 46721802

> **Project:** The project is an interactive isometric pixel art map of New York City, created by a developer using generative AI (specifically Gemini CLI and Cursor) to automate the generation of the massive amount of art required. The author argues that while AI makes "commodity content" easy to produce, it allows creators to focus on the "love" and unique vision behind a project, unlocking scales of work previously impossible for an individual. The site was briefly hugged to death but is back online.
>
> **Discussion:** The discussion centers on the philosophical implications of using AI in creative projects, specifically the tension between technological capability and artistic value. The creator engaged in a debate regarding the "scale" of AI output, acknowledging the valid concerns about diminished human value and the obsolescence of traditional crafts, while maintaining optimism about the new domains AI unlocks.

Other commenters appreciated the technical execution and the write-up, with several requesting similar maps for other cities like San Francisco and Tokyo. There were also minor technical reports of CORS and rate-limiting errors on the site, which the creator addressed. A notable counterpoint was raised regarding the necessity of AI, with users citing examples of massive manual projects like Minecraft 1:1 builds and miniature sculptures, suggesting that such work is possible without generative models, albeit with different labor trade-offs.

---

## [GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers](https://gptzero.me/news/neurips/)
**Score:** 813 | **Comments:** 430 | **ID:** 46720395

> **Article:** The article from GPTZero, an AI detection service, claims to have identified 100 instances of AI-generated "hallucinations" (fabricated facts, data, or references) within papers accepted to the prestigious NeurIPS 2025 AI conference. The report highlights specific examples, such as citations to non-existent authors like "John Doe" and "Jane Smith," suggesting that LLMs were used to generate parts of these academic submissions without proper human verification. The findings imply a significant breach of academic integrity, where AI-generated content containing blatant falsehoods has bypassed the conference's peer-review process.
>
> **Discussion:** The Hacker News discussion expresses significant alarm and cynicism regarding the report's findings, viewing them as a symptom of a broader crisis in scientific integrity. The consensus is that this issue harms scientific research and that using LLMs to fabricate content constitutes fraud.

Key themes in the discussion include:
*   **Erosion of Scientific Trust:** Commenters are deeply concerned that the ease of generating plausible-sounding but false papers will worsen the existing problems of data falsification and p-hacking. There is a strong sentiment that such actions are not science and should be punished severely.
*   **Systemic Failures in Peer Review:** A central point of debate is the failure of the peer-review process. Many were shocked that obvious hallucinations passed review, leading to speculation that reviewers are overworked, lack incentives, and cannot possibly verify every reference. Some commenters noted that it's common for reviewers to trust authors and focus on methodology rather than fact-checking citations, a practice that is now shown to be vulnerable.
*   **Skepticism of AI Detectors:** Several users expressed distrust in GPTZero's methodology, questioning the validity of attributing these errors to AI without a baseline comparison to the rate of human-generated reference errors in pre-2020 papers. There is a concern that such errors have always existed and that AI detectors are unreliable.
*   **Broader Cultural Problems:** The conversation expanded beyond the immediate issue to critique the culture of AI/ML research itself. Some commenters argued that the field was already plagued by "fake it 'til you make it" mentalities, optics, and hype, and that AI slop is merely a new tool exacerbating a pre-existing problem of substance being overlooked in favor of presentation.
*   **Proposed Solutions:** Potential solutions were briefly discussed, such as mandating reproducible code (a "PoC or GTFO" approach) and improving screening processes. However, the feasibility of these solutions was questioned due to the immense scale of the academic publishing system and the lack of resources for thorough verification.

---

## [In Europe, wind and solar overtake fossil fuels](https://e360.yale.edu/digest/europe-wind-solar-fossil-fuels)
**Score:** 577 | **Comments:** 568 | **ID:** 46719491

> **Article:** An article from Yale Environment 360 reports that for the first time, wind and solar power have generated more electricity in Europe than fossil fuels. This milestone was reached in the first half of 2025, driven by a rapid expansion of renewable capacity and a corresponding decline in fossil fuel usage. The shift is attributed to a combination of falling renewable energy costs, supportive EU policies, and the ongoing energy crisis exacerbated by the reduction of Russian gas supplies. The article highlights that this transition is happening faster than many official forecasts predicted, signaling a fundamental change in the continent's energy landscape.
>
> **Discussion:** The Hacker News discussion largely validates the article's significance but expands the context beyond the headline, focusing on geopolitical implications, economic costs, and the practical challenges of a full energy transition.

A major theme is the geopolitical fallout, particularly regarding China and Russia. Commenters argue that Europe's pivot to renewables weakens Russia's economic leverage, potentially making it more dependent on China. Some suggest China benefits from Russia's self-isolation and economic decline, as it gains access to cheap resources. Others believe China has an interest in the Ukraine war continuing to distract the West, setting a precedent for its own territorial ambitions (e.g., Taiwan) while it operates with less scrutiny.

The economic debate is sharp. Several users, particularly from North America and Australia, share anecdotes about the low cost and high return on investment for residential solar, contrasting it with high energy prices in Europe. This leads to a broader argument about European industrial competitiveness. Some commenters blame green policies and high taxes for making EU businesses less competitive compared to the US and China. However, others counter that Europe's high energy costs are also due to a lack of domestic fossil fuel reserves and that energy independence and security are valuable goals that outweigh purely monetary costs.

Finally, the discussion delves into the technical realities of the transition. Users point out that the article's data refers only to electricity generation, not total energy consumption (which includes transport and heating). While this is a crucial milestone, a full transition requires electrifying these sectors. The conversation highlights both challenges and solutions: the intermittency of renewables is a key concern, but commenters note the rapid drop in battery costs is beginning to solve the evening peak-demand problem. For heating, heat pumps and thermal storage are presented as viable solutions, especially since wind power is often strongest in winter when heating demand is highest.

---

## [Qwen3-TTS family is now open sourced: Voice design, clone, and generation](https://qwen.ai/blog?id=qwen3tts-0115)
**Score:** 560 | **Comments:** 175 | **ID:** 46719229

> **Article:** Alibaba's Qwen team has open-sourced the Qwen3-TTS family, a suite of text-to-speech models capable of high-quality voice generation, design, and cloning. The release includes models of varying sizes (from 0.6B to 18B parameters) and supports multiple languages. The official announcement provides audio samples demonstrating the system's capabilities, including cross-lingual voice cloning where a reference voice can speak in a different language.
>
> **Discussion:** The HN community's reaction is a mix of awe at the technology's capability and concern over its implications. Many commenters were genuinely shocked by the quality, with some noting it gave them "chills" and could be a game-changer for projects like restoring old, degraded audio from radio plays. However, the quality was also noted to have a distinct character; multiple users observed that the English samples sounded like "anime voices" or popular YouTubers, suggesting a potential training bias.

The discussion quickly pivoted to the ethical and societal risks. The ease and quality of voice cloning were described as "terrifying," with users highlighting the potential for scams and misinformation. This led to a consensus that digital media can no longer be trusted at face value, with one user stating we must "assume everything behind a screen is fake unless rigorously... proven otherwise."

On the technical side, users explored implementation. A Hugging Face demo was shared for easy testing of voice cloning, which several people tried and confirmed was "uncanny good" and highly accurate. For local execution, users noted challenges with installation (especially on non-NVIDIA hardware like Macs) but shared workarounds and a custom CLI tool. There were also some criticisms regarding the model's performance in Japanese and a general sentiment that the Qwen team should focus more on coding capabilities rather than TTS.

---

## [I was banned from Claude for scaffolding a Claude.md file?](https://hugodaniel.com/posts/claude-code-banned-me/)
**Score:** 494 | **Comments:** 395 | **ID:** 46723384

> **Article:** The author, Hugo Daniel, recounts being permanently banned from using Claude Code after running an automated process to "scaffold" a `CLAUDE.md` file. He was using a script to repeatedly invoke the CLI, generate a configuration file, and then use that file in the next iteration to refine it. His account was disabled without warning or explanation, and his appeal was denied. The author argues this highlights a major flaw in AI services: opaque, automated moderation that prioritizes safety over accuracy, with no meaningful recourse for users. He concludes that this experience pushes users towards self-hosted, open-source alternatives where they have full control.
>
> **Discussion:** The Hacker News community discussion centered on skepticism towards the author's narrative, the broader issue of poor customer support from AI companies, and the viability of alternatives.

Many commenters were confused by the author's description of his workflow, suggesting his "scaffolding" process sounded more like a complex, circular prompt injection or a multi-agent setup that could have legitimately triggered security heuristics. A key point of contention was that the author was merely guessing this was the reason for his ban, as Anthropic provided no specific explanation.

This lack of communication and recourse was a major theme. Several users shared their own negative experiences with being banned without cause or support, reinforcing the idea that frontier AI labs offer virtually no customer service. One commenter lamented this as a missed opportunity, arguing that AI companies should be using their own technology to provide "magical" support and that the failure to do so undermines trust in their products.

The conversation then branched into two main solutions. The first was a pragmatic acceptance that on a company's platform, they make the rules, and users should be prepared to be ejected at any time. The second, more popular solution was to move away from proprietary services entirely. Commenters advocated for running local, open-source models, which, while potentially less powerful, offer uncensored capabilities, full control, and the ability to build custom multi-agent systems without fear of arbitrary bans. The discussion also took a brief detour to critique the author's writing style as overly verbose and to mock Grok's "safety" alignment with Elon Musk's interests.

---

## [Bugs Apple Loves](https://www.bugsappleloves.com)
**Score:** 484 | **Comments:** 197 | **ID:** 46727587

> **Article:** The article is a single-page website titled "Bugs Apple Loves," which lists a collection of persistent, long-standing software bugs across Apple's ecosystem (macOS, iOS, iCloud, etc.). The list includes issues such as Spotlight search freezing, Bluetooth audio stuttering, Apple Watch unlock failures, and difficulties with account creation and verification. The site appears to be a curated compilation of common user complaints, likely intended to highlight the perceived decline in software quality and polish at Apple.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, with users sharing personal anecdotes of similar frustrations. A central theme is the tension between engineering resources and business priorities. One commenter argues that Apple has enough engineers, but they are directed toward new "10X" features rather than paying down technical debt, noting a shift from Apple's past focus on polish (e.g., Snow Leopard) to the current release cycle.

Several users shared specific, painful experiences that mirror the article's content, such as struggles with Apple ID verification (especially with custom domains or non-Apple browsers), the need to keep the Gmail app on iPhone for reliable search, and various macOS glitches like Bluetooth stuttering and Apple Watch unlock failures.

The conversation also touched on the structural reasons for these issues. One commenter described a process where bugs are simply deferred to future releases rather than being fixed, and expressed skepticism that hiring more engineers would help, suggesting it might just lead to shipping more bugs. The discussion concluded with a mix of humor and resignation, with a notable XKCD comic linked to underscore the difficulty of fixing established software behaviors.

---

## [Why does SSH send 100 packets per keystroke?](https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/)
**Score:** 420 | **Comments:** 245 | **ID:** 46723990

> **Article:** The article investigates why SSH sends approximately 100 packets for every keystroke. The author, developing a high-performance game over SSH, used `tcpdump` to analyze traffic and found the behavior puzzling. The investigation reveals that this is due to SSH's keystroke timing obfuscation feature, designed to prevent traffic analysis attacks where an observer could infer what is being typed based on packet timing. To achieve this, SSH sends "chaff" packets to mask the real keystroke packets. The author also notes that the overhead of processing these packets consumed about 20% of a CPU core, a surprisingly high cost for the data rate.
>
> **Discussion:** The discussion centered on several key themes. A primary focus was the security trade-off of SSH's keystroke obfuscation. While some commenters expressed concern about disabling this feature, the author clarified that the performance trade-off was acceptable for their specific use case (a game), where security is less critical than responsiveness.

Technical solutions for reducing packet count were proposed, most notably `TCP_CORK`, which allows the kernel to buffer data and send it in larger chunks, reducing overhead without the latency penalty of disabling `TCP_NODELAY`. The high CPU cost of SSH's packet processing was also questioned, with some finding the 20% core usage for ~10,000 packets/sec surprisingly inefficient, while others noted the overhead of encryption.

The article's writing style was a minor point of contention, with some commenters critiquing the author's use of AI-like phrases like "smoking gun." Finally, there was a broader debate about the author's choice to use SSH for a high-performance game, with some suggesting more suitable protocols like UDP with a custom crypto layer (e.g., QUIC, GameNetworkingSockets) would be more appropriate, while others appreciated the creative constraint of working within SSH's limitations.

---

## [Douglas Adams on the English–American cultural divide over "heroes"](https://shreevatsa.net/post/douglas-adams-cultural-divide/)
**Score:** 419 | **Comments:** 407 | **ID:** 46719222

> **Article:** The article discusses a cultural divide in storytelling between the UK and the US, as articulated by author Douglas Adams. Adams observed that American narratives typically feature "winners"—competent, autonomous heroes who master their environment and achieve clear success (e.g., James Bond, Indiana Jones). In contrast, British narratives often center on "losers"—reluctant, bumbling, or socially awkward protagonists who fail or merely survive despite their efforts (e.g., Blackadder, Arthur Dent). Adams suggests this reflects a deeper cultural difference: the American tendency to celebrate success and mastery, versus the British tendency to find humor and sympathy in failure and social ineptitude.
>
> **Discussion:** The Hacker News discussion largely validates Adams' observation, with users expanding on the cultural divide and offering counterpoints. Many commenters, particularly those from the UK, strongly agreed with the sentiment, noting that American humor can be difficult to relate to. Several users pointed to specific media as evidence, such as the stark tonal difference between the UK and US versions of *The Office*—the former being gloomy and cynical, the latter optimistic and sunny. The "maker" and tradesman communities in the US were highlighted as a notable exception, where self-deprecating humor and a celebration of failure are common, closely mirroring the British sensibility.

The conversation also explored how this divide applies to broader fiction. Some argued that globally successful British properties like *Harry Potter* or *The Lord of the Rings* succeeded in the US precisely because their protagonists (Harry, Frodo) fit the "reluctant hero" mold that Adams describes, or because their Britishness was perceived as fantastical. Others offered counterexamples of American "lovable losers," like Charlie Brown, though debate ensued over whether he is truly a "hero" or simply a figure of contempt. A recurring theme was the difficulty of generalizing about two diverse nations, with some commenters cautioning against over-simplification while acknowledging the pattern holds true for mainstream Hollywood productions.

---

## [It looks like the status/need-triage label was removed](https://github.com/google-gemini/gemini-cli/issues/16728)
**Score:** 285 | **Comments:** 74 | **ID:** 46721179

> **Article:** The article links to a GitHub issue in the `google-gemini/gemini-cli` repository where a bot named `gemini-cli[bot]` became stuck in an infinite loop. The bot repeatedly added and removed the `status/need-triage` label from the issue, generating over 4,600 comments in a short period. Each comment contained a contradictory explanation for the label change, effectively arguing with itself.
>
> **Discussion:** The Hacker News discussion focused on the absurdity of the situation and the technical failures involved. Commenters were largely amused and critical, describing the event as a "classic CI bug" amplified by LLMs. Key points included:

*   **The Loop Mechanism:** Users analyzed how the bot likely failed to recognize its own actions, treating its label changes as external events that required a correction, which in turn triggered another correction, creating a feedback loop. The sheer volume of 4,600 iterations highlighted the lack of basic loop detection.
*   **Cost and Noise:** Several users pointed out the practical consequences, such as the massive number of email notifications sent to repository watchers and the potentially high cost of inference calls for an unaffiliated developer hosting the bot.
*   **Recurring Issue:** It was noted that this was not an isolated incident, with links provided to other recent issues in the same repository exhibiting similar bot loops.
*   **Broader Implications:** The discussion touched on the irony of "AI progress" resulting in such inefficient and "stupid" behavior. Comparisons were drawn to classic automation failures (like a Salesforce rule creating an email loop) and the philosophical challenge of asking an LLM to solve the halting problem in a prompt.

---

## [Design Thinking Books (2024)](https://www.designorate.com/design-thinking-books/)
**Score:** 283 | **Comments:** 128 | **ID:** 46718061

> **Article:** The article is a curated list of recommended books on "Design Thinking" published in 2024. It aims to provide a collection of essential reading for practitioners, covering foundational theories, practical guides, and modern applications of the design thinking methodology.
>
> **Discussion:** The Hacker News discussion largely bypassed the specific book list in the article, instead engaging in a broader debate about the value and definition of "Design Thinking" itself. The conversation revealed a significant divide between those who view it as a practical, essential framework and those who consider it an over-simplified buzzword.

Key themes included:
*   **Criticism of "Design Thinking":** Several commenters dismissed the concept. One argued it's an oversimplified subset of the more robust "Systems Thinking" and encouraged readers to study the latter instead. Another commenter found the term hyperbolic and unnecessary, suggesting it's merely a rebranding of "good old regular design" to separate it from visual output. A third user was critical of the book list itself, calling it a generic collection of popular minimalism titles.
*   **Defense of "Design Thinking" as a Pragmatic Tool:** In response to the Systems Thinking critique, others defended Design Thinking as a practical and operational framework for product design. They argued that while Systems Thinking is a broader superset, Design Thinking provides an effective, actionable methodology for specific projects, similar to human-centered design.
*   **Additional Book Recommendations:** Despite the meta-debate, several users contributed valuable book suggestions not mentioned in the original article. These included classics like "Don't Make Me Think" by Steve Krug (for web usability), "Creative Confidence" by Tom and David Kelley (from IDEO and Stanford's d.school), and "The Design of Design" by Fred Brooks.
*   **Personal Experiences and Critiques:** A developer shared their mixed experience reading "The Design of Everyday Things," finding it highly academic and, in one instance regarding password security, somewhat out of touch with modern reality. In response, another user recommended "Refactoring UI" as a more practical alternative for developers and engineers.
*   **Humor and Meta-Commentary:** The discussion also featured lighter moments, including a joke about the article's title being a "garden-path sentence" (misinterpreted as books that think) and a meta-observation that the article's author likely isn't using AI, based on a specific formatting detail in the text.

---

## [Capital One to acquire Brex for $5.15B](https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/)
**Score:** 280 | **Comments:** 210 | **ID:** 46725288

> **Article:** Reuters reports that Capital One is acquiring the fintech company Brex for $5.15 billion. The deal represents a significant haircut from Brex's peak valuation of $12.3 billion, which it achieved in a 2021 funding round. The acquisition is expected to bolster Capital One's commercial banking portfolio with Brex's technology and customer base.
>
> **Discussion:** The Hacker News community's reaction is centered on the financial implications of the deal, particularly the sharp decline in Brex's valuation from its $12.3 billion peak. Many commenters view this as a "bad deal" for Brex's later-stage investors and employees, who are likely taking a significant loss on their equity, while earlier investors and founders still secured a profitable exit.

A key theme is the end of the "ZIRP" (Zero Interest Rate Policy) era, which is seen as the primary reason for the inflated valuations of fintech companies like Brex. Users argue that Brex's business model, which involved extending credit without traditional guarantees, was only viable in a low-interest-rate environment. The consensus is that the acquisition price reflects a return to fundamentals based on discounted cash flows rather than speculative growth.

There is also speculation about Brex's recent performance. Some users suggest that Brex lost its leadership position to competitors like Ramp and failed to successfully pivot into AI. However, others defend the acquisition as a strategic win for Capital One, which is acquiring a growing asset at a reasonable multiple of its revenue (around 7x). The discussion concludes with a mix of sympathy for affected employees and skepticism towards both Brex and Capital One's business practices.

---

## [30 Years of ReactOS](https://reactos.org/blogs/30yrs-of-ros/)
**Score:** 250 | **Comments:** 153 | **ID:** 46716469

> **Article:** This article celebrates the 30th anniversary of ReactOS, an open-source operating system project dedicated to providing a Windows-compatible environment built from scratch. The post reflects on the project's long history, its engineering challenges, and its enduring goal of creating a free, open-source alternative to Windows capable of running native drivers and applications.
>
> **Discussion:** The Hacker News discussion is a mix of admiration for the project's longevity and a pragmatic assessment of its current relevance. A central theme is the debate over ReactOS's practical utility versus its value as a pure engineering exercise. Many commenters argue that for running Windows applications, solutions like Wine/Proton on Linux have become far more advanced and practical, especially for modern software and gaming. They contend that ReactOS missed its window of opportunity and is now more akin to a project like GNU Hurd—a challenging and impressive feat of reverse engineering, but unlikely to see widespread adoption.

The conversation also heavily revolves around the potential impact of AI on a project like ReactOS. While some speculate that AI could accelerate development, others raise a critical legal and philosophical roadblock: ReactOS's "clean room" design requires contributors to certify they have not seen leaked Windows source code. Since modern AI models may have been trained on such leaks, using their output would compromise this legal safeguard, likely preventing the project from accepting AI-generated code.

Finally, commenters express both nostalgia and hope. Some share personal stories of trying ReactOS over the years and noting its slow but steady progress, while others fantasize about a billionaire sponsor funding the project to completion. There's a general consensus that while ReactOS may not be a practical replacement for Windows today, its existence and continued development are valued by the community.

---

## [Tree-sitter vs. Language Servers](https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/)
**Score:** 236 | **Comments:** 62 | **ID:** 46719899

> **Article:** The article compares Tree-sitter and Language Server Protocol (LSP) for code editing features. It positions Tree-sitter as a fast, error-tolerant parser generator primarily used for syntax highlighting, structural editing, and code folding. In contrast, it describes LSP as a protocol enabling rich, semantic language features (like autocompletion, go-to-definition, and refactoring) by communicating with a dedicated language server. The author notes that while LSP can theoretically handle syntax highlighting, Tree-sitter is generally preferred for this due to its speed and synchronous nature, whereas LSP's asynchronous nature can introduce latency. The article concludes that the two technologies are complementary: Tree-sitter excels at immediate, syntax-based feedback, while LSP provides deep, semantic understanding of the code.
>
> **Discussion:** The discussion largely validates the article's premise but offers significant nuance and technical depth. A key theme is the complementary nature of Tree-sitter and LSP. Several users, including a developer from the Roslyn (C#) team, explain that the ideal setup uses Tree-sitter for immediate, low-latency syntax highlighting and structural analysis, while an LSP handles semantic features asynchronously. This prevents UI sluggishness, as LSP round-trips can be slow.

There is a debate on the performance of LSPs. While the article and some commenters suggest LSPs can be slow for highlighting, the Roslyn developer counters that modern LSPs (like Roslyn) are highly optimized with incremental parsing, aiming for microsecond response times and running in-process to avoid network latency. They argue that a well-designed LSP should not feel sluggish.

The conversation also highlights practical use cases. Tree-sitter is praised for its utility in developing new programming languages and powering advanced editor features like structured editing (e.g., Emacs's Combobulate). LSP is favored for its semantic capabilities, such as distinguishing between mutable and immutable variables in Rust for more nuanced syntax highlighting.

Several comments address practical concerns. One user points out that the absence of a package for a specific language in a distribution (like Arch Linux) doesn't mean Tree-sitter lacks support, as third-party grammars exist. Another user asks for resources on building browser-based editors, leading to a detailed response explaining the roles of parsers, LSPs, and Tree-sitter, complete with links to specifications and tutorials. Finally, a minor but notable side-discussion emerges about the value of human-written articles, with users agreeing that writing forces deeper thinking and that AI-generated content often lacks this "distillation of thought."

---

## [Macron says €300B in EU savings sent to the US every year will be invested in EU](https://old.reddit.com/r/europe/comments/1qjtvtl/macron_says_300_billion_in_european_savings_flown/)
**Score:** 175 | **Comments:** 193 | **ID:** 46722594

> **Article:** French President Emmanuel Macron, speaking at the World Economic Forum in Davos, announced a major policy shift for the European Union. He stated that the €300 billion in European savings that are currently sent to the US annually will be redirected and invested within the EU. The goal is to bolster the European economy and create a more self-sufficient investment landscape.
>
> **Discussion:** The Hacker News discussion is largely skeptical of Macron's announcement, viewing it as political posturing rather than a feasible economic plan. The consensus is that capital flows are driven by market forces and investment returns, not political declarations.

Key points of the discussion include:
*   **Economic Feasibility:** Several users argue that the statement is economically nonsensical. They point out that in a floating exchange rate system, capital cannot simply be "redirected" by decree; investors will naturally seek the highest returns, and the EU must first create more attractive investment opportunities and growth policies to retain capital.
*   **Political Motivation:** Many commenters believe Macron's figures are exaggerated and the announcement is primarily aimed at an international audience, specifically Donald Trump, rather than being a serious domestic policy.
*   **EU Competitiveness:** There is a broader critique of the EU's economic environment. Users suggest that the EU is uncompetitive due to excessive regulation, bureaucracy, and a lack of innovation compared to the US. Some mention that the EU has failed to secure major trade deals (like Mercosur) for decades, suggesting this new goal is equally unrealistic.
*   **Counterpoints on Savings:** A sub-discussion emerged about the personal savings rates in the EU versus the US, with some users expressing surprise at how low the US rate is, while others clarify that the EU's higher rate may be influenced by different economic structures and social safety nets.
*   **Tangential and Humorous Comments:** The conversation also included unrelated jokes about Macron's sunglasses (which an article link clarified were due to an eye infection), references to Paul Graham and Y Combinator as examples of what Europe lacks, and discussions on specific financial instruments like the MSCI World index.

---

## [CSS Optical Illusions](https://alvaromontoro.com/blog/68091/css-optical-illusions)
**Score:** 170 | **Comments:** 14 | **ID:** 46722570

> **Article:** The article showcases a collection of CSS-based optical illusions created by Alvaro Montoro. It demonstrates how modern CSS techniques (like gradients, transforms, and animations) can be used to recreate classic visual phenomena such as the "Rotating Snakes," "Scintillating Grid," and "Café Wall" illusions directly in the browser. The post serves as both a demonstration of CSS capabilities and a gallery of visual tricks.
>
> **Discussion:** The Hacker News community reacted positively to the post, with commenters expressing admiration for the technical execution and sharing their own experiences with similar effects. Several users noted that while the illusions are visually interesting, they often appear as unintentional bugs when used in UI design. 

A significant portion of the discussion focused on the specific illusions presented. One user identified the technical names for the "appearing and disappearing dots" illusions (McAnany's and Ninio's types) and provided links to academic sources and their own recreations. Others shared personal projects, such as a recreation of the Ames window illusion using CSS. 

There were also practical observations: a few users pointed out that the CodePen previews appeared dark until interacted with, and others suggested that these illusions could theoretically be used to create CAPTCHAs. The conversation concluded with a reflection on how such visual tricks might help researchers understand the brain's visual processing mechanisms.

---

## [ISO PDF spec is getting Brotli – ~20 % smaller documents with no quality loss](https://pdfa.org/want-to-make-your-pdfs-20-smaller-for-free/)
**Score:** 151 | **Comments:** 100 | **ID:** 46717507

> **Article:** The article from the PDF Association announces that the ISO PDF specification is being updated to support Brotli compression. The stated goal is to reduce PDF file sizes by approximately 20% without any loss of quality. The article frames this as a significant efficiency gain for document storage and distribution, noting that major open-source PDF libraries like MuPDF and Ghostscript are already implementing support for the new feature.
>
> **Discussion:** The Hacker News discussion is overwhelmingly critical of the decision to adopt Brotli, focusing on several key themes:

The most prominent criticism is the choice of Brotli over Zstandard (zstd). Commenters almost universally argue that zstd is superior for this use case, offering significantly faster decompression speeds (a crucial factor for "read-many" documents like PDFs) while providing comparable or better compression ratios. The decision is frequently attributed to "incompetence" or the fact that Brotli is a Google-developed algorithm, implying corporate influence over technical merit.

There is significant skepticism about the practical impact and necessity of this change. Many argue that the 20% size saving is marginal and doesn't justify the massive compatibility headache of introducing a breaking change. This directly contradicts the article's claim that the feature will "work seamlessly with existing readers," as new PDFs using Brotli will be unreadable by older software. This will lead to a very long and fragmented adoption period.

Technical details of the implementation are also questioned. One commenter points out that using Brotli's standard web-centric dictionary is a "bizarre" choice for PDFs, as the content is very different from web pages. This could lead to suboptimal compression and bakes an outdated, web-specific dictionary into a format intended for long-term archival.

Finally, some dismiss the entire effort as redundant. They argue that modern transport (HTTP) and filesystem-level compression already handle this efficiently, making it unnecessary to build generic compression into the file format itself. The article itself is also criticized by some as being low-quality "AI slop."

---

## [Scaling PostgreSQL to power 800M ChatGPT users](https://openai.com/index/scaling-postgresql/)
**Score:** 145 | **Comments:** 54 | **ID:** 46725300

> **Article:** OpenAI published an engineering blog post detailing how they scaled PostgreSQL to support over 800 million ChatGPT users. The article explains that while PostgreSQL performed well for read-heavy workloads, it faced challenges with write-heavy traffic due to its Multiversion Concurrency Control (MVCC) implementation. This architecture causes write amplification (copying entire rows for updates) and read amplification (scanning multiple tuple versions), leading to table bloat and complex autovacuum tuning. To address this, OpenAI offloaded write-intensive, shardable workloads to Azure CosmosDB (a NoSQL database) while maintaining PostgreSQL for read-heavy operations. They also scaled their PostgreSQL infrastructure significantly, adding nearly 50 read replicas to handle read traffic and keep replication lag near zero.
>
> **Discussion:** The Hacker News discussion focused on the technical trade-offs of database scaling and the nature of OpenAI's engineering choices. A central theme was the debate over PostgreSQL's scalability limits. Some commenters expressed surprise that OpenAI had to leave the PostgreSQL ecosystem entirely for writes, suggesting that native sharding solutions like table partitioning and Foreign Data Wrappers (FDW) could have handled the load. Others defended the move, noting that migrating to a different database is a mature engineering decision when fundamental architectural limitations (like MVCC overhead) are hit, and that "migrations are not fun."

There was also significant skepticism regarding the novelty and depth of the article. Several users pointed out that the strategies described—sharding, using read replicas, and optimizing queries—are standard industry practices for scaling databases, and felt the post lacked technical specifics. One commenter speculated that the blog post might be AI-generated due to its generic tone and repetitive context.

Finally, the discussion touched on the broader context of OpenAI's infrastructure. Users debated the role of Azure in the stack, with some joking about Microsoft being "bested" by open-source software on their own cloud, while others noted that Azure's managed PostgreSQL services likely handle much of the underlying complexity. There was also interest in seeing more from OpenAI's engineering blog and questions about the specifics of their replication setup, particularly how they manage consistency across 50 read replicas.

---

## [Recent discoveries on the acquisition of the highest levels of human performance](https://www.science.org/doi/abs/10.1126/science.adt7790)
**Score:** 125 | **Comments:** 60 | **ID:** 46722853

> **Article:** The article, "Recent discoveries on the acquisition of the highest levels of human performance," challenges the conventional belief that early specialization and intense practice from a young age are the primary paths to elite achievement. The research suggests that top performers in fields like science, arts, and sports often follow a "sampling period" in their youth, exploring a wide range of activities before narrowing their focus later in life. This broad experience is linked to greater creativity and the ability to make novel connections. The study also finds that early high achievers are often different individuals from the later top performers, with many world-class experts not standing out as exceptional in their youth. The proposed path to greatness involves early diversification, followed by a period of focused, deliberate practice in a chosen domain.
>
> **Discussion:** The Hacker News discussion centered on validating, contextualizing, and critiquing the article's findings. Many commenters felt the research aligned with personal observations, citing anecdotes of friends who were "well-rounded" in their youth and later achieved great success, while early specialists often burned out. This led to a debate on whether early success is a product of innate talent or intense parental pressure, with some sharing personal stories of struggling when their natural aptitude was no longer sufficient without developed discipline.

A significant portion of the conversation focused on statistical critiques, particularly Berkson's Paradox. Several users argued that the observed negative correlation between early and later top performers could be an artifact of selection bias—by only studying those who reached the absolute peak, the data may create a misleading pattern. This statistical lens was used to question the study's conclusions, suggesting the relationship might be more complex than presented.

The discussion also connected the article's themes to existing popular literature, with multiple users noting its strong resemblance to the core arguments in David Epstein's book, *Range: Why Generalists Triumph in a Specialized World*. The concept of "cross-domain synthesis" was highlighted as a key benefit of a diverse background, enabling creative problem-solving that specialists might miss. Finally, a brief but pointed sub-thread emerged debating the pop-culture interpretation of ADHD, with one user arguing that the "hyperfocus" often associated with the condition is a misrepresentation and not a reliable indicator of high performance.

---

## [White House Posts Digitally Altered Image of Woman Arrested After ICE Protest](https://www.theguardian.com/us-news/2026/jan/22/white-house-ice-protest-arrest-altered-image)
**Score:** 122 | **Comments:** 16 | **ID:** 46725268

> **Article:** The Guardian article reports that the White House digitally altered an image of a woman arrested during an ICE protest to make her appear more menacing. When questioned about the manipulation, the Deputy Communications Director responded not with a denial, but with a post on X (formerly Twitter) stating that "the memes will continue" and mocking those who defend alleged perpetrators. The article highlights the administration's use of generative AI for propaganda and its disregard for the presumption of innocence.
>
> **Discussion:** Commenters expressed alarm at the normalization of government fabrication of evidence and the erosion of due process. The discussion focused on the broader implications of the White House's actions, with users drawing parallels to political instability in other nations, such as South Africa during the "State Capture" years. Key themes included the dangerous precedent of a government using AI to alter contemporaneous images, the official response's dismissal of legal norms (specifically the presumption of innocence), and the stylistic mimicry of populist rhetoric. While one user questioned the relevance of a specific detail regarding the separation of church and state, the consensus among critics was that the incident represented a significant and "bonkers" departure from constitutional norms.

---

