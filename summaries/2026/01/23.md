# Hacker News Summary - 2026-01-23

## [Bugs Apple Loves](https://www.bugsappleloves.com)
**Score:** 859 | **Comments:** 404 | **ID:** 46727587

> **Article:** The article is a website titled "Bugs Apple Loves," which catalogues a list of persistent, long-standing software bugs in Apple's ecosystem. The bugs listed are common user frustrations, such as Spotlight search freezing, issues with Apple ID verification on the web, problems with Siri-generated alarms and ringtones, and macOS-specific glitches like Bluetooth audio stuttering and Apple Watch unlock failures. The site's design is minimalist, and it appears to be a grassroots effort to publicly document these unresolved issues.
>
> **Discussion:** The Hacker News discussion is largely sympathetic to the article's premise, with users sharing their own frustrations and offering analysis on why these bugs persist. A key theme is the tension between engineering priorities and business goals. One commenter argues the issue isn't a lack of engineers but a focus on new "10X" features over paying down technical debt, noting that Apple has shifted away from its past focus on polish (e.g., the "bug-fix-only" Snow Leopard release).

Several users share personal anecdotes that validate the article's claims, citing specific problems with Apple ID creation, email search (with one user keeping the Gmail app on their iPhone for this reason), and contact syncing with Office 365. The discussion also touches on the systemic nature of the problem, with one commenter describing a development cycle where unfixed bugs are simply pushed to the next release, creating a growing backlog. There is some debate about the website's aesthetics, with one user calling its design "AI-generated," while another defends it as functional for its purpose. Overall, the sentiment is that the list is a relatable and "petty" but accurate reflection of declining software quality at Apple.

---

## [I was banned from Claude for scaffolding a Claude.md file?](https://hugodaniel.com/posts/claude-code-banned-me/)
**Score:** 668 | **Comments:** 582 | **ID:** 46723384

> **Article:** The article details the author's experience of being banned from using Claude Code after attempting to "scaffold" a `CLAUDE.md` file. The author describes a workflow involving multiple Claude instances to generate and evaluate prompt files, which they believed was a legitimate use case for project setup. Upon receiving a ban for "violating usage policies," they speculate that their activity triggered prompt injection heuristics. The post highlights the lack of recourse or explanation from Anthropic support and serves as a critique of the "black box" nature of AI moderation and the absence of human support channels for paid users.
>
> **Discussion:** The Hacker News discussion centered on skepticism regarding the author's narrative, the broader issues of AI company support, and the trade-offs of using hosted vs. local models.

Many commenters were confused by the technical details of the blog post, noting that the author's description of their workflow was vague and didn't clearly explain what they were doing. Several users pointed out that the author was merely guessing that their file scaffolding caused the ban, as Anthropic provided no specific reason. This ambiguity fueled a debate about the fairness of opaque moderation systems.

A significant portion of the conversation focused on the lack of customer support from AI companies like Anthropic. Users shared personal anecdotes of being banned without recourse or explanation, leading to a consensus that relying on hosted services for critical work is risky. One commenter suggested that Anthropic's failure to use its own AI for customer support is a missed opportunity and a strategic weakness, as it implies they don't trust their own technology with sensitive decisions.

The discussion also touched on the "safety" of different AI models, with one user sarcastically noting that Grok's safety filters prioritize protecting its creator's beliefs. Ultimately, the prevailing sentiment was that users should move to self-hosted, open-source models to avoid arbitrary bans and gain full control, even if it means sacrificing the performance of top-tier models like Opus.

---

## [Why does SSH send 100 packets per keystroke?](https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/)
**Score:** 613 | **Comments:** 322 | **ID:** 46723990

> **Article:** The article investigates why SSH connections send an unusually high number of packets (around 100) for every single keystroke. The author, who is building a game over SSH, initially suspected their own code or network settings. Through debugging and consulting an AI (Claude), they discovered the cause is a security feature in OpenSSH designed to prevent keystroke timing attacks. To obfuscate the exact timing of user input, SSH deliberately injects "chaff" packets alongside the real data. The author also explored modifying the Go `crypto/ssh` library to disable this behavior for their specific use case, where low latency is prioritized over this security mitigation.
>
> **Discussion:** The discussion centered on several key themes. A significant portion of the conversation humorously critiqued the AI assistant's (Claude) communication style, with multiple users noting its tendency to use dramatic phrases like "smoking gun" and corporate jargon. Technically, users offered solutions to the packet overhead problem, primarily discussing `TCP_CORK` as a method to coalesce packets without adding latency, and contrasting it with `TCP_NODELAY`, which would increase latency. The security implications of disabling SSH's keystroke obfuscation were debated; while the author justified it for their game, others warned strongly against it in production environments, citing the real risk of timing attacks. Finally, some commenters questioned the author's choice of SSH for a high-performance game, suggesting more suitable protocols like UDP with libraries such as QUIC or Valve's GameNetworkingSockets, though the author defended the choice as an interesting technical challenge.

---

## [AI Usage Policy](https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md)
**Score:** 419 | **Comments:** 202 | **ID:** 46730504

> **Article:** The article is the AI Usage Policy for the Ghostty terminal emulator project. It explicitly permits AI tools for internal development but imposes strict rules on external contributions. Key restrictions include: a ban on AI-generated pull requests (PRs) without prior explicit approval, a requirement that all AI-assisted code be fully verified by a human, and a prohibition on using AI to generate documentation or issue reports. The policy's stated rationale is not an anti-AI stance, but a response to a high volume of low-quality, unverified, and often incorrect AI-generated contributions ("slop") that consume maintainer time. The project also notes that due to the unsettled legal status of AI-generated content, it cannot guarantee the copyrightability of such contributions.
>
> **Discussion:** The discussion centers on the practical and philosophical challenges of integrating AI into software development, particularly in open source. A primary theme is the distinction between the tool and the user: many commenters agree that AI is not the problem itself, but rather a force multiplier for low-quality work from unskilled or irresponsible developers. The consensus is that human accountability for code quality remains paramount, and that "slop" is a failure of professional standards, not the tool. However, the conversation acknowledges that the sheer speed and ease of AI generation has dramatically lowered the barrier to producing such low-quality contributions, creating a new challenge for open-source maintainers who are often volunteers with limited time.

The discussion also explores several other key points. There is a debate on the long-term goals of AI tools, with one commenter questioning the narrative that these tools are meant to augment rather than replace human thinking, pointing to industry marketing that promises ever-increasing capabilities. Legal and copyright concerns are raised, with the uncertainty around the copyrightability of AI-generated code being a significant, though not yet critical, risk for projects. Practical solutions are also discussed, such as the difficulty of maintaining full transcripts of AI sessions for review and the idea of donating AI credits to the project instead of submitting raw AI-generated code. Ultimately, the policy is widely seen as a balanced and necessary response to a new, AI-driven wave of contribution spam that threatens the sustainability of volunteer-run projects.

---

## [Proton Spam and the AI Consent Problem](https://dbushell.com/2026/01/22/proton-spam/)
**Score:** 384 | **Comments:** 241 | **ID:** 46729368

> **Article:** The article "Proton Spam and the AI Consent Problem" by David Bushell expresses frustration with receiving unsolicited marketing emails from Proton, a company known for its privacy-focused services. Bushell argues that this incident highlights a broader issue in the tech industry: the aggressive, non-consensual rollout of AI features. He contends that the tech industry, particularly the AI sector, operates on the principle of non-consent, forcing new features and products on users who have not opted in. The author frames this as a symptom of a potential bubble, where the pressure to show growth and adoption of AI overrides user experience and respect for user choice.
>
> **Discussion:** The Hacker News discussion largely validates the article's central thesis, with commenters expressing widespread frustration over unsolicited marketing and the forced integration of AI into services. A dominant theme is the erosion of user consent. Many users shared personal anecdotes of receiving unwanted emails from Proton and other services, with one commenter noting that Proton was the only source of spam for a dedicated honeypot email address. This sentiment is extended to the broader tech industry, with users criticizing how companies, including privacy-focused ones like Proton and DuckDuckGo, are "jumping on the AI train" and abandoning their principles.

A second major theme is the poor quality of many AI implementations. One commenter pointed out that many AI features, such as Shopify's code assistant or Amazon's product Q&A, are dysfunctional and produce incorrect results, questioning the logic of releasing such flawed products. This ties into the "bubble" theory, where the rush to appear innovative with AI leads to poorly executed features that alienate users.

The discussion also touched on the lack of consequences for such marketing practices. While some noted that regulations like GDPR in the EU provide some recourse, others argued that the potential gains from the AI "bubble" make companies willing to risk fines. A specific technical explanation was offered for the Proton incident, with a commenter suggesting that such "mistakes" are often the result of junior developers being tasked with implementing features designed to be intrusive by design. A Proton CTO's comment was also cited, claiming the email was a technical error, though this did little to quell the broader sentiment of frustration.

---

## [I built a light that reacts to radio waves [video]](https://www.youtube.com/watch?v=moBCOEiqiPs)
**Score:** 376 | **Comments:** 82 | **ID:** 46728808

> **Article:** The article links to a YouTube video demonstrating a custom-built light device that visually reacts to ambient radio waves (likely Wi-Fi and other RF signals). The creator, "tzvc," built a physical installation that translates the invisible electromagnetic spectrum into visible light, showing how our environment is saturated with unseen signals. The video serves as a proof-of-concept for making the invisible world of radio frequency communications perceptible to the human eye.
>
> **Discussion:** The Hacker News community reacted with enthusiasm, describing the project as "cool," "fantastic," and "mesmerizing." Commenters appreciated the artistic and sensory value of translating invisible radio waves into visible light, viewing it as adding a new dimension to human perception.

Several users engaged in technical and conceptual brainstorming. A key discussion point involved the desire for better visualization and directionality. One commenter noted that the current effect looked like "random noise" and suggested testing the device in remote areas to see clearer reactions to specific signals like phones or Bluetooth speakers. Another user elaborated on a concept for a 3D visualizer that could map specific frequencies to colors and determine the direction of signals using multiple antennas, potentially creating a real-time overlay of the RF environment. Another user linked to an existing video demonstrating similar RF imaging technology.

The conversation also touched on practical applications and broader context. Users mentioned seeing similar technology used in military settings (specifically for detecting drone signals in Ukraine) and suggested commercializing the device via Kickstarter. There was also a philosophical reflection on how modern life bathes us in an invisible "sea of radiation" from electricity and technology.

---

## [Capital One to acquire Brex for $5.15B](https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/)
**Score:** 365 | **Comments:** 292 | **ID:** 46725288

> **Article:** Capital One has announced it will acquire the fintech firm Brex for $5.15 billion. The deal represents a significant discount from Brex's peak valuation of $12.3 billion in 2022, which occurred during the era of near-zero interest rates (ZIRP). The acquisition is framed as Capital One expanding its business customer portfolio by acquiring Brex's customer base and infrastructure.
>
> **Discussion:** The Hacker News discussion centers on the financial mechanics of the acquisition, specifically the impact of liquidation preferences on different stakeholders. The consensus is that while the deal is a "fair price" for Capital One, it is a disappointing outcome for Brex's investors and a potentially devastating one for employees.

Key themes include:

*   **The Impact of ZIRP and Market Shifts:** Commenters view the acquisition as a correction of "fintech exuberance" fueled by the Zero Interest Rate Policy (ZIRP). As interest rates rose, Brex's model of extending credit without personal guarantees became less viable, and the company failed to pivot successfully (unlike competitor Ramp).
*   **Liquidation Preferences and the "Waterfall":** A major focus is on how the $5.15 billion payout is distributed. Users explain that because Brex raised hundreds of millions at a $12.3 billion valuation, late-stage investors have senior liquidation preferences (often 1x or more). This means investors get their money back first, potentially leaving little to nothing for common stockholders (founders and employees).
*   **Winners and Losers:**
    *   **Investors:** Late-stage investors likely break even or take a small loss, but early investors may still see returns.
    *   **Founders:** The CEO, Pedro Franceschi, is expected to receive a retention package and potentially sold shares in secondary markets, securing a low 8-figure sum, but far less than a $12 billion valuation would suggest.
    *   **Employees:** The discussion highlights that employees with stock options (especially those hired at the peak valuation) are likely "fucked," with underwater options or minimal payouts. RSU holders may receive a fraction of expected value.
*   **Corporate Culture and Future:** There is debate on how Capital One will integrate Brex. Some fear aggressive layoffs and stripping of the business (similar to Bending Spoons), while others note Capital One's history of allowing acquired units to maintain autonomy.
*   **Brex's Strategic Position:** Users noted that Brex had lost market leadership to competitors like Mercury and Ramp, contributing to the lower acquisition price.

---

## [Scaling PostgreSQL to power 800M ChatGPT users](https://openai.com/index/scaling-postgresql/)
**Score:** 270 | **Comments:** 121 | **ID:** 46725300

> **Article:** OpenAI published a blog post detailing how they scaled their PostgreSQL infrastructure to support over 800 million ChatGPT users. The post explains that while PostgreSQL performed well for read-heavy workloads, it faced significant challenges with write-heavy traffic due to the inherent inefficiencies of its Multi-Version Concurrency Control (MVCC) implementation. This architecture leads to write amplification, table/index bloat, and complex vacuuming requirements. To address this, OpenAI offloaded write-intensive, shardable workloads to Azure CosmosDB (a NoSQL solution) while continuing to use PostgreSQL for read-heavy operations. They also utilized Azure's flexible server instances, added nearly 50 read replicas to handle read traffic, and implemented aggressive caching strategies to reduce database load.
>
> **Discussion:** The Hacker News discussion focused heavily on the technical implications of OpenAI's architecture and the limitations of PostgreSQL. A central theme was the critique of PostgreSQL's MVCC, which users identified as the primary bottleneck for write-heavy workloads. Several commenters suggested alternative database technologies that might handle these loads better, specifically mentioning TiDB (which uses an LSM-tree storage engine) and Rockset (an OpenAI acquisition that originally built on RocksDB). There was some confusion and debate regarding OpenAI's decision to use CosmosDB instead of leveraging PostgreSQL's native sharding capabilities via Foreign Data Wrappers (FDW) or table partitioning, with some users arguing that Postgres can handle sharding out of the box.

Beyond the technical database debate, the community discussed the broader engineering strategy. Many praised the pragmatic approach of keeping PostgreSQL for what it does best rather than attempting a complex migration to a new database system. However, some commenters found the blog post itself to be generic and lacking in novel technical depth, viewing the solution as a standard application of read replicas and offloading. Financial concerns regarding OpenAI's sustainability were also raised, questioning the long-term viability of such expensive scaling strategies. Finally, there was a minor tangent regarding the authorship of the post (noting the Chinese names and Azure links) and skepticism about the stability of their infrastructure given past downtime.

---

## [European Alternatives](https://european-alternatives.eu)
**Score:** 260 | **Comments:** 99 | **ID:** 46731976

> **Article:** The article links to "European Alternatives," a curated directory of digital services and software that are European-based and GDPR-compliant. The site organizes alternatives to popular US and global tech platforms across categories like cloud computing, email, messaging, and productivity tools, aiming to help users migrate away from non-European providers.
>
> **Discussion:** The discussion centers on the growth and utility of European tech alternatives, with users generally welcoming the resource while debating its scope and the feasibility of a distinct European tech ecosystem. Many commenters noted the list has improved significantly since a previous submission in 2021, though some suggested adding categories for AI tools and hardware vendors. A recurring theme was the tension between localization and globalization; while some argued that nationalistic fragmentation makes everyone poorer, others countered that regional alternatives are necessary for data sovereignty and competition.

Technical and practical critiques emerged, particularly regarding the quality of European cloud providers. One user criticized Scaleway for poor handling of non-ASCII addresses, while another cited OVH's data loss as a more significant concern. On the topic of missing categories like operating systems and programming languages, commenters largely agreed that open-source software already fulfills these needs, making proprietary regional alternatives less critical.

Economic viability was a key concern, with several users pointing out that European tech salaries lag significantly behind US counterparts. However, optimism was expressed that EU investment initiatives could eventually narrow this gap. The discussion concluded with a mix of idealism and pragmatism: while some hoped for globally interoperable open-source solutions, others accepted that regional alternatives are a necessary step in a fragmented digital landscape.

---

## [Why medieval city-builder video games are historically inaccurate (2020)](https://www.leidenmedievalistsblog.nl/articles/why-medieval-city-builder-video-games-are-historically-inaccurate)
**Score:** 210 | **Comments:** 137 | **ID:** 46726857

> **Article:** The article argues that popular medieval city-builder games are historically inaccurate because they prioritize modern gameplay conventions over historical reality. Key inaccuracies include:
- **Urban Planning:** Games feature organic, winding streets, whereas real medieval cities had planned, straight layouts dictated by property boundaries and practicality.
- **Economics:** Games drastically underrepresent the massive labor force required for agriculture (historically ~96% of the population) and simplify complex supply chains.
- **Aesthetics:** Games depict earthy, muted tones, while historical evidence suggests medieval clothing and buildings were often brightly colored.
- **Gameplay vs. Reality:** The article notes that true historical simulation—featuring famine, disease, and lack of player agency—would be frustrating rather than fun, leading designers to create an idealized version of the past that matches player expectations rather than historical data.
>
> **Discussion:** The discussion largely agrees with the article's premise but explores the necessary trade-offs between historical accuracy and entertainment. A central theme is that "fun" must take precedence over realism; commenters argue that tedious tasks like manual crop irrigation or random disasters (plague, floods) would ruin the gaming experience, comparing it to the unrealistic mechanics found in FPS games. 

Users highlighted specific games that attempt greater accuracy. *Banished* was praised for capturing the struggle of subsistence living, while *Manor Lords* was noted for its non-grid layouts and inclusion of historical elements like women's work (gardening and clothing production). 

There was also a debate on player psychology. One user noted that players often reject historical accuracy (such as straight roads) because it breaks immersion, preferring the romanticized, "prettier" version of the Middle Ages. The conversation concluded with a humorous acknowledgment that all computer games are inherently inaccurate to the medieval period, as the technology didn't exist.

---

## [Booting from a vinyl record (2020)](https://boginjr.com/it/sw/dev/vinyl-boot/)
**Score:** 203 | **Comments:** 57 | **ID:** 46730885

> **Article:** The article details a technical project to boot a computer from a vinyl record. The author explains the process of converting a bootable disk image into an audio signal, which is then pressed onto a vinyl record. The computer boots by playing this audio back through a standard cassette interface (a legacy hardware port found on early PCs like the IBM PC 5150). The article includes the technical steps, the audio file generation, and a video demonstrating the successful boot process, highlighting the novelty and feasibility of using analog media for digital data transfer.
>
> **Discussion:** The Hacker News community reacted with enthusiasm to the project, praising its creativity and the tangible nature of older storage media. The conversation quickly expanded from vinyl records to other historical and unconventional methods of data distribution.

Several users reminisced about early computing storage methods, noting that:
*   **Audio cassettes** were a common medium for software on 8-bit machines like the Acorn Electron and Commodore 64, though they were notoriously unreliable and often required copying to a new tape immediately.
*   **Radio broadcasts** were used in various countries (notably Poland during the communist era) to transmit software for computers like the Atari 800 and ZX Spectrum over the airwaves.
*   **"Flexi-discs"** (vinyl-like records included in magazines) were used to distribute software, though they were fragile and often limited to one or two plays.

Technical discussions touched on the **early PC's cassette interface**, which many younger users were surprised to learn existed. Commenters noted it was a short-lived feature on the original IBM PC, removed to make room for more ISA slots in later models. There was also a tangent about the physicality of older hardware, with users recalling how the sounds of floppy or hard drives could often indicate mechanical issues or fragmentation.

Finally, a minor sub-thread discussed the technical hurdles of the article's source website, specifically a complex cookie consent popup that blocked access to the embedded video, prompting users to share command-line tools (like `yt-dlp`) to bypass the browser entirely.

---

## [Radicle: The Sovereign Forge](https://radicle.xyz)
**Score:** 190 | **Comments:** 85 | **ID:** 46732213

> **Article:** The article introduces Radicle, a peer-to-peer (P2P) code collaboration protocol designed to be a sovereign alternative to centralized platforms like GitHub. It emphasizes local-first architecture, where users run nodes that store repositories and metadata (issues, patches) as signed Git objects. This allows for full functionality offline and eliminates reliance on central servers. The system uses self-certifying identities (DIDs) to ensure repository integrity and stable identities, enabling users to clone and verify code from untrusted peers.
>
> **Discussion:** The discussion centers on Radicle's architecture, its comparison to other decentralized projects, and the inherent challenges of P2P systems.

**Architecture and Comparison**
Users compared Radicle to Tangled and Forgejo. Tangled is described as federated (client-server) and built on the AT Protocol, relying on servers ("knots") and a central AppView. In contrast, Radicle is strictly P2P with no central authority or servers; every node is equal, and data is stored locally. Similarly, Forgejo uses federated servers communicating via ActivityPub, whereas Radicle uses self-certifying identities without server-based access control.

**Trust and Identity**
A key point of debate was how trust is established. While Radicle solves the technical problem of verifying repository integrity via cryptographic identities, users questioned how one decides *which* identities to trust initially. The consensus was that this remains a social problem, similar to establishing trust on GitHub, where reputation is built through code review and social networks.

**Privacy and Content Moderation**
Several users raised concerns about the permanence of P2P data, specifically regarding mistakes, privacy leaks, and illegal content (referencing Usenet and HAM radio arrests). The Radicle team responded that they are working on making defaults safer and exploring network-level revocation features. However, users noted that centralized systems offer no guarantee of deletion either and that Radicle supports running nodes over Tor for anonymity.

**Usability and Network**
Technical questions covered IPv6 and alternative networks like Yggdrasil, with users confirming Radicle supports these. A developer from the team (endiangroup) was active in the comments, answering questions and engaging with the community.

---

## [Microsoft mishandling example.com](https://tinyapps.org/blog/microsoft-mishandling-example-com.html)
**Score:** 185 | **Comments:** 69 | **ID:** 46731996

> **Article:** The article details a significant security flaw in Microsoft's Autodiscover protocol used by Outlook. Due to the protocol's design, if a user configures an email address for a domain that lacks a specific `autodiscover` subdomain (e.g., `example.com`), the system will fall back to requesting configuration from a higher-level domain (e.g., `autodiscover.com`). Because Microsoft failed to secure the `autodiscover.com` domain, any email address using a domain without its own autodiscover subdomain would have its credentials and configuration requests routed to the owner of `autodiscover.com`. In this specific case, the traffic was sent to Sumitomo Electric Industries (SEI), exposing test credentials and other sensitive data from various organizations.
>
> **Discussion:** The Hacker News discussion focused on the technical failure, its security implications, and a broader critique of Microsoft's product management. The primary technical explanation was that the Autodiscover protocol's fallback mechanism is fundamentally flawed, as it prioritizes finding a working configuration over security, leading it to leak credentials to untrusted third parties. Commenters expressed alarm that Outlook sends full user credentials (email and password) to Microsoft's servers during the setup process, viewing this as a breach of privacy expectations, though some noted this is common practice for modern SaaS email clients.

Beyond the specific bug, the incident was used to criticize Microsoft's broader engineering and branding decisions. Users cited historical examples of poor guidance, such as Microsoft's past recommendation to use the reserved `.local` TLD for Active Directory, which caused conflicts with multicast DNS (mDNS). The conversation also included skepticism towards Microsoft's handling of its brands (Office, Xbox) and speculation that the leak could be an intentional NSA backdoor, though this was a minor point. A recurring theme was the danger of misusing IANA-reserved domains (like `.example`, `.test`) in production environments, with commenters advising the use of custom, impossible domains to prevent accidental data leaks.

---

## [What has Docker become?](https://tuananh.net/2026/01/20/what-has-docker-become/)
**Score:** 183 | **Comments:** 197 | **ID:** 46731748

> **Article:** The article "What has Docker become?" critiques Docker's evolution from a developer-focused tool into a complex, monetized platform. It argues that Docker created a standard so successful it became infrastructure, but as open infrastructure, it is difficult to monetize. The piece highlights the company's strategic missteps, such as abandoning Docker Swarm for Kubernetes and struggling to find a sustainable business model. It also notes the decline of Docker Desktop's user experience, which has led developers to seek simpler, more performant alternatives like OrbStack, Podman, or Nix-based workflows.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, focusing on Docker's declining developer experience and the company's struggle to monetize its open-source success. A dominant theme is the frustration with Docker Desktop, particularly on Windows and Mac, where users describe it as "fickle," "opaque," and prone to requiring full reinstalls to fix errors. This has created an opening for competitors, with multiple users praising OrbStack as a vastly superior, lightweight alternative to Docker Desktop on macOS.

On the business strategy front, commenters debate why Docker has struggled financially despite its ubiquity. The consensus is that "open infrastructure is hard to monetize," with some suggesting Docker should have stuck with its simpler Swarm orchestration instead of ceding the market to Kubernetes. There is also significant discussion around the broader economic model of open source, with commenters arguing that hyperscalers like AWS and Google profit from free labor without contributing back. This led to a tangent on alternative licensing models, such as "Fair Source" or "Fair Code," designed to prevent this exploitation.

Finally, the conversation touched on practical alternatives to the Docker ecosystem. Many developers expressed a preference for tools like Podman, Nix, or devcontainers, though the latter was criticized for performance lag. The discussion also highlighted Docker's lingering advantages, primarily its enterprise support, seamless desktop setup, and the vast ecosystem of Docker Hub, which remains a significant barrier to entry for competitors.

---

## [Updates to our web search products and  Programmable Search Engine capabilities](https://programmablesearchengine.googleblog.com/2026/01/updates-to-our-web-search-products.html)
**Score:** 175 | **Comments:** 149 | **ID:** 46730436

> **Article:** Google is discontinuing its "Search the entire web" feature for its Programmable Search Engine (formerly Custom Search). New engines will be limited to searching a maximum of 50 specific domains, while existing full-web engines must transition to alternative solutions by January 1, 2027. Google is directing users requiring full-web search capabilities toward its enterprise solutions, such as Vertex AI Search, which lacks transparent public pricing and requires direct contact.
>
> **Discussion:** The Hacker News community reacted with concern and criticism, viewing this as a significant blow to independent developers and niche search engines that relied on Google's index. The primary themes were the loss of a free, accessible API for building custom search tools and the increasing centralization of web search behind enterprise paywalls.

Many commenters expressed frustration that this move effectively ends the era of indie search engines building on Google's infrastructure. Users shared examples of affected projects, including privacy-focused engines and kid-safe search tools. The discussion highlighted the difficulty and cost of building a proprietary index, with one user sharing their project of indexing 34 million documents on bare metal, while others noted the challenges of scraping the web without being blocked.

The conversation also touched on the broader implications for competition. Some connected this to recent antitrust rulings, speculating that Google might be restricting API access to make it harder for competitors like Kagi to operate, while others noted that competitors like Bing have also shut down their API products, leaving few options for programmatic search. A recurring sentiment was the danger of relying on third-party platforms, described as a "loan that can be called in," prompting discussions on the necessity of owning core infrastructure despite the high barrier to entry.

---

## [White House Posts Digitally Altered Image of Woman Arrested After ICE Protest](https://www.theguardian.com/us-news/2026/jan/22/white-house-ice-protest-arrest-altered-image)
**Score:** 166 | **Comments:** 27 | **ID:** 46725268

> **Article:** The Guardian article reports that the White House digitally altered an image of a woman arrested after an ICE protest to make her appear more menacing. When questioned about the alteration, the White House did not deny it; instead, the Deputy Communications Director responded via a tweet mocking the defense of alleged criminals and stating that "the memes will continue." The article notes that the arrest occurred at a protest against an ICE official who also serves as a church pastor.
>
> **Discussion:** The Hacker News discussion focuses on the implications of government use of AI-generated or altered media and the erosion of institutional norms. Commenters expressed alarm that the White House is fabricating images and openly admitting to it, viewing this as evidence of a breakdown in social order and disregard for due process. Several users drew parallels to political instability in other countries, such as South Africa during the "State Capture" years.

A significant portion of the debate centered on the technical and ethical aspects of the image manipulation. While some speculated on which AI provider was used (with Palantir mentioned as a guess), others argued that the specific tool is less important than the intent. The discussion highlighted a shift from the difficulty of Photoshop manipulation to the ease of AI generation, though one commenter countered that the real issue is not the accessibility of the tools, but the character of those wielding them in power.

There was also minor debate regarding the separation of church and state, with one user arguing that a government employee's private religious activities shouldn't be conflated with government acts, though the general consensus remained critical of the administration's actions. The tone of the official White House response was described as mimicking Trump's tweet style, further fueling the discussion on the normalization of "bonkers" behavior in politics.

---

## [Talking to LLMs has improved my thinking](https://philipotoole.com/why-talking-to-llms-has-improved-my-thinking/)
**Score:** 136 | **Comments:** 127 | **ID:** 46728197

> **Article:** The article "Talking to LLMs has improved my thinking" argues that interacting with Large Language Models (LLMs) acts as a catalyst for clearer thought. The author describes using LLMs not as a source of truth, but as a tool to articulate vague, half-formed ideas. By prompting the model with these nascent thoughts, the author is forced to refine them, resulting in a "clean verbal form" that clarifies their own understanding. The piece posits that this process helps surface tacit knowledge and structures internal monologues more effectively than thinking in isolation.
>
> **Discussion:** The Hacker News discussion largely validates the author's experience, with many users sharing that LLMs serve as effective "intellectual sparring partners" or "sounding boards" for crystallizing vague concepts. Several commenters compared the current utility of LLMs to the early days of Wikipedia, noting that they lower the barrier to entry for exploring complex topics and making "boring" subjects more approachable.

However, the conversation introduces significant nuance and caution. A primary counterpoint is the risk of "polished generic framings," where the LLM flattens the originality and uniqueness of a user's idea into a superficially coherent but ultimately shallow summary. Users warned that accepting these generic outputs could lead to a loss of intellectual depth.

The discussion also touched on the concept of "cognitive debt," contrasting the benefits of using LLMs for structured tasks (like coding or teaching) with the potential harm of using them for personal decision-making. The consensus leaned toward LLMs being powerful tools for exploration and debate, provided the user maintains critical oversight and engages in a back-and-forth dialogue rather than passively accepting the first output.

---

## [Replacing Protobuf with Rust to go 5 times faster](https://pgdog.dev/blog/replace-protobuf-with-rust)
**Score:** 136 | **Comments:** 94 | **ID:** 46730214

> **Article:** The article describes a performance optimization where the author replaced Protocol Buffers (Protobuf) with a custom Rust implementation for a Postgres query parser. The author argues that by avoiding the overhead of generic Protobuf serialization and deserialization, they achieved a 5x speed improvement. The new approach uses a more direct, memory-efficient method tailored specifically to their use case, effectively bypassing the need for a generic data interchange format by sharing memory directly.
>
> **Discussion:** The Hacker News discussion is largely critical of the article's premise and title, focusing on the misleading attribution of the performance gain to Rust rather than the fundamental change in architecture. The consensus is that the speedup comes from removing serialization overhead and using a more direct data access method, a change that could yield similar benefits in other languages.

Key points of discussion include:
*   **Misleading Attribution:** Many commenters argue that the performance improvement is due to replacing a generic serialization format (Protobuf) with a native, optimized implementation, not specifically because of Rust. They suggest the "Rust" in the title is clickbait to attract attention.
*   **Architectural Trade-offs:** The core change was moving from a serialized data format to direct memory sharing between processes. Commenters point out that while this is faster, it introduces significant complexity and instability, as managing shared memory is difficult and error-prone. This is why kernels exist to abstract these challenges.
*   **Context of Protobuf:** Defenders of Protobuf noted that its primary benefits are cross-language compatibility, ease of use via code generation, and maintainability, not raw speed. They argue that for most companies, these features outweigh the performance costs, and that the comparison is unfair as it ignores Protobuf's intended purpose.
*   **Alternative Technologies:** The discussion briefly touched on alternatives like FlatBuffers and Cap'n Proto, which are designed for zero-copy or direct memory access and are often faster than Protobuf out-of-the-box. However, some noted these can be less ergonomic or have their own trade-offs.
*   **Language-Specific Details:** A side thread discussed Rust's capabilities regarding recursion and tail-call optimization (TCO), with users noting that explicit TCO is an experimental feature in Rust and not yet stable.

---

## [Recent discoveries on the acquisition of the highest levels of human performance](https://www.science.org/doi/abs/10.1126/science.adt7790)
**Score:** 135 | **Comments:** 71 | **ID:** 46722853

> **Article:** The article, published in *Science*, analyzes the developmental paths of individuals who achieve the highest levels of performance (e.g., Nobel laureates, world-class athletes, chess grandmasters). The core finding is that the path to elite achievement is often counterintuitive and non-linear. The study reveals that top performers as adults are frequently not the same individuals who were top performers as children; in many domains, nearly 90% of the top youth performers do not become the top adult performers. Instead, many of the eventual world-class achievers demonstrated lower performance than their peers during their early years. The research suggests that a period of diverse exploration and lower-intensity practice, followed by a later phase of focused, deliberate practice, is a more common path to sustained, exceptional achievement than early, intense specialization.
>
> **Discussion:** The Hacker News discussion centered on validating, contextualizing, and challenging the paper's findings. A dominant theme was the immediate identification of the research's conclusions as not new, with multiple commenters pointing to David Epstein's book *Range* and statistical concepts like Berkson's Paradox and regression to the mean as existing frameworks for the same idea. Many argued that the observed "discrepancy" between early and late achievers is a statistical artifact of selecting for extreme success; if you only look at the very top, you'll naturally see different people at different life stages due to random variation and the fact that early specialization doesn't guarantee long-term success.

Personal anecdotes were widely shared, often contrasting two archetypes: the "grinder" child who excelled early but burned out or plateaued, and the "dabbler" adult who leveraged broad interests into later, breakthrough success. This led to a debate on the relative power of innate ability versus disciplined effort, with some commenters noting that raw cleverness is insufficient without the later development of work ethic.

A significant sub-thread debated the role of early specialization, particularly in sports. While some data from hockey suggested that top youth performers do often become professional athletes, others countered that this is a high-pressure, high-attrition path that filters out most early standouts. The discussion also touched on systemic issues, noting that modern educational and career systems are often biased toward rewarding early specialization, potentially filtering out the "generalists" who might achieve greater later success. Finally, there was a brief but notable side-discussion debunking the popular social media conflation of ADHD with "hyperfocus" and high performance, with commenters emphasizing that ADHD is a clinical obstacle, not a superpower for achievement.

---

## [Improving the usability of C libraries in Swift](https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/)
**Score:** 135 | **Comments:** 32 | **ID:** 46726526

> **Article:** The Swift.org blog post details new features in Swift 6.2 that significantly improve the usability of C libraries. The article explains how to use the `import` declaration to automatically generate Swift interfaces for C modules, and how to use API Notes to customize the mapping of C types and functions to more idiomatic Swift equivalents. This allows developers to interact with C libraries using native Swift syntax (e.g., using Swift's `String` instead of C's `char*`) without writing manual wrapper code, streamlining the process of integrating low-level C code into Swift applications.
>
> **Discussion:** The Hacker News discussion is largely positive regarding the technical improvements to C interoperability in Swift, though it branches into broader opinions about the language and Apple's ecosystem.

Key points include:
*   **Technical Appreciation:** Several users expressed enthusiasm for the specific features discussed, particularly API Notes, which many were previously unaware of. The integration of Swift Package Manager with C, C++, and Objective-C was also praised as being smoother than in many other ecosystems.
*   **Swift's Future and Strategy:** A recurring theme is that Swift's steady improvements in tooling and interoperability with existing codebases (like C/C++) are a major driver for its adoption. Users see this as a pragmatic strategy by Apple, which needs to maintain its vast low-level system code in C/C++ while providing a modern language for application developers.
*   **Criticisms and Concerns:** While the interop is praised, some users voiced concerns about the language's complexity. Specific pain points mentioned include the steep learning curve of Swift Concurrency, the proliferation of complex language features, and the perceived verbosity and difficulty of using pointers in Swift (e.g., the distinction between `UnsafeMutablePointer`, `UnsafeRawPointer`, etc.). Some also questioned whether these improvements are primarily for Apple's benefit in legal or business contexts rather than for the open-source community.
*   **Alternatives:** A few commenters expressed a preference for other languages for systems programming, citing Objective-C's simplicity or considering Zig, Rust, or C++ over Swift for low-level work.

---

