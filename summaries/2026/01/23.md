# Hacker News Summary - 2026-01-23

## [Bugs Apple Loves](https://www.bugsappleloves.com)
**Score:** 876 | **Comments:** 425 | **ID:** 46727587

> **Article:** The article is a single-page website titled "Bugs Apple Loves," which lists a collection of persistent, long-standing bugs and usability issues across Apple's software ecosystem. The list covers a wide range of problems, including Spotlight search freezing, broken Mail search, issues with Apple ID account creation, Bluetooth audio stuttering on macOS, and problems with contact syncing. The site's tone is one of exasperated humor, highlighting bugs that users have endured for years despite multiple OS updates.
>
> **Discussion:** The Hacker News discussion is largely positive, with commenters appreciating the site's relatable and humorous list of grievances. The conversation quickly pivots from the website's design—which one user noted as "obvious AI-generated"—to the root causes of Apple's software quality issues.

A central theme is that the problem isn't a lack of engineering talent but a matter of corporate priorities. As one commenter explained, large tech companies have plenty of engineers, but they are often directed to work on new "10X" features rather than paying down technical debt. This is contrasted with Apple's past reputation for polish, exemplified by the "Snow Leopard" update which was focused solely on bug fixes—a practice now considered "unthinkable."

Several users shared their own frustrating personal experiences, validating the site's claims. These anecdotes included difficulties creating Apple developer accounts (especially with custom email domains), the need to keep the Gmail app on an iPhone for reliable mail search, and specific macOS bugs like Apple Watch unlock failures and Bluetooth stuttering.

Finally, the discussion touched on the internal processes at Apple. One commenter described a "ceremonial" process where bugs are not fixed but simply rescheduled for a future release, creating a perpetual backlog. This was linked to an xkcd comic about software breaking when an OS is updated, reinforcing the idea that fixing old bugs is often deprioritized in favor of new development.

---

## [AI Usage Policy](https://github.com/ghostty-org/ghostty/blob/main/AI_POLICY.md)
**Score:** 447 | **Comments:** 234 | **ID:** 46730504

> **Article:** The linked document is an AI Usage Policy for the open-source terminal emulator project Ghostty. The policy explicitly allows and encourages maintainers to use AI tools for development. However, it imposes strict restrictions on external contributors. AI-generated pull requests are not accepted unless the contributor has fully verified the code with human testing and can explain the changes. The policy's rationale, as explained by the project lead Mitchellh, is not an anti-AI stance but a response to a flood of low-quality, untested, and often incorrect AI-generated code submissions ("slop") that are wasting maintainer time. The goal is to ensure that all contributions are thoroughly understood and validated by the human submitting them.
>
> **Discussion:** The Hacker News discussion centered on the practical and philosophical challenges of integrating AI into software development, particularly in open-source contexts. The general sentiment was that the Ghostty policy is a reasonable and balanced response to a real problem.

Key themes included:
*   **The "Slop" Problem:** Multiple commenters confirmed the phenomenon of receiving low-quality, spammy, and often nonsensical AI-generated pull requests. This was described as a significant drain on maintainer resources, forcing projects to either implement strict policies or, in some cases, pause external contributions entirely. The lack of shame from contributors submitting such code was noted as surprising.
*   **Human Accountability:** A strong consensus emerged that the ultimate responsibility for any code lies with the human contributor, not the tool. Several commenters argued that if a developer (using AI or not) consistently produces poor-quality work, it's a professional failing. In a professional team setting, AI tools were seen as productivity enhancers that don't compromise quality when used by competent developers who rigorously review the output.
*   **Trust and Open Source:** The rise of AI-generated contributions was seen as eroding trust in open-source projects, especially from new or unknown contributors. Maintainers are becoming more vigilant, raising the barrier for entry.
*   **Philosophical and Legal Concerns:** One commenter raised the unresolved legal question of AI-generated code's copyright status, noting it could pose a future risk for projects. Another commenter engaged in a more philosophical debate, questioning the end goal of AI tools if they are not meant to replace human thinking and verification, and pointing out the fuzzy line between current capabilities and future promises.
*   **Practical Solutions:** Beyond policy, suggestions included donating AI credits to projects instead of submitting raw AI output, with the assumption that maintainers would be better at steering the tools. The difficulty of properly logging and redacting AI coding sessions for transparency was also discussed.

---

## [Proton Spam and the AI Consent Problem](https://dbushell.com/2026/01/22/proton-spam/)
**Score:** 420 | **Comments:** 263 | **ID:** 46729368

> **Article:** The article "Proton Spam and the AI Consent Problem" by David Bushell expresses frustration with receiving unsolicited marketing emails from Proton, a company known for its privacy-focused services. Bushell highlights the irony of a privacy company using aggressive marketing tactics. He connects this to a broader trend in the tech industry where AI features are being aggressively pushed onto users without their consent. The core argument is that the "AI industry is built upon a common principle of non-consent," treating user data and attention as resources to be exploited rather than respecting user choice. Bushell sees this as a symptom of a tech bubble, where growth metrics are prioritized over user experience and genuine value.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, with many users expressing frustration at the pervasive nature of unsolicited marketing and AI features across the tech industry. A dominant theme is the violation of user consent. Commenters argue that companies frequently ignore user preferences, with one noting that services often add new marketing options that are "defaulted to ‘enabled’" even when other preferences are disabled. This is seen as a systemic issue, with one user suggesting it's a "data privacy + lack of consequences problem," noting that in regions with strong regulations like the EU, companies are more cautious due to the risk of fines.

Another major point of discussion is the poor quality of many AI implementations. One commenter points out that AI features often "create a lot of really bad results," citing examples like Shopify's code assistant and Amazon's product Q&A. This leads to questions about the logic behind deploying such flawed products, reinforcing the "bubble" theory mentioned in the article.

The specific case of Proton drew significant commentary. Several users expressed that Proton's marketing tactics were making them consider switching providers, with one user revealing that Proton was the only source of spam on a dedicated honeypot email address. The discussion also touched on the perceived hypocrisy of privacy-focused companies like Proton and DuckDuckGo embracing AI, with some commenters feeling these companies are abandoning their core principles. However, a counterpoint was offered, clarifying that Proton's current AI integrations are optional, opt-in, and do not involve training on user data.

Finally, the conversation explored broader consent issues. One commenter shared a well-known joke about tech companies and consent, while another highlighted Signal's persistent reminders for contact access as another example of a service that doesn't take "no" for an answer. The discussion also included a practical solution for tracking data misuse: using unique email addresses for different services. A Proton CTO's comment was cited, suggesting the marketing email was the result of a technical error rather than a deliberate policy.

---

## [European Alternatives](https://european-alternatives.eu)
**Score:** 410 | **Comments:** 183 | **ID:** 46731976

> **Article:** The article links to "European Alternatives," a website that catalogs European-made alternatives to popular digital services and products. The site categorizes options for cloud computing, hosting, domain registration, email, VPNs, messaging, and other software, aiming to provide a resource for users seeking services based in Europe.
>
> **Discussion:** The Hacker News discussion centers on the value of the resource, the gaps in its coverage, and the broader geopolitical motivations behind seeking European alternatives.

Users generally praised the site as a valuable and growing resource, noting that the list of alternatives has improved significantly since a previous submission in 2021. However, several commenters pointed out missing categories, such as operating systems, programming language toolchains, and hardware vendors. The conversation around these omissions highlighted a consensus that open-source software largely solves the need for non-US software tools, while hardware remains a significant challenge, often relying on Chinese manufacturing.

The discussion quickly expanded to address the underlying "why" of the list. A key theme was the tension between globalism and regionalism. One commenter lamented the trend toward "nationalistic" fragmentation, arguing for global, interoperable alternatives. This sparked a counter-debate where others defended the move toward European alternatives as a necessary response to US overreach (e.g., FISA orders, NSA surveillance) and geopolitical instability, rather than simple nationalism. They argued that competition is healthy and that having regionally-rooted services is a pragmatic necessity in the current climate.

Practical experiences with European services were mixed. While some users recommended providers like Hetzner and OVH, others cited poor customer service compared to US companies or pointed out specific technical failures, such as a European cloud provider's inability to handle non-ASCII characters in addresses.

Finally, the conversation touched on economic viability. One commenter argued that European tech salaries, which are typically half of those in the US, are unsustainable for building a competitive tech sector. A rebuttal suggested that increased EU investment and protectionist policies could foster growth, citing examples like Airbus succeeding with European salaries. The discussion concluded with a personal reflection on how recent global political shifts have eroded trust in international cooperation, forcing a pragmatic re-evaluation of national and regional interests.

---

## [I built a light that reacts to radio waves [video]](https://www.youtube.com/watch?v=moBCOEiqiPs)
**Score:** 410 | **Comments:** 94 | **ID:** 46728808

> **Article:** The article links to a YouTube video demonstrating a DIY project: a physical light device that visually reacts to ambient radio waves (likely Wi-Fi and other RF signals). The creator, "tzvc," built a system that captures RF signals and translates their intensity into the brightness of a light, effectively making invisible radio frequency energy visible. The video shows the device responding to the presence of a smartphone and shifting its patterns over the course of a day.
>
> **Discussion:** The community response is overwhelmingly positive, with commenters describing the project as "cool," "fantastic," and "mesmerizing." The discussion centers on the project's potential to visualize the invisible "sea of radiation" we live in, with several users expressing a desire to see more extreme use cases, such as taking the device to remote locations or using it to track approaching people or drones.

Technical and conceptual extensions were a major theme. One user asked about the specific calibration used to convert signal strength (dB) to light intensity (gamma), while others envisioned more advanced iterations. A prominent idea was the creation of a directional camera system that could map specific frequencies to colors and overlay a 3D visualization of radio waves onto the physical world. Commenters noted that similar technologies exist, referencing a video on RF direction finding and noting that Philips Hue bulbs already use similar RF motion detection principles. The conversation also touched on the real-world implications of this technology, with one user recalling its use in military contexts (specifically detecting drone signals in Ukraine) and another expressing a desire to "see" Wi-Fi signals as clearly as visible light.

---

## [Capital One to acquire Brex for $5.15B](https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/)
**Score:** 373 | **Comments:** 305 | **ID:** 46725288

> **Article:** Capital One is acquiring the fintech company Brex for $5.15 billion. The deal represents a significant discount from Brex's peak valuation of $12.3 billion in 2022, which occurred during the era of near-zero interest rates (ZIRP). While Brex has reportedly grown its revenue substantially since then, the acquisition price reflects a market correction where fintech valuations are trending back toward fundamental metrics like discounted cash flows rather than speculative growth.
>
> **Discussion:** The Hacker News discussion centers on the financial mechanics of the acquisition and the starkly different outcomes for investors versus employees.

**Valuation and Market Context**
Commentators view the $5.15 billion price tag as a "bad deal" for Brex’s later-stage investors, who invested at the inflated $12.3 billion valuation. However, others argue it is a fair price given the end of the ZIRP (Zero Interest Rate Policy) era. The consensus is that fintech companies that thrived on cheap credit and speculative growth are now facing a harsh reality check based on fundamentals.

**The "Liquidation Preference" Debate**
A major thread of the discussion focuses on "liquidation preferences," which determine the payout order during an acquisition.
*   **Investor Protection:** Several users explained that because Brex raised massive amounts of capital, late-stage investors likely have "1x to 2x" liquidation preferences. This means they get paid back their investment first (and sometimes multiple times) before common stockholders see a dime.
*   **Employee Impact:** There is a strong sentiment that employees holding common stock options or RSUs are "fucked" (to use the commenters' terminology). If the total preference stack exceeds the acquisition price, common stockholders receive nothing. Even if the price exceeds the preferences, employees who joined at the peak $12 billion valuation have "underwater" options and will see little to no financial return.
*   **Founder Outcome:** While the CEO, Pedro Franceschi, likely secured a retention package and may have sold shares on secondary markets, commenters note he has traded independence for a role as a division president at Capital One, losing the freedom of being a founder.

**Strategic Outlook**
Users debated Capital One's strategy. Some noted that Brex’s growth rate (cited as 50% YoY) makes the 7x revenue multiple a bargain for Capital One. However, skepticism remains regarding whether that growth is sustainable or if Brex lost momentum to competitors like Ramp. There is also speculation about post-acquisition integration, with some fearing layoffs while others believe Capital One tends to keep acquired units autonomous rather than dismantling them.

---

## [Scaling PostgreSQL to power 800M ChatGPT users](https://openai.com/index/scaling-postgresql/)
**Score:** 280 | **Comments:** 122 | **ID:** 46725300

> **Article:** OpenAI published an engineering blog post detailing how they scaled PostgreSQL to support over 800 million ChatGPT users. The post explains that while PostgreSQL scaled well for read-heavy workloads using read replicas, it faced challenges with write-heavy traffic due to the inefficiencies of its Multi-Version Concurrency Control (MVCC) implementation. To address this, they offloaded write-heavy, shardable workloads to Azure CosmosDB (a NoSQL database) while keeping PostgreSQL for read-heavy workloads. They also implemented aggressive read/write splitting, added nearly 50 read replicas, and optimized queries to handle the massive scale.
>
> **Discussion:** The Hacker News discussion focused on the technical trade-offs of PostgreSQL versus NoSQL databases, the validity of OpenAI's architectural choices, and the quality of the blog post itself.

Several users debated the necessity of leaving PostgreSQL for write-heavy workloads. Commenters pointed out that write amplification caused by MVCC is a known limitation, and suggested that LSM-tree-based databases (like TiDB or RocksDB) are inherently better for high-volume writes. A notable point of confusion was why OpenAI didn't leverage Rockset, a company they acquired that specializes in high-performance indexing, to solve these write issues internally.

There was also a discussion regarding PostgreSQL's native sharding capabilities. While the article implied that sharding required moving away from Postgres, some users argued that Postgres supports sharding via partitioning and Foreign Data Wrappers (FDWs), suggesting OpenAI's architecture might have been simplified.

Other threads touched on the business implications, with users joking about Microsoft being "bested" by open-source software on their own cloud platform, and skepticism regarding OpenAI's financial ability to hire the deep experts required for complex database scaling. Finally, some commenters criticized the blog post itself, finding it generic, repetitive, or lacking in novel technical depth compared to standard scaling practices.

---

## [Microsoft gave FBI set of BitLocker encryption keys to unlock suspects' laptops](https://techcrunch.com/2026/01/23/microsoft-gave-fbi-a-set-of-bitlocker-encryption-keys-to-unlock-suspects-laptops-reports/)
**Score:** 273 | **Comments:** 200 | **ID:** 46735545

> **Article:** A TechCrunch article reports that Microsoft provided the FBI with BitLocker encryption keys to unlock suspects' laptops. The keys were reportedly stored in the suspects' Microsoft accounts as part of the default backup feature in Windows 11. The article highlights that Microsoft receives an average of 20 such requests annually from law enforcement.
>
> **Discussion:** The Hacker News discussion centers on the legal, technical, and privacy implications of Microsoft's actions. Many commenters argue that the headline is misleading, noting that Microsoft is legally compelled to provide data when presented with a valid warrant and has no choice in the matter. The debate frequently shifts to the technical architecture of BitLocker, specifically the default setting in Windows 11 that backs up encryption keys to a user's Microsoft account. While some view this as a necessary safety net for average users who might otherwise lose access to their data, privacy advocates and power users criticize it as a fundamental security flaw that creates a centralized point of failure accessible by governments. The conversation also touches on broader themes, including the trade-off between user convenience and privacy, the reliability of journalism in the tech space, and the recommendation for technical users to switch to open-source alternatives like Linux or VeraCrypt to maintain full control over their encryption keys.

---

## [Booting from a vinyl record (2020)](https://boginjr.com/it/sw/dev/vinyl-boot/)
**Score:** 229 | **Comments:** 73 | **ID:** 46730885

> **Article:** The article details a project to boot a PC from a vinyl record. The author explains how they encoded bootable data as audio waveforms onto a record, which can then be played back into a PC's legacy cassette interface (or a modern equivalent via an audio line-in). The process involves converting binary data into audio tones that the BIOS can interpret as a boot source, effectively treating the record player as a floppy disk drive. The project is presented as a proof-of-concept, demonstrating the low-level flexibility of legacy hardware interfaces and the persistence of analog media in digital contexts.
>
> **Discussion:** The Hacker News community reacted with enthusiasm to the technical novelty of the project, frequently expressing delight at its "doability" and the cleverness of repurposing vinyl records for computing tasks. The discussion quickly broadened to include historical parallels, with users recalling similar analog data distribution methods such as software on flexi-discs (magazine inserts), audio cassettes (for systems like the Acorn Electron and Commodore 64), and even radio broadcasts for Atari and ZX Spectrum computers in Eastern Europe during the communist era.

Several commenters expressed surprise at the existence of the PC's built-in "cassette interface," noting that it was a short-lived feature on early IBM PCs (PC/XT era) that was removed to make room for more ISA slots. This sparked a nostalgic appreciation for the transitional period when PCs still resembled 8-bit microcomputers, featuring BASIC in ROM and TV connectivity.

Technical tangents included speculation on other legacy hardware interfaces, such as using SCSI scanners as boot devices, and the tactile experience of early storage media. Users noted that older storage methods (floppies, hard drives) were "noisy" and physically tangible, allowing users to diagnose issues by sound—a stark contrast to modern, silent abstractions. Finally, there was a minor sub-thread regarding the article's cookie policy and technical methods to bypass it to view the embedded video.

---

## [Radicle: The Sovereign Forge](https://radicle.xyz)
**Score:** 219 | **Comments:** 109 | **ID:** 46732213

> **Article:** The article introduces Radicle, a peer-to-peer (P2P) code collaboration protocol designed to be a "sovereign forge." It aims to provide a decentralized alternative to centralized platforms like GitHub and GitLab. Radicle's architecture is local-first; users run a node that stores repositories and data locally, synchronizing with a P2P gossip network only when desired. It uses self-certifying identities (based on public-key cryptography) to verify repository authenticity without relying on trusted servers. Key features include offline functionality, performance, and the storage of issues and patches as signed Git objects (COBs) that replicate with the repository itself.
>
> **Discussion:** The discussion centered on comparing Radicle's architecture to other decentralized projects, exploring its trust model, and addressing the practical challenges of P2P systems.

A primary theme was architectural comparison. Users contrasted Radicle's purely P2P, local-first model with Tangled (which is built on the federated AT Protocol and relies on servers called "knots") and Forgejo (which uses federated servers via ActivityPub, similar to Mastodon). Commenters emphasized that Radicle has no central servers or client-server distinction; every node is equal, and data is stored and processed locally, making it fully functional offline. In contrast, Tangled and Forgejo rely on server instances and network round-trips for core functionality.

Another major topic was the trust and identity model. Users discussed how Radicle solves the problem of verifying repository authenticity in a P2P network by using stable, cryptographically signed identities. While one commenter worried this merely shifts the trust problem to a PKI-like system, others explained that trust is established socially, similar to how trust is built in real life or on centralized platforms. The self-certifying nature of identities allows users to verify the source of a repository even when cloning from untrusted parties.

Finally, the conversation addressed the inherent challenges of immutable, P2P systems. Participants raised concerns about handling mistakes, such as accidentally posting sensitive information, and the legal risks of hosting content that may become illegal. The Radicle developer (endiangroup) acknowledged these issues, stating they are actively working on making defaults safer and exploring ways to enable content revocation at the network level. A user noted that running a node over Tor is a viable option for privacy-conscious users.

---

## [Why medieval city-builder video games are historically inaccurate (2020)](https://www.leidenmedievalistsblog.nl/articles/why-medieval-city-builder-video-games-are-historically-inaccurate)
**Score:** 218 | **Comments:** 141 | **ID:** 46726857

> **Article:** The article from the Leiden Medievalists Blog argues that popular medieval city-builder games like Age of Empires and Sim City are historically inaccurate because they prioritize gameplay mechanics over historical reality. Key inaccuracies include the prevalence of straight, planned roads (where medieval streets were organic and winding), the oversimplification of food production (ignoring the massive 29:1 farmer-to-non-farmer ratio required for subsistence), and the lack of color in clothing (medieval people used vibrant dyes whenever possible). The author notes that while games like *Banished* and *Manor Lords* attempt to capture the slow struggle of survival, most games are designed to provide a satisfying sense of progression and control that real, chaotic medieval life lacked.
>
> **Discussion:** The discussion largely agrees with the article's premise but explores the necessary trade-offs between historical accuracy and entertainment. A central theme is that "fun" must take precedence over realism; commenters point out that games like *Age of Empires* are balanced for competition, not simulation, and that realistic mechanics—such as managing crop irrigation step-by-step or suffering random village wipes from plagues—would be tedious and frustrating rather than enjoyable. There is a consensus that players often reject strict realism because it breaks immersion; for example, one user notes that straight roads feel like a "glitch" to players expecting the romanticized, winding streets of fantasy medieval settings.

Several commenters highlighted specific examples of historical divergence. One thread corrects the common visual trope of earth-toned clothing, arguing that medieval people loved color and dye. Another user, *legitster*, emphasized the massive economic disparity of the era (the 29:1 farmer ratio) and praised *Banished* and *Manor Lords* for attempting to simulate the sheer effort of subsistence living, with *Manor Lords* specifically noted for including women's work like gardening and weaving. There was also a humorous counterpoint that since there were no computers in the Middle Ages, all computer games are inherently inaccurate. Ultimately, the community concluded that games are simulations designed to evoke a "feeling" of the past rather than a data-accurate representation, and that stripping away the misery of history is essential for a satisfying gameplay loop.

---

## [What has Docker become?](https://tuananh.net/2026/01/20/what-has-docker-become/)
**Score:** 209 | **Comments:** 227 | **ID:** 46731748

> **Article:** The article "What has Docker become?" critiques Docker's evolution from a developer-focused tool into a complex, monetized platform. It argues that Docker has strayed from its original mission of simplifying developer workflows, instead becoming bloated and difficult to use. The piece highlights the frustration with Docker Desktop's performance and reliability, particularly on Windows and Mac, and questions the company's strategic pivots, such as abandoning Docker Swarm for Kubernetes and then attempting to reintroduce Swarm-like features. The author suggests that Docker's core technology has become commoditized infrastructure, making it difficult to monetize, and that the company is struggling to find a sustainable business model beyond its core offering.
>
> **Discussion:** The Hacker News discussion largely validates the article's criticisms, focusing on three main themes: the poor user experience of Docker Desktop, Docker's strategic missteps, and the broader challenge of monetizing open-source infrastructure.

A significant portion of the comments express deep frustration with Docker Desktop, particularly on non-Linux systems. Users describe it as "fickle," "opaque," and prone to errors that require a full reinstall to fix. This sentiment is so strong that many have switched to alternatives like OrbStack, which is praised for being a faster, more reliable, and simpler solution, reportedly developed by a single person. This is seen as a clear example of a large company being outmaneuvered by a smaller, more focused competitor on user experience.

The discussion also delves into Docker's business strategy. Commenters point out that Docker made a critical error by abandoning its own orchestration tool, Swarm, in favor of Kubernetes, only to later reintroduce Swarm-like features. This is viewed as a misstep that ceded the orchestration market and alienated users who found Swarm sufficient for their needs. There's a consensus that Kubernetes is overkill for many use cases. The conversation expands to the broader economic problem of monetizing open-source software. Commenters debate how companies like Docker can sustain themselves when their technology becomes foundational infrastructure that cloud providers (like AWS and Google) offer as a service without contributing back. Several "Fair Source" or alternative licensing models are proposed as potential solutions to prevent large corporations from exploiting free labor.

Finally, there are practical discussions about Docker's current relevance. While some users defend Docker's value (e.g., enterprise support, comprehensive tooling, image distribution), others express a preference for alternatives like Podman or Nix for specific tasks. The conversation also touches on the experience with Dev Containers, with mixed reports on their performance in VS Code.

---

## [Microsoft mishandling example.com](https://tinyapps.org/blog/microsoft-mishandling-example-com.html)
**Score:** 202 | **Comments:** 72 | **ID:** 46731996

> **Article:** The article details a significant security flaw in Microsoft's Autodiscover protocol used by Outlook. The system is designed to automatically configure email clients by querying specific subdomains (like `autodiscover.example.com`). However, if the subdomain is missing, the protocol has a fallback mechanism that attempts to resolve broader domains, eventually trying `autodiscover.com`.

Because Microsoft failed to secure the `autodiscover.com` domain, any email client configured for a domain lacking the specific subdomain (including the IANA-reserved `example.com`) would send login credentials to the server registered at `autodiscover.com`. The article reveals that these credentials were routed to Sumitomo Electric Industries (sei.co.jp), exposing sensitive authentication data due to this protocol design flaw.
>
> **Discussion:** The Hacker News discussion largely agrees that the vulnerability highlights systemic incompetence at Microsoft, with users citing the mishandling of the Xbox and Office brands as evidence of broader organizational issues. The technical consensus is that the Autodiscover protocol is fundamentally flawed; commenters describe it as "braindead" for prioritizing domain availability over security, effectively creating a mechanism that leaks credentials to whoever registers the fallback domains.

Several users expressed alarm that Outlook sends full credentials (username and password) to Microsoft's servers during the setup process, viewing this as a violation of privacy expectations, though others noted this is standard practice for SaaS email clients. The conversation also touched on Microsoft's historical poor practices, specifically their former recommendation to use `.local` for Active Directory—a reserved domain that causes conflicts with Linux and multicast DNS.

There was a debate regarding the use of IANA-reserved domains (like `example.com`) in development environments. While some argued these are safe "safety nets," others warned that relying on them is risky because commercial entities (like Donuts) can and do purchase TLDs, potentially turning reserved namespaces into active threats.

---

## [Updates to our web search products and  Programmable Search Engine capabilities](https://programmablesearchengine.googleblog.com/2026/01/updates-to-our-web-search-products.html)
**Score:** 188 | **Comments:** 162 | **ID:** 46730436

> **Article:** Google announced it is discontinuing the "search the entire web" capability for its Programmable Search Engine (formerly Custom Search) platform. New engines will be limited to searching a maximum of 50 specific domains, while existing full-web engines will lose this functionality by January 1, 2027. Google is directing users requiring broader search capabilities toward its enterprise solutions, such as Vertex AI Search, which lacks transparent public pricing and requires direct contact.
>
> **Discussion:** The Hacker News community reacted with concern and criticism, interpreting the move as Google effectively closing off its search index to indie developers and niche search engines. Users noted that this shift forces small projects behind enterprise gates, eliminating a previously accessible (and monetizable via AdSense) tool for building custom search experiences. The discussion highlighted the fragility of relying on third-party infrastructure, with one user sharing their success in building a 34-million-document index on bare metal as a counter-strategy.

Several threads explored the broader context of search competition. Commenters linked this decision to recent EU rulings requiring Google to provide search index data at marginal cost, speculating that Google might be simplifying its API offerings to comply while making direct access more inconvenient. There was also mention of European alternatives like Ecosia and Qwant building their own index, though skepticism remained about their ability to catch up to Google's quality. Alternatives like Bing were noted to be expensive or shutting down their own API products, leaving few viable options for programmatic search access. The consensus among many commenters was a warning against building core product features on top of volatile third-party platforms.

---

## [White House Posts Digitally Altered Image of Woman Arrested After ICE Protest](https://www.theguardian.com/us-news/2026/jan/22/white-house-ice-protest-arrest-altered-image)
**Score:** 175 | **Comments:** 28 | **ID:** 46725268

> **Article:** The Guardian article reports that the White House digitally altered an image of a woman arrested after an ICE protest, making her tattoos appear more prominent. When asked about the alteration, the White House did not deny it but instead posted a message on X (formerly Twitter) from Deputy Communications Director Kaelan Dorr. The response mocked those defending "perpetrators of heinous crimes" and stated that "the memes will continue." The article highlights the use of generative AI by the government to manipulate contemporaneous imagery and the administration's dismissive attitude toward questions regarding the authenticity of official communications.
>
> **Discussion:** The Hacker News discussion focused on the implications of government-issued disinformation and the lowering of barriers for digital manipulation. Commenters expressed alarm that the White House was fabricating images and responding to journalistic inquiries with dismissive social media posts rather than official statements. Several users drew parallels to political instability in other nations, such as South Africa during the "State Capture" era, suggesting that the "craziness" reflects a breakdown in social order.

A secondary theme involved the accessibility of technology. One commenter drew an analogy to the early internet, noting that while graphic design used to require skill (like Photoshop), AI has given "any idiot" the power to manipulate images. Another user countered that the issue isn't the tool itself, but that the "idiots" wielding it have been elected to run the country. There was also speculation regarding which AI provider the government used, with one user guessing Palantir, though this was not substantiated.

---

## [Proof of Corn](https://proofofcorn.com/)
**Score:** 160 | **Comments:** 108 | **ID:** 46735511

> **Article:** The article "Proof of Corn" is a blog post detailing an experiment to see if an AI (specifically, an autonomous instance of Anthropic's Claude) can manage the entire process of growing and selling corn with minimal human intervention. The author outlines a plan for the AI to make decisions, hire human contractors for physical labor, manage finances, and interact with the real world to produce a crop, with the goal of proving whether an AI can "affect the physical world" beyond just writing code.
>
> **Discussion:** The Hacker News discussion is largely skeptical and analytical, focusing on the practical limitations and philosophical implications of the experiment. Key themes include:

*   **Human Labor vs. AI Autonomy:** Several commenters point out that the experiment relies heavily on human contractors to perform the actual physical labor, leading some to dismiss it as simply "hiring a person to grow corn" rather than a true demonstration of AI capability. One user noted the novelty of machines employing humans for physical tasks.
*   **Technical and Practical Challenges:** Users highlighted significant hurdles for an LLM, such as the lack of real-time sensor data (requiring manual input), "time bias" (overweighting recent events), and a tendency toward vagueness when lacking specific expertise. There is skepticism about whether the AI can handle the complexity and unpredictability of agriculture.
*   **Ethical and Social Concerns:** Commenters expressed unease about the potential for AI to spam businesses with requests and the dystopian prospect of AI micromanaging low-wage human labor. The experiment's impact on the public and the nature of the human-AI relationship were questioned.
*   **Skepticism of the Premise:** Many questioned the validity of the experiment, suggesting that as long as humans are executing the physical actions, the AI is merely a high-level project manager. Others compared it to the "paperclip maximizer" thought experiment, questioning the AI's underlying goals.

---

## [Replacing Protobuf with Rust to go 5 times faster](https://pgdog.dev/blog/replace-protobuf-with-rust)
**Score:** 158 | **Comments:** 107 | **ID:** 46730214

> **Article:** The article describes a project where the author replaced Protocol Buffers (Protobuf) with a custom Rust implementation to achieve a 5x performance increase. The context is a Postgres extension where data is passed between the database and a client application. The author argues that the overhead of serializing and deserializing Protobuf messages was a bottleneck. By defining a shared memory layout in Rust and using it for direct memory access (FFI) instead of Protobuf serialization, they eliminated the parsing step, resulting in the significant speedup. The post frames this as a move away from a generic serialization format to a purpose-built, high-performance solution.
>
> **Discussion:** The Hacker News discussion is largely critical of the article's framing, focusing on the technical and rhetorical aspects rather than the implementation details. The dominant theme is that the performance gain is misleadingly attributed to Rust, when it primarily stems from architectural changes.

Many commenters argue that the speedup is a result of replacing a generic serialization/deserialization process (Protobuf) with direct memory access (FFI), a fundamentally different approach. They contend that a similar performance improvement could be achieved in other languages like C++, Go, or even Lua by eliminating the serialization overhead. The choice of Rust is seen as incidental to the core optimization, which is essentially trading the safety and cross-language compatibility of Protobuf for raw speed via shared memory.

There is also significant skepticism about the practicality and stability of this approach. Commenters point out that sharing memory between processes is complex and can introduce instability, questioning if the trade-offs are worth it for anything other than niche, high-performance use cases. The discussion touches on the ergonomics of alternatives like Cap'n Proto, which offers zero-copy semantics but can be less user-friendly.

Finally, some users critique the headline as clickbait, suggesting a more accurate title would be "It's Faster to Copy Memory Directly than Send a Protobuf." They highlight that the article's conclusion is somewhat obvious: removing a computational step (serialization) will always be faster, but at the cost of losing the features and safety guarantees that Protobuf provides.

---

## [Show HN: Whosthere: A LAN discovery tool with a modern TUI, written in Go](https://github.com/ramonvermeulen/whosthere)
**Score:** 154 | **Comments:** 58 | **ID:** 46731432

> **Project:** Whosthere is a command-line LAN discovery tool written in Go, featuring a modern, keyboard-driven Terminal User Interface (TUI). It identifies devices on a local network using mDNS, SSDP, and ARP scanning without requiring root privileges. The tool includes optional port scanning, a daemon mode with an HTTP API, and YAML-based configuration for theming and behavior. It is currently supported on Linux and macOS.
>
> **Discussion:** The community response to Whosthere was largely positive, with users praising the tool's functionality and modern TUI design. Several users requested specific feature additions, such as reverse DNS lookups to identify devices by name, an option to log new device arrivals for use as a basic intrusion detection system (IDS), and command-line flags for specifying network interfaces and scan parameters.

Technical feedback focused on usability and compatibility. A common point of friction was macOS security settings, which block the unsigned binary by default; users shared workarounds like using the `xattr` command to clear the quarantine flag. Other users noted issues with the tool selecting the correct network interface on macOS and a dependency on X11 libraries that caused compilation failures on some Linux systems. The developer engaged actively in the comments, explaining design decisions—such as scan rate limits to protect network stability—and expressed interest in improving interface detection and CLI configurability.

---

## [Gas Town's Agent Patterns, Design Bottlenecks, and Vibecoding at Scale](https://maggieappleton.com/gastown)
**Score:** 142 | **Comments:** 155 | **ID:** 46734302

> **Article:** The article profiles "Gas Town," an experimental software project by Steve Yegge built entirely by AI agents using "vibecoding"—a method where the human architect directs agents without reviewing the code. The project is a large-scale, chaotic system designed to manage AI agents themselves, featuring unconventional concepts like "Ralph loops" (a recursive prompting technique) and "zoopmorphic" architecture diagrams. The author critiques the project's manic style and unreadable, AI-generated diagrams but acknowledges Yegge's ambition to push the boundaries of human-AI collaboration, despite the project's inefficiencies and "shitty, quarter-built plane" state.
>
> **Discussion:** The HN discussion is sharply divided, centering on whether Gas Town is a groundbreaking experiment or a chaotic mess lacking rigor. Skeptics argue that the project is essentially "satire" or a "sand castle," criticizing the unreadable AI-generated diagrams and the impracticality of "vibecoding" at scale, where no human understands the 225k+ lines of code. They contend that this approach lacks scientific rigor and fails to produce usable software.

Conversely, defenders view Yegge's work as necessary "cutting-edge" exploration that moves the field forward, even if it is messy. Some users drew parallels to historical shifts like assembly to compilers, suggesting that not reviewing code may become standard. A notable point of contention was a crypto pump-and-dump scam allegedly endorsed by Yegge and the creator of "Ralph" loops, which soured the perception of the project for several commenters. Technical critiques also emerged, with one user pointing out specific performance bottlenecks in the system's concurrency handling.

---

## [Improving the usability of C libraries in Swift](https://www.swift.org/blog/improving-usability-of-c-libraries-in-swift/)
**Score:** 141 | **Comments:** 32 | **ID:** 46726526

> **Article:** The Swift.org blog post details new features in Swift 6.2 that significantly improve the usability of C and C++ libraries within Swift code. It moves beyond the basic, often clunky manual bridging by introducing two key mechanisms: enhanced C++ interoperability (allowing direct inclusion of C++ headers) and "API Notes." API Notes are metadata files that allow library authors to annotate C APIs with Swift-specific attributes (like ownership conventions, error handling, and nullability) without modifying the original C source code. This enables Swift to generate much more idiomatic and safe interfaces automatically, reducing the need for boilerplate wrapper code and making C/C++ libraries feel more native to the Swift ecosystem.
>
> **Discussion:** The discussion is largely positive regarding the technical improvements, with several key themes emerging:

*   **Improved Developer Experience:** Many commenters, including those with prior experience, found the new features like API Notes to be a significant upgrade over manual bridging, making C interop less "clunky" and more approachable.
*   **Strategic Importance for Apple:** A recurring point is that Apple's investment in C/C++ interop is pragmatic. It acknowledges that their vast existing codebases in C and C++ (for system and embedded code) are unlikely to be rewritten in Swift. This interoperability provides a crucial migration path and allows developers to build new Swift layers on top of mature, low-level code.
*   **Complexity and "Sharp Cliffs":** While the "happy path" is praised, some users caution that wrapping highly complex or arcane C APIs can still present significant challenges. The new tools may not solve every edge case, and developers are advised to carefully assess an API's complexity before committing to a Swift wrapper.
*   **Language Design Debates:** The conversation branched into broader opinions on Swift. While some see it as a "slow and steady" contender for the future, others criticize its growing complexity, particularly with advanced features and concurrency. A specific point of contention was Swift's numerous pointer types (`UnsafePointer`, `UnsafeRawPointer`, etc.), which some found comically complex while others defended them as a necessary system for mapping C's ambiguous pointer semantics into a safer, more explicit type system.
*   **Skepticism of Apple's Motives:** A minority view expressed skepticism, suggesting that Apple's open-source efforts are primarily driven by the need to appear more open in antitrust and legal contexts rather than fostering a genuine open-source ecosystem.

---

