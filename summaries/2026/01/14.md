# Hacker News Summary - 2026-01-14

## [FBI raids Washington Post reporter's home](https://www.theguardian.com/us-news/2026/jan/14/fbi-raid-washington-post-hannah-natanson)
**Score:** 820 | **Comments:** 517 | **ID:** 46616745

> **Article:** The Guardian reports that the FBI raided the home of Washington Post reporter Hannah Natanson as part of an inquiry tied to classified materials. The article notes that raids on journalists in the US are rare. Natanson recently published a feature detailing her work interviewing 1,169 federal employees about the impact of the Trump administration's policies on the civil service. The raid is viewed as an attempt to identify her sources, potentially to prosecute them or to intimidate the press.
>
> **Discussion:** The Hacker News discussion is highly critical of the FBI's actions, interpreting the raid as a significant escalation against press freedom and a symptom of authoritarian overreach. While a few commenters noted that a warrant makes the seizure technically "normal" legal procedure, the prevailing sentiment is that using "classified materials" as a vague justification to target a reporter is dangerous and politically motivated.

Commenters drew several parallels and criticisms:
*   **Erosion of Rights:** Many argued that while Second Amendment rights are fiercely defended, First Amendment rights (free press) are being eroded with little public pushback. The raid was described as "fascism at its finest," with users noting that the expansion of executive power is a long-term, bipartisan issue, not just a product of the current administration.
*   **Comparison to Fiction and History:** Users referenced the TV show *Andor*, suggesting the administration is intentionally provoking citizens to fuel a rebellion, and the "boiling frog" analogy to describe public apathy.
*   **Technical and Legal Fears:** There was significant concern that the government would use the raid to compel tech companies or use advanced decryption to identify and prosecute Natanson's sources, potentially charging them with treason. Commenters expressed skepticism about the security of devices from state actors.
*   **Nature of the Reporting:** One user distinguished this from a "Snowden-type leak," characterizing Natanson's work as aggregating "a thousand small cuts" of federal misbehavior rather than a single massive data dump, suggesting the goal is to expose systemic dysfunction rather than one specific crime.

---

## [When hardware goes end-of-life, companies need to open-source the software](https://www.marcia.no/words/eol)
**Score:** 373 | **Comments:** 121 | **ID:** 46609492

> **Article:** The article argues for a "right to repair" for software, proposing that when a hardware product reaches its end-of-life (EOL), the manufacturer should release its software and hardware specifications as open source. The author uses the example of a smart kitchen scale that became a "brick" after its servers were shut down. The goal is to allow the community to maintain, update, or create alternative applications for the hardware, preventing functional devices from becoming e-waste and preserving the consumer's investment. The author clarifies they are not asking for the entire proprietary codebase, but rather the essential documentation and code needed for community-led development on top of the hardware.
>
> **Discussion:** The Hacker News discussion presents a nuanced and largely skeptical view of the article's proposal, focusing on the technical and economic complexities.

A central theme is the conflict between security and post-EOL usability. Commenters point out that modern devices with secure boot and code signing chains "fail closed," making it impossible to run new software after the manufacturer's support ends. While some suggest escrowing signing keys or requiring a physical button press to enable third-party firmware, others warn that releasing keys could create a massive security risk, allowing attackers to sign malicious firmware and build botnets.

The feasibility of the proposal was also debated. Several users argued that simply releasing hardware specs is of limited value, as the difficult part is often reverse-engineering complex protocols or dealing with locked-down hardware. They contend that for many devices, the community would lack the resources or incentive to maintain them, leading to abandoned open-source projects.

Finally, there was significant discussion around the economic implications. Some commenters worried that mandatory open-sourcing would become a form of corporate entitlement, dumping responsibility onto the community. Others feared it would stifle innovation by making it harder for companies to sell new products if older ones could be endlessly supported by the community. The idea of government regulation, particularly from the EU, was mentioned as a potential catalyst, but the overall sentiment was that the issue is far more complex than the article suggests.

---

## [A 40-line fix eliminated a 400x performance gap](https://questdb.com/blog/jvm-current-thread-user-time/)
**Score:** 351 | **Comments:** 76 | **ID:** 46609630

> **Article:** The article details a performance bottleneck in the Java Virtual Machine (JVM) where fetching the user-mode CPU time for a single thread was unexpectedly slow. The author found that the standard method (`ThreadMXBean.getThreadUserTime()`) could take up to 28 microseconds, while a more direct syscall (`clock_gettime` with `CLOCK_THREAD_CPUTIME_ID`) was significantly faster at around 70 nanoseconds. This represented a 400x performance gap. The issue was traced to the JVM's implementation, which performed multiple redundant lookups and other overhead. A 40-line patch was submitted to the OpenJDK to streamline this process, aligning it with the faster syscall and effectively eliminating the performance gap.
>
> **Discussion:** The discussion centered on the technical details of the fix and the broader context of performance analysis. Key points included:

*   **Technical Deep Dive:** Commenters explored the underlying mechanisms, clarifying that `clock_gettime` for thread-specific CPU time requires a kernel syscall and cannot be handled by the vDSO (virtual dynamic shared object) which avoids context switches for other clocks. This led to a deeper conversation about how the kernel handles these requests.
*   **Alternative Optimizations:** One user proposed an even faster method (potentially 10x faster) using software performance events (`PERF_COUNT_SW_TASK_CLOCK`) read from a shared memory page, which could avoid syscalls entirely, though it comes with its own set of complexities and requirements.
*   **Measurement and Skepticism:** A point of debate was the accuracy of the measurements. One commenter cautioned that making claims about nanosecond-level precision requires extremely stable and accurate clocks, suggesting the results should be qualified.
*   **General Appreciation:** The community praised the writeup for its clarity and the impact of the fix, with several users sharing their own experiences using flamegraphs to uncover similar "hidden" performance costs in codebases.

---

## [I Hate GitHub Actions with Passion](https://xlii.space/eng/i-hate-github-actions-with-passion/)
**Score:** 348 | **Comments:** 267 | **ID:** 46614558

> **Article:** The article "I Hate GitHub Actions with Passion" is a rant about the difficulties of debugging and fixing GitHub Actions workflows. The author's primary frustration stems from a workflow that failed on one platform (likely a specific runner architecture) but not others, making it incredibly difficult to diagnose the root cause. The author laments the lack of a local runner that perfectly replicates the GitHub Actions environment and criticizes the platform for hiding crucial details about the runner's setup. The core takeaway is a strong recommendation to avoid putting complex logic directly into GitHub Actions workflow files. Instead, developers should keep all logic in their own scripts (e.g., Makefiles, shell scripts, or other languages) and use GitHub Actions merely as a wrapper to execute those scripts, making them easier to test and debug locally.
>
> **Discussion:** The Hacker News discussion largely validates the author's frustrations and expands on the core themes of debugging difficulty and architectural best practices. The most prominent advice, echoed by multiple users, is to treat CI workflows as "dumb" orchestrators that simply call external, testable scripts. This decouples the logic from the CI platform, allowing for local development and reducing dependency on CI-specific features.

A central theme of the discussion is the need for better debugging tools. Several users recommend tools that provide SSH access to failed runners, such as `action-tmate` (with a modern alternative, `upterm`) or CI platforms that offer a "Rebuild with SSH" feature. This is seen as a direct solution to the "black box" problem of not knowing why a workflow failed.

The comments also address the author's specific pain points:
*   **Local Replication:** The tool `act` is frequently mentioned as a way to run workflows locally, but many users caution that it is not a perfect drop-in replacement and can fail to replicate the exact CI environment, especially for complex workflows.
*   **Environment Management:** One commenter argues that the author's problem wasn't GitHub Actions itself, but a "skill issue" in managing build dependencies. They suggest using tools like Nix or Mise to ensure consistent tooling versions between local and CI environments, thereby preventing such environment-specific failures.
*   **Platform Comparison:** Some users point out that other CI platforms like GitLab or SourceHut offer superior debugging experiences, suggesting the issue is somewhat unique to GitHub Actions' design philosophy.

Overall, the community agrees on the core problem and the recommended solution: keep CI workflows simple, manage dependencies externally, and advocate for better built-in debugging capabilities.

---

## [1000 Blank White Cards](https://en.wikipedia.org/wiki/1000_Blank_White_Cards)
**Score:** 344 | **Comments:** 60 | **ID:** 46611823

> **Article:** The Wikipedia article describes "1000 Blank White Cards" (1KBWC) as a party game and creative exercise. It is a "flanerie" or "nomic" style game, meaning the rules are not fixed but are created and modified by the players during the game itself. The game uses a deck of blank cards; players draw cards and can either play existing cards or create new ones by writing rules, victory conditions, or other effects on a blank card and adding it to the deck. The game is typically played until a player achieves a win condition, after which the group often votes on which new cards to keep for future games. The core of the game is its emergent, player-driven nature, where the gameplay and strategy evolve unpredictably based on the cards created.
>
> **Discussion:** The Hacker News discussion primarily focuses on the concept of emergent, player-defined rules and the social dynamics they create. Many commenters drew parallels to other games and experiences. A significant portion of the conversation centered on similar games, such as "We Didn't Playtest This At All," "Fluxx" (a commercial game with a similar concept but a fixed set of cards), and "Mao," a card game where new rules are invented by the winner but are not explained to new players, creating a puzzle of deduction. Several users shared personal anecdotes of playing "Pizza Box," a variant played with a coin and a pizza box where players draw circles and write rules inside them. The discussion also explored the game's metagame, with one user providing a detailed example of how a simple card concept (sheep) evolved into a complex, self-referential narrative over multiple games. The conversation touched on the game's theoretical underpinnings, linking it to concepts like game theory, mechanism design, and the challenges of creating rules that are both fun and balanced.

---

## [ASCII Clouds](https://caidan.dev/portfolio/ascii_clouds/)
**Score:** 323 | **Comments:** 56 | **ID:** 46611507

> **Article:** The article links to a portfolio piece titled "ASCII Clouds" by caidan.dev. It showcases a visually impressive web animation that renders a dynamic, cloud-like scene using ASCII characters. The effect is achieved by applying a post-processing shader that maps the brightness of a 3D scene to a set of ASCII characters, creating a retro, text-based aesthetic for a modern graphical output.
>
> **Discussion:** The Hacker News community's reaction was largely positive, with many commenters finding the visual effect "pretty cool" and "inspiring." However, the discussion quickly evolved into a technical and philosophical debate about the implementation and nature of the piece.

A key point of contention was the technique's complexity. Some users, like greggman65, pointed out that applying an ASCII shader is a relatively common and straightforward post-processing effect, providing numerous links to existing libraries and examples in frameworks like Three.js and Babylon.js. This suggested the work was more of a "technological show piece" than a novel invention, a sentiment echoed by others who didn't fully understand its purpose.

This led to a debate on artistic authenticity. A prominent critique, raised by users like ksymph and Bimos, was that using color and varying brightness for the ASCII characters defeats the purpose of the medium, which is traditionally defined by monochrome, character-based representation. They argued that if color is used, the characters are redundant. Others countered this by defending the piece as "art," where artistic freedom allows for such stylistic choices.

Finally, the discussion was enriched by contributions that contextualized the effect. One user shared their own similar project from 2007, and another offered an alternative, minimalist implementation of a "shader" within the Emacs text editor, demonstrating the concept from a completely different angle.

---

## [SparkFun Officially Dropping AdaFruit due to CoC Violation](https://www.sparkfun.com/official-response)
**Score:** 317 | **Comments:** 316 | **ID:** 46616488

> **Article:** SparkFun, an electronics distributor, announced it is officially dropping AdaFruit as a partner and will no longer carry their products. The announcement, made on SparkFun's website, cites a "Code of Conduct violation" by AdaFruit. The specific allegations are vague, claiming AdaFruit sent "offensive, antagonistic, and derogatory emails" and "inappropriately involving a SparkFun customer with a private matter." The post frames the decision as a necessary step to protect its employees and customers.
>
> **Discussion:** The Hacker News discussion is dominated by speculation and the emergence of conflicting narratives from both companies. The community's reaction can be broken down into several key themes:

*   **Criticism of SparkFun's Vagueness:** Many commenters found SparkFun's public statement to be unprofessional and an "invitation for speculation." They argued that by alluding to a "private matter" without providing details, SparkFun was engaging in drama rather than transparent communication.

*   **AdaFruit's Counter-Narrative:** AdaFruit's leadership (Limor Fried and Phil Torrone) responded in a forum thread, completely inverting SparkFun's story. They claim the conflict began after they reported SparkFun's founder for a long-term harassment campaign against them. They allege that SparkFun retaliated by cutting them off from the Teensy microcontroller, which SparkFun now exclusively manufactures.

*   **The "Teensy" Connection:** A crucial piece of context emerged: SparkFun recently became the exclusive manufacturer and distributor for the popular Teensy microcontroller board. This detail reframes the situation for many, suggesting the dispute may be a business power-play over a key product, rather than purely a Code of Conduct issue.

*   **Community Skepticism:** While some commenters shared positive personal anecdotes about SparkFun, the prevailing sentiment is one of uncertainty. Users acknowledge that both companies are respected in the industry and express sadness over the conflict, while also noting that business competition between the two could be a motivating factor.

*   **Concerns about Business Practices:** The discussion raised questions about the impact on consumers, particularly in Europe where SparkFun has been a primary distributor for AdaFruit. There was also a minor thread about the longevity of the URL SparkFun used for its announcement.

---

## [I’m leaving Redis for SolidQueue](https://www.simplethread.com/redis-solidqueue/)
**Score:** 286 | **Comments:** 117 | **ID:** 46614037

> **Article:** The article "I'm leaving Redis for SolidQueue" details a developer's decision to switch their background job processing from Redis to SolidQueue, a new job queue system built into the Rails ecosystem that uses a database (like PostgreSQL or SQLite) as its backend. The author argues that for many applications, the operational overhead of managing a separate Redis instance is unnecessary. SolidQueue offers a "good enough" solution that simplifies the technology stack by removing a dependency, leveraging the existing database, and integrating seamlessly with Rails. The move is framed as a trade-off: sacrificing the raw speed and specific advanced features of Redis for the benefits of simplicity, reduced maintenance, and a more cohesive development experience. The article suggests that for the vast majority of applications, the performance of a database-backed queue is more than sufficient.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, focusing on the trade-offs between simplicity and performance, and the growing dominance of PostgreSQL.

A central theme is the appeal of simplifying one's infrastructure. Many commenters agree that reducing operational overhead by removing a dedicated Redis service is a significant win, especially for smaller teams or applications that don't have extreme performance requirements. However, this is balanced by concerns about reliability and edge cases, with one user asking for more production experience with SolidQueue's job retries and failure handling, acknowledging that Redis is "battle-tested."

The conversation then pivots to the scalability question. While some share anecdotal evidence of hitting performance bottlenecks with Postgres-backed queues (even at seemingly modest throughputs like 5k jobs/minute), others counter that such issues are often due to misconfiguration. The Elixir ecosystem's Oban is frequently mentioned as a high-performance example of a database-backed queue, though one commenter critically deconstructs its "million jobs per minute" benchmark, arguing it relies on unrealistic batching and doesn't reflect true single-job throughput.

A significant point of contention is database choice and coupling. One commenter notes that SolidQueue's development is influenced by its creators' (Basecamp) preference for MySQL, which can limit the use of PostgreSQL-specific performance optimizations. This leads to a broader debate on whether it's wise to use the primary production database for the job queue. The consensus leans towards using a separate database instance for the queue, even if it's still a relational database, to avoid impacting the main application's performance.

Finally, the discussion touches on practical considerations like payload size (where Redis excels) and the fundamental nature of the comparison. Some argue that comparing Redis to a SQL queue is an "apples to oranges" scenario, representing different architectural philosophies, while others see it as a valid choice between two tools that can solve the same business problem.

---

## [Ford F-150 Lightning outsold the Cybertruck and was then canceled for poor sales](https://electrek.co/2026/01/13/ford-f150-lightning-outsold-tesla-cybertruck-canceled-not-selling-enough/)
**Score:** 280 | **Comments:** 359 | **ID:** 46618901

> **Article:** An Electrek.co article reports that the Ford F-150 Lightning outsold the Tesla Cybertruck, yet Ford has discontinued the vehicle due to poor sales performance. The article frames this as a failure of Ford's execution on an otherwise successful product, contrasting it with the Cybertruck's continued, albeit troubled, production.
>
> **Discussion:** The discussion centers on the reasons behind the F-150 Lightning's cancellation and offers a critical assessment of the Tesla Cybertruck. A key theme is that sales figures are misleading without context; for a giant like Ford, the Lightning's sales volume was insufficient to justify its production costs, whereas Tesla can sustain lower volumes for its niche, high-priced Cybertruck.

Commenters heavily criticized Ford's execution on the Lightning. They pointed to dealer markups creating artificial scarcity, poor design choices like an unnecessarily large "frunk" that reduced practicality, and an unappealing aesthetic. The consensus was that Ford failed to capitalize on a promising vehicle.

Conversely, the Cybertruck was widely described as an "abject failure" and a "bad product." Detractors cited its numerous recalls, insurance and repairability issues, and polarizing design. Some noted that its poor sales were exacerbated by its strong association with Elon Musk's political activities, which alienated the pro-EV demographic. However, a minority defended Tesla's "gamble" on a radical design, arguing it was a welcome break from the conformity of the truck market.

The discussion also branched into broader EV industry trends. Many commenters expressed excitement for new, simpler, and more affordable EV concepts like the Slate Auto, seeing them as a more practical vision for the future than expensive, tech-laden trucks. There was also a defense of Toyota's hybrid-first strategy, suggesting it was a more prudent approach than the "all-in" EV push by other legacy automakers who have since faced significant financial losses.

---

## [Ask HN: Share your personal website](https://news.ycombinator.com/item?id=46618714)
**Score:** 256 | **Comments:** 947 | **ID:** 46618714

> **Question:** A user posted an "Ask HN" question inviting others to share the URLs of their personal websites. The post itself is a simple, open-ended request for community members to showcase their personal web presences, which could include blogs, portfolios, or any other type of personal site.
>
> **Discussion:** The discussion quickly evolved from a simple sharing of links into a collaborative project. A user named 'susam' took the initiative to create and maintain a public directory of all the shared websites on GitHub Pages, providing a link to the directory and its source repository. This act of centralizing the information became a key part of the conversation, with 'susam' and others encouraging the community to contribute to the directory. The rest of the comments primarily consist of users sharing their personal website URLs, often in direct response to the Ask HN prompt, with 'susam' actively acknowledging and adding each new submission to the central directory.

---

## [The $LANG Programming Language](https://news.ycombinator.com/item?id=46610557)
**Score:** 256 | **Comments:** 61 | **ID:** 46610557

> **Article:** The article is a new HN feature page, accessible at `https://news.ycombinator.com/showlang`. It's a curated list of "Show HN" submissions for programming languages. The page was created by HN admin "dang" to centralize and make these specific types of projects more discoverable.
>
> **Discussion:** The discussion, led by HN admin "dang", revolves around the creation and technical implementation of the new `/showlang` page. A key point was that the initial implementation caused a significant performance issue on the site because it dynamically loaded all the old threads, a problem that was temporarily mitigated by removing the direct link from the front page.

The conversation then broadened into a meta-discussion about the visibility of "Show HN" posts for new projects. User jeswin shared an experience where their "Show HN" for a new language, Tsonic, failed to appear on the list. This prompted a helpful response from dang, who explained it was due to a software glitch that dropped upvotes and offered to manually add the post to the new list after some edits. This highlighted the challenges of getting new projects noticed on the platform.

Other comments were more lighthearted, with users playfully misinterpreting the "$LANG" title as an actual language name and sharing their own language ideas. The thread also served as a practical example of the new feature, as dang manually added other previously missed language submissions (like "MoonBit") to the list during the discussion.

---

## [So, you've hit an age gate. what now?](https://www.eff.org/deeplinks/2026/01/so-youve-hit-age-gate-what-now)
**Score:** 253 | **Comments:** 209 | **ID:** 46619030

> **Article:** The EFF article "So, you've hit an age gate. what now?" provides practical advice for users encountering mandatory age verification prompts on websites. It outlines the risks of submitting personal data (like government IDs or facial scans) to third-party verification services, including privacy violations and data breaches. The article suggests strategies to bypass these gates, such as using privacy-enhancing tools like VPNs to appear from a different jurisdiction, utilizing ad-blockers to remove overlay prompts (where possible), and seeking alternative sources for content. It frames these gates not just as inconveniences but as significant threats to online anonymity and civil liberties, encouraging users to resist normalizing the surrender of sensitive personal information.
>
> **Discussion:** The Hacker News discussion reveals a community deeply skeptical of age verification laws and the companies implementing them. There is a strong consensus that these measures are ineffective at best and dangerous data grabs at worst. Users shared various methods to circumvent these gates, ranging from technical solutions like VPNs and ad-blockers to more creative workarounds like using fake IDs, AI-generated faces, or even video game screenshots to fool verification algorithms.

A central theme is the distrust of the motivations behind these checks. Many commenters argued that companies like Google and Meta use age verification as a pretext to harvest biometric data ("they just want your face") for profit, rather than for genuine safety concerns. This cynicism extends to the government, with users viewing these regulations as a slide towards increased surveillance and a loss of online anonymity. The discussion also touched on the real-world consequences, with one user noting their child quit a popular game rather than submit to a facial scan, highlighting a growing friction between privacy-conscious users and platform requirements.

---

## [The insecure evangelism of LLM maximalists](https://lewiscampbell.tech/blog/260114.html)
**Score:** 243 | **Comments:** 263 | **ID:** 46609591

> **Article:** The article "The insecure evangelism of LLM maximalists" argues that the aggressive promotion of Large Language Models (LLMs) for coding often stems from insecurity. The author posits that if LLMs were truly superior, their results would speak for themselves without needing constant, hostile evangelism. The piece suggests that the push for LLMs is partly driven by a desire to devalue the craft of programming, perhaps to make it more accessible or to justify replacing programmers. The author shares their personal experience where, despite being an experienced developer, they found LLMs did not increase their productivity and often produced code that required significant rework, concluding that the tools are not yet a net positive for their own workflow.
>
> **Discussion:** The Hacker News discussion is a microcosm of the broader debate on AI in programming, with users falling into several camps. A central theme is the tension between code quality and business velocity. Some commenters argue that in a competitive market, the developer who delivers features faster—even with "slop" that hides bugs until after a promotion—is more valuable to a business than a meticulous programmer. This pragmatic view is countered by others who warn of the long-term liability of accumulating poorly understood, low-quality code.

A recurring point is the skill-dependent nature of LLMs. Several developers assert that LLMs are excellent for specific tasks like boilerplate, writing SQL queries, or exploring unfamiliar domains, but struggle with complex, nuanced problems. The sentiment that "if you think LLMs always write better code, you might be a below-average programmer" was frequently echoed, suggesting that LLMs are an average of existing code and thus a ceiling for mediocre developers but a floor for experts.

The discussion also explored the fear that LLMs could devalue the profession by creating a generation of developers who never learn the fundamentals, instead spending their careers in a frustrating loop of "vibe coding" and debugging non-deterministic output. This was framed as a shift from creative problem-solving to tedious "digital clerical work." Ultimately, many commenters expressed fatigue with the polarized debate, calling for a focus on practical results and evidence over ideological arguments.

---

## [The Gleam Programming Language](https://gleam.run/)
**Score:** 236 | **Comments:** 138 | **ID:** 46611667

> **Article:** The article introduces Gleam, a statically typed functional programming language that compiles to both Erlang (for the BEAM virtual machine) and JavaScript. It emphasizes simplicity, a strong type system, and a good developer experience, aiming to bring the safety and guarantees of modern functional languages to the robust, concurrent ecosystems of Erlang and web development.
>
> **Discussion:** The Hacker News discussion presents a multifaceted view of Gleam, balancing its technical strengths against practical adoption hurdles. The overall sentiment is cautiously optimistic, with many acknowledging its elegance while pointing out real-world friction points.

Key themes in the discussion include:

*   **The Value of a Strong Type System:** A central debate revolves around the necessity of static types, especially for distributed systems. One commenter argued that types are less useful in distributed applications where data over the wire is inherently uncertain. This was strongly countered by others who contended that types are the *best* tool for defining and validating data contracts and communication protocols, preventing entire classes of errors.

*   **Practical Ecosystem and Library Concerns:** Several developers highlighted significant pain points. The lack of built-in, automatic serialization for types was a major complaint, though others noted it's a solvable problem with code generation. A user migrating from Elixir was discouraged by the absence of ad-hoc polymorphism (like Elixir's protocols), the lack of macros, and the fact that basic functionality like filesystem access isn't in the standard library due to cross-target compatibility issues.

*   **BEAM Interoperability Debate:** A direct rebuttal emerged to the claim that Gleam can't easily access the rich Erlang/Elixir ecosystem. A contributor demonstrated that Gleam can call any Erlang or Elixir function directly using `@external` annotations, proving that the entire BEAM ecosystem is, in fact, available.

*   **Dual-Targeting Philosophy:** The decision to compile to both BEAM and JavaScript was questioned, with one user expressing skepticism that a language can excel at two very different targets. However, a proponent explained that the dual-targeting is a feature, not a compromise, enabling full-stack Gleam development. They clarified that BEAM-specific features are simply placed in separate libraries, keeping the core language clean.

*   **Developer Experience:** On the positive side, several users, including a student who found it reignited their passion for programming, praised Gleam's clean syntax, algebraic data types, and pattern matching, viewing it as a more accessible entry point to functional programming than Erlang or Elixir.

---

## [Roam 50GB is now Roam 100GB](https://starlink.com/support/article/58c9c8b7-474e-246f-7e3c-06db3221d34d)
**Score:** 207 | **Comments:** 213 | **ID:** 46617668

> **Article:** Starlink has updated its Roam mobile plan, doubling the monthly high-speed data allowance from 50GB to 100GB for the same price of $150/month. The plan is designed for users traveling with their satellite dish, such as in RVs or boats. A key feature is that once the 100GB of high-speed data is consumed, the service does not cut off but instead throttles speeds to a still-usable 500 kbit/s. This "soft cap" allows for continued basic connectivity for tasks like messaging, email, and some voice calls, rather than a complete service stoppage.
>
> **Discussion:** The Hacker News community's reaction to the Starlink Roam plan update was multifaceted, touching on the utility of throttled data, the plan's value proposition, and broader company politics.

A dominant theme was the appreciation for "soft caps" over "hard caps." Many commenters shared personal experiences where throttled speeds (around 500 kbps or similar) were sufficient for essential tasks like email, messaging, remote work, and even low-bandwidth VoIP calls. This was contrasted with the frustration of being completely cut off or facing exorbitant overage fees. Some shared nostalgic anecdotes of when 512 kbps was considered a fast home internet connection.

The financial aspect of the plan was also debated. While some saw the 100GB increase as a good value, others pointed out that the old plan allowed for paying per gigabyte ($1/GB) after the cap, which could be cheaper for users who occasionally exceeded their limit by a small amount. However, for those who regularly used more than 50GB, the new plan or the unlimited option was seen as an improvement.

Several comments highlighted practical use cases, with users relying on the service for RV travel, as a backup during home internet outages, and for work in remote locations. One user even mentioned using it for "vibe coding" in the woods.

The discussion also branched into more speculative territory, with one user proposing the creation of a low-cost, throttled mobile MVNO, noting that Starlink's model might be a way to circumvent traditional carrier restrictions.

Finally, the conversation inevitably included political commentary. One user declared they would never support the company due to its association with Elon Musk, which prompted a counter-argument from another user who credited Musk for helping Iranian citizens access the internet during government shutdowns.

---

## [Show HN: OSS AI agent that indexes and searches the Epstein files](https://epstein.trynia.ai/)
**Score:** 198 | **Comments:** 89 | **ID:** 46611348

> **Project:** The project is an open-source AI agent that provides semantic search across the publicly available Epstein legal files. Built by a single developer in a few hours, it uses vector embeddings to allow users to query the documents. The developer notes plans to update the dataset as more files are released.
>
> **Discussion:** The community response was a mix of technical questions, skepticism regarding the scope of the data, and political commentary. 

Several users questioned the reliability of the AI, with one user joking about the inevitability of "spicy hallucinations," while others debated the ethics of using AI to generate images to visualize the redacted content. There was a significant focus on the limitations of the available data; multiple commenters pointed out that only about 1-2% of the Epstein files have been released and are heavily redacted, leading to concerns that the search results represent a very incomplete picture. 

Technically, users inquired about the use of vector embeddings and caching of prompts. The discussion also quickly turned to the broader political implications, with users debating Donald Trump's past connections to Epstein and the difficulty of prosecuting powerful individuals. One commenter humorously mistook "OSS" for the Office of Strategic Services rather than Open Source Software.

---

## [Claude Cowork Exfiltrates Files](https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files)
**Score:** 180 | **Comments:** 82 | **ID:** 46622328

> **Article:** The article from PromptArmor details a security vulnerability in "Claude Cowork," a feature allowing users to upload documents for the AI to analyze. The exploit works by embedding a malicious prompt injection into a file (e.g., a .docx) using invisible text. When a user uploads this file and asks Claude to analyze it, the hidden instructions take over, commanding the AI to exfiltrate sensitive data from the user's file system to an external server controlled by the attacker. The attack leverages the AI's ability to access local files and make external API calls, effectively turning the AI into a data-stealing tool without the user's knowledge.
>
> **Discussion:** The Hacker News discussion is highly critical and skeptical, focusing on three main themes: the nature of the vulnerability, the responsibility of the AI companies, and the feasibility of a fix.

Many commenters argue that prompt injection is an inherent, unsolvable flaw in AI models, comparing it to fundamental security problems like SQL injection or phishing. They contend that as long as AI can process instructions from untrusted sources (like user-uploaded files), it will be vulnerable. There is significant frustration directed at Anthropic for what is perceived as a "sloppy" implementation of security, with users feeling that the company is shifting the burden of safety onto them with "unreasonable precautions" rather than solving the core architectural issue.

A counter-narrative emerged, with some users pointing out the specific, multi-step requirements of the attack (e.g., the user must explicitly grant file access and upload a malicious file), suggesting the risk is being exaggerated for commercial purposes by PromptArmor. However, others countered that real-world phishing is highly effective and users would easily fall for such a lure. The conversation also touched on practical mitigation strategies, such as GitHub's secret scanning to revoke exposed API keys, and broader anxieties about the inevitability of a "billion-dollar attack" as AI agents become more integrated into critical systems.

---

## [Verizon outages reported across U.S.](https://www.firstcoastnews.com/article/news/nation-world/verizon-outage-reported/507-ef3cb3d0-f595-432f-9f84-d1690a5085a7)
**Score:** 177 | **Comments:** 136 | **ID:** 46620835

> **Article:** The article reports on widespread Verizon outages across the United States. While the primary report focuses on Verizon, the scope and cause of the disruption are the main points of interest and speculation in the subsequent discussion.
>
> **Discussion:** The Hacker News discussion quickly evolved from confirming the outage's geographic scope to speculating on its cause, with users split between technical failure and malicious intent.

A significant portion of the discussion centered on the possibility of a cyberattack. Some users theorized about state-sponsored attacks or government intervention, citing historical precedents like the BART subway cell service shutdown. However, others countered that such a widespread, multi-carrier event (as reported by some) would be a massive and unlikely operation.

The more pragmatic and likely explanation, according to many commenters, is a simple operational error. Users pointed to a history of major outages caused by bad software updates or configuration pushes from companies like Cloudflare and Meta, suggesting this is the most probable cause.

The conversation also branched into practical matters:
*   **Scope Verification:** Users debated the severity by cross-referencing Downdetector data, with some noting that Verizon's outage reports were significantly higher than other carriers, arguing against a coordinated attack.
*   **Knock-on Effects:** Commenters reported related issues, such as intermittent DNS problems on other ISPs like Comcast, suggesting a potential ripple effect from the Verizon disruption.
*   **User Resilience:** One thread prompted a broader discussion on digital preparedness, with users debating what the average citizen could do to maintain communication during a major network failure.

---

## [Minor says ICE took his iPhone, later found in used-electronics vending machine](https://www.propublica.org/article/videos-ice-dhs-immigration-agents-using-chokeholds-citizens)
**Score:** 167 | **Comments:** 65 | **ID:** 46611375

> **Article:** An article by ProPublica investigates U.S. Immigration and Customs Enforcement (ICE) agents using banned chokeholds and excessive force against U.S. citizens during encounters. The report highlights specific incidents, including one where a teenager, Arnoldo, was chased and assaulted by agents. Although agents confiscated Arnoldo's iPhone, he used the "Find My" feature to locate the device in a used-electronics vending machine near an ICE detention center. This recovered footage corroborated the family's account of the incident.
>
> **Discussion:** The discussion on Hacker News focused primarily on the systemic issues of law enforcement accountability rather than the technological aspects of the story. Commenters expressed dismay that the use of banned techniques persists across unrelated cases, suggesting this indicates a "systems problem" rather than isolated misconduct. Some users broadened the critique to a general lack of adherence to laws by the government, citing unrelated examples like the Epstein files and TikTok. While one user noted the irony of the "Find My" feature exposing the agents, the majority of the conversation remained critical of ICE's conduct and the lack of legal consequences.

---

## [GitHub should charge everyone $1 more per month to fund open source](https://blog.greg.technology/2025/11/27/github-should-charge-1-dollar-more-per-month.html)
**Score:** 166 | **Comments:** 151 | **ID:** 46618027

> **Article:** The article proposes that GitHub should increase its subscription price for all paid users (individuals and organizations) by $1 per month. This extra dollar would automatically be funneled into a fund to support open-source maintainers, particularly those whose projects are widely used as dependencies. The author argues this is a simple, scalable solution to the chronic underfunding of critical open-source infrastructure. It frames the current situation as unsustainable, where companies profit from free labor, and suggests this model would make open-source contribution more viable and sustainable without requiring users to directly change their behavior.
>
> **Discussion:** The Hacker News discussion reveals a deep philosophical and practical divide on the nature of open-source software and its funding.

A central conflict emerged between two opposing views of open-source. One side, articulated by commenters like Dilettante_ and grayhatter, argued that open-source is fundamentally a "gift" and that developers willingly enter a social contract to provide it for free, without expectation of compensation. They believe introducing financial expectations changes the nature of the work and that the freedom to use the code as-is is paramount. The opposing view, which the article supports, holds that this "gift" economy is exploitative and unsustainable, and that the immense labor behind critical software deserves compensation.

Several practical concerns were raised about the proposal's implementation and consequences:
*   **Incentivizing Bad Behavior:** A major theme was the fear that a new revenue stream would incentivize the creation of low-quality or malicious packages to game the system. Commenters worried about a flood of "garbage dependencies" and "dependency spam" designed to inflate download numbers and siphon funds, a problem already present in ecosystems like npm.
*   **The "Transitive Dependency" Problem:** It was noted that simply rewarding top-level projects is insufficient; the fund should support the entire dependency tree (the "lock file"), otherwise it would disproportionately reward those who simply wrap existing work.
*   **Corporate Role and "Enshittification":** Skepticism was directed at Microsoft (GitHub's owner) as a trustworthy steward of such a fund. Some argued that adding profit motives to FOSS would lead to "enshittification" and that Microsoft should be paying users for the data it collects via AI tools like Copilot, not the other way around.

Finally, some commenters debated the scope, clarifying that the proposal likely targets paying (organizational) users, not the vast free user base, making the economic impact more manageable than a mass exodus. Others used the thread to comment on broader trends, such as the consolidation of power among a few tech giants and the value of non-monetarily motivated creation in a "hustle culture" world.

---

