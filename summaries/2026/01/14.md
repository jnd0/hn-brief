# Hacker News Summary - 2026-01-14

## [Scott Adams has died](https://www.youtube.com/watch?v=Rs_JrOIo3SE)
**Score:** 944 | **Comments:** 1461 | **ID:** 46602102

> **Article:** The article links to a YouTube video announcing the death of Scott Adams, the creator of the Dilbert comic strip. A top comment provides a detailed Wikipedia excerpt detailing his final months. Adams had been battling prostate cancer and, in late 2025, his health declined rapidly, leaving him paralyzed from the waist down. He publicly sought access to a specific cancer drug and received support from political figures. By January 2026, he announced he was in hospice care, stating that a "transition" was imminent, and he passed away shortly thereafter at the age of 68.
>
> **Discussion:** The discussion is a mix of immediate reactions to the news and a complex reflection on Scott Adams' legacy. Many commenters express sadness and share personal anecdotes about how much they valued Dilbert and his books on corporate life, crediting him with providing "endless wisdom and amusement." There is a recurring sentiment to separate his creative genius from his later, more controversial political commentary, with several users choosing to remember him primarily for Dilbert's brilliant capture of 1990s corporate culture.

The conversation also delves into the nature of fame and mortality. One user reflects on how quickly cultural icons can fade from relevance, noting that the medium of print cartoons is now largely dead and that younger generations may not know his work. This leads to a broader meditation on how life moves on at a "breakneck" pace after an individual is gone. The discussion concludes with a brief, shared sentiment of "Fuck cancer" in response to the details of his painful final year.

---

## [AI generated music barred from Bandcamp](https://old.reddit.com/r/BandCamp/comments/1qbw8ba/ai_generated_music_on_bandcamp/)
**Score:** 771 | **Comments:** 553 | **ID:** 46605490

> **Article:** Bandcamp has updated its policy to explicitly bar AI-generated music from its platform. The article links to a Reddit discussion about this announcement, which is positioned as a move to keep the platform "human" and protect the work of human artists from being diluted by low-effort, automated content.
>
> **Discussion:** The discussion on Hacker News is largely supportive of Bandcamp's decision, framing it as a necessary defense of human creativity and a reaction to the influx of low-quality "AI slop" on other platforms like Spotify. A key theme is the contrast between Bandcamp, seen as a curated space for human artists, and Spotify, which many users feel is becoming saturated with AI-generated tracks that are often indistinguishable from or misleadingly labeled compared to human work. This has reportedly led some users to abandon Spotify for Bandcamp to ensure they are supporting real artists.

However, the conversation also explores the nuances of AI's role in music creation. Many commenters distinguish between purely AI-generated content and AI-assisted workflows. An analogy is drawn to "vibe coding," where developers use AI as a powerful tool or "scaffolding." Some argue that AI tools can be a legitimate part of a professional artist's process, helping them realize a pre-existing human vision. This leads to a debate on where to draw the line: is the artist's intention and curation enough to make an AI-assisted track "human," or does the final product need to be primarily shaped by human skill and effort?

The community is divided on enforcement and the potential for false positives. While some advocate for a hard, unambiguous rule (banning all AI-generated audio), others worry that a blanket ban could inadvertently punish artists who use AI as a sophisticated tool within a larger creative process. The discussion also touches on the ethical implications, with one commenter sharing a particularly egregious example of a deceased artist's Spotify page being used to promote a new AI-generated song, an act described as "absolutely fucking gross."

---

## [Apple Creator Studio](https://www.apple.com/newsroom/2026/01/introducing-apple-creator-studio-an-inspiring-collection-of-creative-apps/)
**Score:** 490 | **Comments:** 400 | **ID:** 46601157

> **Article:** Apple has announced "Apple Creator Studio," a new subscription service that bundles its suite of professional creative applications. The subscription includes Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage. It also adds new AI features and premium content to its iWork suite (Keynote, Pages, and Numbers). The service is priced at $12.99/month or $129/year, with a significant educational discount available at $2.99/month or $29.99/year. Apple will continue to offer one-time purchase options for the individual Mac applications, so the subscription is an alternative, not a replacement.
>
> **Discussion:** The announcement sparked a debate centered on Apple's business strategy and the value proposition for users. A primary point of contention is the shift towards a subscription model. While some users expressed frustration at another monthly fee, others pointed out that Apple is offering a choice, as one-time purchases for the core apps remain available. The pricing was generally viewed as competitive, especially the educational discount, though some questioned the value for users who only need one or two of the applications.

Commenters also discussed the broader implications of this move. Many saw it as a direct attempt to compete with Adobe's Creative Cloud and its business model. Beyond the business aspect, a significant portion of the discussion pivoted to the state of Apple's pro-level software and hardware. One user lamented the discontinuation of Aperture, while others questioned the frequency of major updates for these professional apps. A lengthy thread also emerged about the limitations of iPadOS, with users expressing frustration that Apple has not released a full version of Xcode for the iPad, highlighting the platform's restrictive nature for serious development work.

---

## [Scott Adams has died](https://www.usatoday.com/story/entertainment/celebrities/2026/01/13/scott-adams-dead-dilbert-creator-prostate-cancer/88158828007/)
**Score:** 418 | **Comments:** 3 | **ID:** 46603431

> **Article:** This article reports the death of Scott Adams, the creator of the comic strip *Dilbert*, at the age of 69. The cause of death was prostate cancer, which he had publicly disclosed diagnosised in 2023. The article provides a brief biography of his career, highlighting *Dilbert*'s massive success in depicting office culture and its syndication in over 65 countries. It also briefly touches upon the controversy that led to the end of *Dilbert*'s publication in major newspapers following Adams' controversial racial comments in 2023.
>
> **Discussion:** The discussion on Hacker News was extremely brief and procedural, consisting entirely of users flagging the post as a duplicate of an earlier submission regarding Scott Adams' death. Moderators confirmed the duplicate and moved the few existing comments to the original thread. Consequently, there was no substantive discussion of Scott Adams' life, work, or controversies in this specific comment section. The conversation was limited to standard site moderation actions.

---

## [We can't have nice things because of AI scrapers](https://blog.metabrainz.org/2025/12/11/we-cant-have-nice-things-because-of-ai-scrapers/)
**Score:** 395 | **Comments:** 207 | **ID:** 46608840

> **Article:** The article, from the MetaBrainz blog, details how aggressive AI scrapers are forcing the open data project to implement restrictive changes. MetaBrainz, which maintains MusicBrainz (a community-maintained, open music database), is experiencing crippling server load from scrapers that ignore `robots.txt` and bypass the project's provided bulk data downloads. To keep the service running, they are forced to add authentication barriers, remove less-secure API endpoints, and require logins for certain features. The author argues that these actions harm legitimate users and undermine the project's open nature, stating that AI companies are "externalizing their data acquisition costs" onto volunteer-run infrastructure. The core message is that AI companies, by ignoring simple courtesies and efficient methods, are destroying the "nice things" (open, accessible web services) that the community built.
>
> **Discussion:** The Hacker News discussion is largely sympathetic to MetaBrainz, with a consensus that AI companies are acting as bad actors by overloading small, volunteer-run projects. The central theme is a critique of the AI industry's "tragedy of the commons," where they burn goodwill and force open projects to lock down by ignoring efficient bulk download options in favor of aggressive, inefficient scraping.

Several key points emerged:
*   **Inefficiency and Bad Manners:** Many commenters highlighted the irony that scrapers ignore `robots.txt` and bypass provided bulk downloads, choosing instead to hammer APIs and websites page-by-page. This is seen as both rude and technically inefficient.
*   **The Lock-Down Effect:** There was agreement that the inevitable result of this behavior is the closure of open access. The original poster's changes (requiring authentication, removing debug APIs) were seen as a necessary but unfortunate step that hurts legitimate developers and hobbyists.
*   **Proposed Solutions:** A few ideas were floated, such as forcing large scrapes through centralized archives like Common Crawl, or creating a new standard like `/.well-known/` to point to data dumps. However, it was noted that scrapers are unlikely to follow these conventions.
*   **Broader Context:** Commenters shared personal stories of shutting down their own small public sites due to scraping load. There was also a discussion on the difficulty of finding legitimate data sources, which ironically pushes some researchers toward scraping.
*   **Counter-Arguments:** One dissenting voice argued that the web is already too noisy and "AI-influenced" for web scraping to be useful for training, suggesting that the real danger is the closure of high-quality, curated datasets (like those on Hugging Face or Amazon) rather than the open web.

In essence, the discussion is a lament for the loss of the "old web" and a call for AI companies to act more responsibly by using the resources provided by open projects instead of destroying them.

---

## [Anthropic invests $1.5M in the Python Software Foundation](https://discuss.python.org/t/anthropic-has-made-a-large-contribution-to-the-python-software-foundation-and-open-source-security/105694)
**Score:** 378 | **Comments:** 167 | **ID:** 46601902

> **Article:** Anthropic is investing $1.5 million into the Python Software Foundation (PSF). The donation, structured as a multi-year sponsorship, is specifically earmarked to improve the security of the Python ecosystem, particularly the PyPI package repository. This includes developing automated tools for proactive review of uploaded packages and creating a dataset of known malware. The move is framed as a strategic investment in a critical piece of infrastructure that the AI industry, including Anthropic itself, heavily relies on.
>
> **Discussion:** The Hacker News discussion was largely positive but multifaceted, touching on the strategic, practical, and philosophical implications of the donation.

A central theme was the strategic importance of the investment. Commenters noted that it makes perfect sense for Anthropic to fund the security of a foundational technology that the entire AI ecosystem is built upon. This was contrasted with the corporate structure of other package managers like NPM (owned by Microsoft), highlighting the importance of ensuring that open-source infrastructure like PyPI remains secure and well-funded. The discussion also acknowledged that other tech giants like Google and Microsoft are already sponsors, but this new injection of funds was seen as a welcome boost.

The conversation frequently pivoted to the broader topic of funding open-source infrastructure. A highly upvoted comment referenced the book "Roads and Bridges," framing this donation as a necessary step from the private sector in supporting the "unseen labor" of digital infrastructure. There was a general sentiment that big tech and VC firms should be doing more to fund the critical open-source software they depend on.

However, some users questioned the scale and motivation of the donation. One prominent thread debated whether $1.5 million was a small sum for a company of Anthropic's size, with some calling it "peanuts" and "cheap PR." The counter-argument was that while small, it's still a positive action that should be encouraged rather than shamed. Another skeptical user pointed out that since Anthropic is not profitable, the money ultimately comes from their investors (like Nvidia), making it a strategic reinvestment rather than pure philanthropy.

Finally, the discussion briefly touched on technical aspects, with some users arguing that type hints are crucial for "agentic programming" and AI tooling. There was also some criticism directed at the PSF's past management decisions, with one commenter arguing that funds were misallocated towards "outreach" while critical infrastructure issues were neglected, forcing third parties to step in.

---

## [Influencers and OnlyFans models are dominating U.S. O-1 visa requests](https://www.theguardian.com/us-news/2026/jan/11/onlyfans-influencers-us-o-1-visa)
**Score:** 375 | **Comments:** 275 | **ID:** 46603535

> **Article:** A Guardian article reports that influencers and OnlyFans models are increasingly using the O-1B visa, intended for individuals with "extraordinary ability" in arts, film, or television. The article notes that while this visa was once reserved for Hollywood stars and renowned musicians, its criteria—such as high commercial success, high remuneration, and major achievements—are now being met by top-tier social media and adult content creators. This shift is redefining who qualifies as an "extraordinary" talent for U.S. immigration, moving beyond traditional entertainment industries into the creator economy.
>
> **Discussion:** The Hacker News discussion presents a mix of acceptance, confusion, and economic analysis regarding the trend. The community largely debates whether these creators fit the visa's intent. Many commenters argue that the O-1B visa's criteria are broad enough to include high-earning digital creators, as they demonstrate significant commercial success and cultural influence, similar to traditional entertainers. Several users point out that the distinction between a "Hollywood star" and a top influencer is minimal in the eyes of immigration law, as both are in the business of entertainment.

A prominent economic argument emerges, suggesting that these models are ideal migrants. They are seen as generating significant tax revenue with minimal use of public services, as their work is remote and they are typically young and healthy. Some also note that they help balance gender demographics among immigrants.

However, there is a distinct undercurrent of skepticism and moral concern. Some users express disbelief that the visa, often sought by highly skilled scientists and engineers, is being granted to adult entertainers. This ties into a discussion on the legal definition of prostitution versus pornography/online content creation, with some highlighting the historical stigma and strict immigration questions regarding such activities. Others dismiss these concerns, viewing the creator economy as the legitimate "future of culture" and arguing that the U.S. should not filter out "fun people." Ultimately, the consensus leans towards the view that if the system's rules allow it, it is a legitimate and even savvy use of the visa, driven by the economic reality of the modern creator economy.

---

## [Local Journalism Is How Democracy Shows Up Close to Home](https://buckscountybeacon.com/2026/01/opinion-local-journalism-is-how-democracy-shows-up-close-to-home/)
**Score:** 368 | **Comments:** 249 | **ID:** 46600850

> **Article:** The article argues that local journalism is the bedrock of democracy, functioning as a direct accountability mechanism for local government. It posits that while national media focuses on broad, often polarizing narratives, local reporters cover the tangible issues that affect daily life—such as school boards, zoning laws, and infrastructure projects. The author contends that the decline of local news, driven by media consolidation and the loss of advertising revenue to tech giants, leaves citizens less informed and less able to engage in civic life, effectively hollowing out democracy from the ground up.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, focusing on the causes of local journalism's decline and potential solutions. A central theme is the economic devastation caused by the shift of advertising revenue (particularly classifieds like property listings) to centralized online platforms, a trend noted in both the US and UK. This has led to media consolidation, where large conglomerates lack accountability to local communities and often shut down papers without regard for local needs.

Commenters offered varied solutions and critiques. Some advocated for direct civic action, like subscribing to local papers or contacting officials, sharing personal success stories of influencing local policy. Others debated funding models, with proposals ranging from public funding (similar to the BBC) to a mandatory "journalism fee" based on municipal budgets, though concerns were raised about government influence over a free press. There was also a call for leaner, non-profit, or cooperative models funded by community subscriptions.

The conversation also touched on the quality and bias of remaining local journalism. While some argued it has become a "puff piece" industry that kowtows to power, others countered that this has always been the case due to reliance on local access and advertising. The role of social media as a potential replacement was viewed with skepticism, with users noting that platforms like NextDoor and Facebook Groups often devolve into gossip or complaints rather than substantive journalism. Finally, a few comments veered into broader political grievances, including the decline of Twitter and general concerns about societal centralization eroding democratic values.

---

## [Signal leaders warn agentic AI is an insecure, unreliable surveillance risk](https://coywolf.com/news/productivity/signal-president-and-vp-warn-agentic-ai-is-insecure-unreliable-and-a-surveillance-nightmare/)
**Score:** 329 | **Comments:** 98 | **ID:** 46605553

> **Article:** The article, based on warnings from Signal's president and VP, argues that "agentic AI" (AI systems that can take actions on a user's behalf) represents a significant and underestimated security threat. These systems are described as insecure, unreliable, and a potential "surveillance nightmare." The core issue is that for an AI agent to be useful, it requires broad access to a user's data and systems, creating a massive attack surface. This combination of unreliability and privileged access makes them a prime target for exploitation and a tool for pervasive surveillance, whether by malicious actors or the corporations providing the AI services.
>
> **Discussion:** The Hacker News discussion reveals a community that largely agrees with the article's security concerns but debates the underlying causes and solutions. A central theme is skepticism about Signal's motives, with some users suggesting the warnings are a strategic move to position Signal for an upcoming entry into the AI market, a theory supported by a recent article about Signal's creator planning to "do for AI what he did for messaging."

The debate then splits into two main camps regarding the root cause. One group argues this is fundamentally an operating system problem, asserting that decades of poor security practices, weak process isolation, and a failure to take sandboxing seriously are now being exposed by the untrustworthy nature of AI. The opposing view is that the problem is inherent to the AI's function: true isolation is impractical because an agent needs access to be useful, and the cost and complexity of building fully isolated systems (like microkernels) have historically been rejected by the market.

From an enterprise perspective, commenters express frustration. Many see a disconnect between the promise of agentic AI and the reality of its unreliability and liability. The consensus is that for most businesses, predictability is more important than autonomy, making "human-in-the-loop" a necessary feature for the foreseeable future. There's also deep pessimism about the industry's ability to manage these risks, citing a general failure to implement "zero trust" principles even in major corporations. Practical advice for IT managers was also debated, with some suggesting technical controls like sandboxing and firewalls, while others countered that these measures don't solve the core problem of an AI needing privileged access to be effective.

---

## [When hardware goes end-of-life, companies need to open-source the software](https://www.marcia.no/words/eol)
**Score:** 313 | **Comments:** 93 | **ID:** 46609492

> **Article:** The article argues for a "right to repair" for software, stating that when hardware reaches its end-of-life (EOL), the software that controls it should be open-sourced. The author uses the example of a "Smart Kitchen Scale" that becomes useless when the company stops supporting its app. The proposal is not for companies to release their entire proprietary codebase, but to publish the essential hardware specifications and connection protocols on a platform like GitHub. This would empower the community to build their own applications, preventing functional hardware from becoming e-waste and allowing users to maintain control over their devices.
>
> **Discussion:** The Hacker News discussion presents a nuanced and skeptical view of the article's proposal, focusing heavily on the technical and economic complexities. A central theme is the conflict between security and post-EOL usability. Several users argue that modern devices rely on secure boot and code signing chains to "fail closed," preventing malicious takeovers. They caution that releasing signing keys or forcing devices to "fail open" could create massive security risks, such as turning old IoT devices into a botnet. Instead, commenters suggest alternative mechanisms, like a physical button press to authorize third-party firmware, to balance security with user control.

Many users challenge the practicality of the article's core suggestion. They argue that simply releasing hardware specs is often useless, as the real challenge is reverse-engineering complex, undocumented protocols. Furthermore, there's a recurring concern that mandating open-sourcing would lead to companies releasing unusable, un-compilable code just to comply with the letter of the law.

The discussion also explores broader implications. Some see this as a perfect case for government regulation (the "EU do their magic"), while others warn against offloading responsibility onto "the community," viewing it as a form of corporate entitlement. The conversation is grounded in real-world examples, with users citing products like Aura Frames and Kodak Pulse Frames that were bricked by their manufacturers, illustrating the problem the article aims to solve. However, a counterpoint was made that a cited example of Bose open-sourcing its software was based on false reports.

---

## [Network of Scottish X accounts go dark amid Iran blackout](https://www.heraldscotland.com/news/25759181.network-scottish-x-accounts-go-dark-amid-iran-blackout/)
**Score:** 312 | **Comments:** 255 | **ID:** 46599574

> **Article:** This article from The Herald reports on a network of X (formerly Twitter) accounts, which posed as Scottish users, that went offline during a recent internet blackout in Iran. The accounts, identified by the UK Defence Journal and analyzed by the disinformation firm Cyabra, were spreading extreme and fabricated stories about civil unrest in Scotland, such as protests at Balmoral Estate and tanks in Edinburgh. The investigation suggests these accounts were part of an Iranian-linked influence operation designed to sow division and chaos, not only in the UK but also to influence international perceptions of Western stability. The article highlights how foreign actors exploit authentic political debates, like Scottish independence, to amplify division and undermine trust in democratic processes.
>
> **Discussion:** The Hacker News discussion reveals a deep skepticism about the nature of online information and the actors behind it. While some users initially express surprise or interest in the technical method of tracking disinformation networks by observing usage drops during blackouts, the conversation quickly pivots to broader concerns about the authenticity of online discourse.

A central theme is the debate over the true target of such influence operations. One commenter suggests the goal isn't to convince Scottish people of these outlandish claims, but rather to influence foreign audiences, particularly Americans, to believe that Western nations are on the brink of collapse. This idea is supported by references to other known disinformation campaigns, such as the "King of Slop" who profited from posing as a British nationalist.

The discussion then broadens to a general cynicism about the prevalence of sock-puppet accounts and "engagement bait" across all social media platforms, including Hacker News itself. Commenters speculate that even prominent figures can be radicalized by these online environments and express a desire for simpler, chronological feeds free from algorithmic manipulation.

Finally, a significant counter-narrative emerges, questioning the source of the intelligence. Commenters point out that the report originates from Cyabra, an Israeli-based firm, and express skepticism about information from a company with ties to a state that is in direct geopolitical conflict with Iran. This leads to a wider point about hypocrisy, with users reminding each other that state-sponsored astroturfing is a global phenomenon, not exclusive to adversaries like Russia or China, and that the intense focus on Russian bots may itself be a form of "programmed response."

---

## [90M people. 118 hours of silence. One nation erased from the internet](https://state-of-iranblackout.whisper.security/)
**Score:** 266 | **Comments:** 339 | **ID:** 46603910

> **Article:** The linked article, titled "90M people. 118 hours of silence. One nation erased from the internet," details a massive and prolonged internet shutdown in Iran. It frames the event as the complete digital isolation of a nation of 90 million people for 118 hours, linking the blackout to the government's violent suppression of widespread protests. The article is presented by "Whisper Security," and some HN commenters noted it appeared to be AI-generated and potentially promotional in nature, with the original link later becoming inaccessible and requiring an archive link.
>
> **Discussion:** The Hacker News discussion centered on the severity of the situation in Iran, the mechanics of state-sponsored internet censorship, and the global response. A primary theme was the immense human cost, with commenters debating the death toll of protesters, which one user claimed exceeded the Tiananmen Square massacre, while another offered a lower, unverified figure. There was significant anger directed at the perceived silence of international human rights organizations, with some suggesting that Iranian lives are treated as less valuable than those in other regions.

The conversation also delved into the technical and political aspects of the blackout. Users discussed how authoritarian regimes now view internet control as an essential tool to prevent organized dissent, referencing its use during the Arab Spring. A debate emerged on whether democratic nations also possess the capability to shut down the internet, with several users arguing that such strategic control is a standard part of national security planning for any technologically advanced state.

Finally, the discussion touched upon the broader geopolitical context and the nature of the reporting itself. Some commenters were skeptical of the article, calling it "AI slop" and a "submarine ad" for a security startup. Others questioned the effectiveness of outside intervention, with a debate on whether foreign support for protestors could be a net negative for Iran. The conversation also included a meta-discussion on American media portrayals of Iran, with some users feeling the reporting is often propagandistic and fails to capture the complexity of the situation, such as Iran's own role in regional proxy conflicts.

---

## [A 40-line fix eliminated a 400x performance gap](https://questdb.com/blog/jvm-current-thread-user-time/)
**Score:** 262 | **Comments:** 55 | **ID:** 46609630

> **Article:** The article by the QuestDB team investigates a massive performance discrepancy in the JVM. They discovered that asking the JVM "What is the CPU time of this thread?" was 400 times slower than necessary. The root cause was that the JVM used `clock_gettime(CLOCK_THREAD_CPUTIME_ID)`, which requires a system call (context switch) to the kernel. By changing a single line of code to use `clock_gettime(CLOCK_MONOTONIC)` instead—a call that can often be handled via the vDSO (virtual dynamic shared object) without a context switch—they achieved a 400x speedup. The "40-line fix" in the title refers to the necessary changes to the JVM source code to implement this optimization correctly across the board.
>
> **Discussion:** The Hacker News discussion praised the article as a great example of how a small, targeted fix can yield massive performance gains, especially in legacy or "organic" code. Several users engaged in a deep technical debate about the underlying mechanisms. The core of this discussion was whether `clock_gettime` uses vDSO to avoid a context switch. It was clarified that while some clocks (like `MONOTONIC`) can be read via vDSO, thread-specific clocks like `CPUTIME_ID` must go to the kernel to access the task struct, making the system call unavoidable in the standard implementation.

Other key points included:
*   **Accuracy and Methodology:** One user questioned the precision of the measurements (nanoseconds), stressing the importance of a stable clock for such claims.
*   **Alternative Optimizations:** A user proposed an even faster method (potentially 10x faster than the fix) using software perf events (`PERF_COUNT_SW_TASK_CLOCK`) to read CPU time from a shared memory page, completely avoiding syscalls.
*   **General Tools:** The value of flamegraphs for identifying unexpected performance bottlenecks was highlighted by multiple developers sharing their own experiences.
*   **Overhead of `procfs`:** A question was raised about the high overhead of reading from `/proc` filesystems, given that the data is in-memory.

---

## [Show HN: Self-host Reddit – 2.38B posts, works offline, yours forever](https://github.com/19-84/redd-archiver)
**Score:** 243 | **Comments:** 57 | **ID:** 46602324

> **Project:** This project, "Self-host Reddit," is an open-source tool that allows users to create and manage a local, offline archive of Reddit content. The project's goal is to preserve internet history by enabling individuals to host massive datasets, which are made available via torrent. The system is designed to be "yours forever," functioning without an internet connection once the data is downloaded. The repository also includes tools for profiling subreddits to help prioritize archiving efforts and provides an API and an MCP (Model Context Protocol) server for potential integrations.
>
> **Discussion:** The Hacker News discussion was multifaceted, touching on the project's technical aspects, its ethical implications, and its potential use cases.

Several users engaged directly with the project's creator (19-84) to ask for technical details, such as how to check for private subreddits in the data and how to get the project running (a user who encountered setup issues noted the creator quickly added missing example files). Other comments suggested potential applications, like integrating the archive with the discontinued Apollo app or using AI to categorize a personal video collection.

A significant portion of the conversation revolved around the ethics of archiving and the motivations behind deleting content. One user expressed frustration that deleted or protest-overwritten comments make the site less useful, while another countered that users have a right to exercise autonomy over their contributions and leave public discourse. The topic of using the data to train AI models was also raised, with one user questioning whether content creators were compensated, while another argued that posting on an open forum implies an expectation of public use.

Finally, there was a minor debate over the inclusion of data from Voat, an alternative platform known for hosting extremist communities. The project's creator defended the decision on the grounds of preserving any available complete dataset.

---

## [What a year of solar and batteries saved us in 2025](https://scotthelme.co.uk/what-a-year-of-solar-and-batteries-really-saved-us-in-2025/)
**Score:** 238 | **Comments:** 340 | **ID:** 46602532

> **Article:** The article by Scott Helme details his family's experience installing a 10kW solar array and two Tesla Powerwall 2 batteries at their UK home. He provides a detailed financial breakdown for the first year of operation (2025). The system generated 10.2 MWh of electricity, of which 6.5 MWh was used directly by the home, 2.1 MWh charged the batteries, and the rest was exported to the grid. The total annual electricity consumption for the household (including charging two electric vehicles) was a very high 31.8 MWh. The total savings for the year amounted to £4,284.80, factoring in avoided grid costs, grid export payments, and the UK government's Smart Export Guarantee (SEG). Helme concludes that the system has a projected payback period of around 9-11 years, which he considers reasonable.
>
> **Discussion:** The Hacker News discussion centered on the plausibility of the author's high energy consumption, the economics of solar and battery storage, and the practicalities of DIY vs. professional installation.

A primary theme was the sheer scale of the household's energy usage (31.8 MWh), which many commenters found exceptionally high. This was largely attributed to the author's two electric vehicles, with one user noting their own EV and electric heating usage is one-fifth of that amount. This led to a debate on whether the savings were a result of efficiency or a form of Jevon's paradox, where increased efficiency leads to greater consumption.

The conversation also delved into the financial and technical aspects of home energy systems. Commenters compared battery options, questioning the popularity of Tesla Powerwalls versus more cost-effective alternatives like BYD, and discussed the potential for using electric vehicle batteries for home storage (V2G). The feasibility of DIY battery projects was a notable topic, with users sharing experiences of achieving costs near €100/kWh, but also highlighting regulatory and safety hurdles that often require certified electricians.

Finally, there was practical advice for those considering similar investments. Users discussed the impact of expiring tax credits, the long-term value of pairing solar with a durable roof, and the potential of "grid cycling" (charging batteries from the grid at off-peak rates). The general sentiment was that while the upfront cost is significant, the long-term economics are becoming increasingly favorable, especially with rising grid electricity prices.

---

## [Every GitHub object has two IDs](https://www.greptile.com/blog/github-ids)
**Score:** 231 | **Comments:** 59 | **ID:** 46602591

> **Article:** The article "Every GitHub object has two IDs" explains that GitHub's GraphQL API uses two different types of identifiers for its objects. The older format is a simple, human-readable string like `010:Repository2325298`, which contains a type prefix and the database ID. The newer format is a longer, opaque-looking string that is actually a base64-encoded payload. The author demonstrates how to decode this new format, revealing that it contains a version number and the original database ID. The article concludes that while these IDs appear to have structure, developers should treat them as opaque as per GitHub's guidance to avoid their code breaking if the format changes in the future.
>
> **Discussion:** The Hacker News discussion revolves around the classic API design tension between opaque identifiers and structured ones. The central theme is Hyrum's Law: if an API provides an identifier, consumers will inevitably depend on its observable characteristics, even if documentation forbids it.

Many commenters strongly agree with the article's implicit warning, stating that developers should strictly follow the "opaque" guidance. Relying on the internal structure is considered fragile and self-inflicted pain, as GitHub could change the format at any time. The discussion suggests that if an API provider truly wants to prevent this, they must use encryption or random, non-sequential IDs (like UUIDs) to make the identifiers completely unintelligible.

However, other participants offered a pragmatic defense for GitHub's design choices. They argued that the structure isn't just for fun; it serves critical engineering purposes:
*   **Sharding:** The scope prefix (e.g., repository ID) allows GitHub to co-locate all related objects on the same database shard, which is essential for performance and reliability.
*   **Ordering:** The sequential database ID at the end ensures related objects are stored together and can be efficiently queried in order.
*   **Versioning:** The version prefix allows for future migrations without breaking old clients.

A highly detailed comment provided the most up-to-date information, explaining that the newest IDs are not just base64 but a base64-encoded MessagePack payload containing a version and the database ID. This commenter, along with others, reiterated the official best practice: developers should query for the dedicated `databaseId` field if they need the numeric ID, rather than trying to parse the global node ID themselves.

---

## [The truth behind the 2026 J.P. Morgan Healthcare Conference](https://www.owlposting.com/p/the-truth-behind-the-2026-jp-morgan)
**Score:** 225 | **Comments:** 47 | **ID:** 46605332

> **Article:** The article is a piece of creative fiction presented as a travelogue. The narrator attends the 2026 J.P. Morgan Healthcare Conference in San Francisco, but instead of focusing on the business aspects, they describe the event as a quasi-religious or mythological gathering. The conference is framed as a ritual centered on a "Great Organism" whose "beating heart" is located beneath the Westin St. Francis hotel. The narrator details the bizarre customs, the "pilgrims" (attendees), and the strange energy of the event, treating the high-stakes financial dealings as a form of arcane ceremony. The piece is written in a deadpan, pseudo-anthropological style, blending mundane observations of conference life with increasingly fantastical and absurd metaphors.
>
> **Discussion:** The HN community universally recognized the article as a work of fiction and was highly appreciative of its creative premise and writing style. The dominant sentiment was that it was a fun, clever, and "incredible read," with many commenters using the phrase "absolute cinema" to describe its quality. The discussion revolved around a few key points:

*   **Appreciation of the Genre:** Commenters enjoyed the piece as a piece of absurdist or surrealist fiction, with some comparing its tone to the work of Kurt Vonnegut or suggesting it would make a good SCP Foundation article.
*   **Real-World Context:** While enjoying the fiction, some users provided factual context. One pointed out that the official J.P. Morgan website warns about such unofficial sites, grounding the fictional premise in a real-world detail. Others discussed the actual purpose of the conference, which is not the sessions but the networking, deal-making (M&A, IPOs), and fundraising that happens in side meetings.
*   **Personal Anecdotes:** A few comments shared personal experiences from attending the real conference or similar industry events. These anecdotes reinforced the article's satirical elements by confirming that the official sessions are often forgettable and that the real action happens in private meetings, dinners, and informal gatherings, mirroring the narrator's focus on the "unseen" event.

---

## [The UK is shaping a future of precrime and dissent management (2025)](https://freedomnews.org.uk/2025/04/11/how-the-uk-is-shaping-a-future-of-precrime-and-dissent-management/)
**Score:** 223 | **Comments:** 274 | **ID:** 46600194

> **Article:** The article from Freedom News (April 2025) argues that the UK is rapidly implementing a "precrime" framework that focuses on dissent management rather than public safety. It highlights several legislative and technological developments: the expansion of "precharge" injunctions that restrict behavior based on predicted future actions, the use of AI and data analytics to identify potential "troublemakers" before protests, and the increasing criminalization of protest itself (e.g., "Serious Disruption Prevention Orders"). The author contends that these measures, often framed as risk management or public order protection, effectively lower the threshold for state intervention, allowing the government to suppress dissent and control populations before any actual crime is committed, creating a chilling effect on civil liberties.
>
> **Discussion:** The Hacker News discussion largely mirrors the article's concerns, with commenters drawing immediate parallels to dystopian fiction and questioning the efficacy and morality of such systems. Many users referenced *Minority Report* and *Black Mirror*, noting that fiction often serves as a warning for real-world policy. The debate touched on the distinction between preventing actual crimes (like conspiracy) versus policing "thought crimes" or dissent, with some arguing that the line is becoming dangerously blurred.

A recurring theme was the political motivation behind these measures, with users suggesting that a government facing unpopularity resorts to controlling the narrative and suppressing opposition rather than engaging in open debate. There was also a strong focus on the "slippery slope" argument: the concern isn't just how current governments use these tools, but how future, potentially more authoritarian ones might exploit them. Ultimately, the consensus leaned toward skepticism, viewing these developments as a gradual erosion of civil liberties under the guise of security.

---

## [The insecure evangelism of LLM maximalists](https://lewiscampbell.tech/blog/260114.html)
**Score:** 220 | **Comments:** 211 | **ID:** 46609591

> **Article:** The article "The insecure evangelism of LLM maximalists" argues that the aggressive promotion of Large Language Models (LLMs) for programming is often driven by insecurity rather than genuine productivity gains. The author contends that many evangelists are not top-tier programmers and that LLMs produce mediocre, "slop" code that is often worse than what a competent human would write. He suggests that the evangelists' hostility stems from a need to validate their own reliance on the tools and to pressure others into adopting them, thereby masking their own perceived inadequacies. The author concludes that while LLMs can be useful as "digital clerks" for simple tasks, they are not a replacement for skilled programming, and the insistence that they are represents a devaluation of the craft.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, with a strong consensus that while LLMs are useful tools, their evangelists often overstate their capabilities and that code quality remains a critical concern. The central theme is the tension between business velocity and engineering quality. Many commenters argue that in a corporate environment, a "mediocre programmer" using LLMs to rapidly generate features will be valued more highly than a competent programmer who produces clean, maintainable code, as the business rewards visible progress over long-term stability.

There is widespread agreement that LLMs excel at specific, well-defined tasks like boilerplate generation, writing SQL queries, or handling Git operations, but struggle with complex, nuanced, or novel problems. A recurring point is that if an LLM's output consistently seems better than a developer's own work, it may indicate a lack of skill in that developer, as LLMs are fundamentally "average" text generators.

Commenters also expressed a shared fear that over-reliance on LLMs could de-skill the industry, creating a generation of developers who can "vibe code" but lack a deep understanding of the systems they build. This could lead to an unmaintainable mess of "slop" code. The discussion also touched on the psychological aspect, with some noting that the "ick" factor of reading verbose, non-human-generated code is a valid concern related to taste and long-term maintainability. Ultimately, the conversation concluded with a call to move past the polarized debate and focus on practical results and shared achievements.

---

## [Games Workshop bans staff from using AI](https://www.ign.com/articles/warhammer-maker-games-workshop-bans-its-staff-from-using-ai-in-its-content-or-designs-says-none-of-its-senior-managers-are-currently-excited-about-the-tech)
**Score:** 217 | **Comments:** 115 | **ID:** 46607681

> **Article:** IGN reports that Games Workshop (GW), the maker of Warhammer, has banned its staff from using AI in content or design. According to the article, none of the company's senior managers are "currently excited about the tech." The policy appears to be a strategic decision to protect the company's intellectual property and maintain the unique, handcrafted quality of its brand, which is known for its highly detailed miniatures and distinct lore.
>
> **Discussion:** Discussion unavailable.

---

