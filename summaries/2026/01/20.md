# Hacker News Summary - 2026-01-20

## [American importers and consumers bear the cost of 2025 tariffs: analysis](https://www.kielinstitut.de/publications/americas-own-goal-who-pays-the-tariffs-19398/)
**Score:** 676 | **Comments:** 659 | **ID:** 46680212

> **Article:** The linked article from the Kiel Institute for the World Economy analyzes the economic impact of the 2025 US tariffs. It concludes that the costs of these tariffs are overwhelmingly borne by American importers and consumers, rather than foreign exporters. The analysis draws on economic principles and event studies of previous tariff shocks (such as on Brazil and India) to demonstrate that export prices from the affected countries generally do not decline; instead, trade volumes collapse. The article frames the policy as economically detrimental to the US, effectively an "own goal."
>
> **Discussion:** The Hacker News discussion largely validates the article's economic conclusions but branches into political speculation, historical analogies, and debates over the tariffs' strategic intent.

**Economic Consensus and Principles**
There is a strong consensus among commenters that tariffs function as a consumption tax paid by domestic buyers. Several users describe this as "Economics 101," arguing that it is basic supply and demand: if the cost of an import rises due to a tariff, the price for the consumer increases, or the product becomes unavailable. Some users pointed to the Chinese Producer Price Index remaining relatively flat during the tariff period as evidence that Chinese exporters did not absorb the costs, leaving US consumers to pay the difference.

**Political and Strategic Intent**
The discussion features divergent views on the purpose of the tariffs:
*   **Incompetence vs. Strategy:** One thread debated whether the tariffs were a result of economic misunderstanding or a deliberate strategy. While one user asked if the reduction in trade volume was the "intended result," others countered that the unpredictability of the policy was damaging business confidence and investment regardless of intent.
*   **Geopolitical Accusations:** A highly upvoted comment suggested the tariffs served the interests of foreign adversaries (specifically Russia) rather than the US.
*   **Mercantilism:** Users identified the policy as a return to mercantilism, noting the rare agreement between Neoliberal and Keynesian economists that this approach is flawed. However, a dissenting view argued that Keynesian economics supports reducing global trade imbalances, and while tariffs are a crude tool, they address real problems caused by surplus nations.

**Legal and Institutional Concerns**
Commenters expressed concern regarding the legality of the tariffs and the stability of US institutions:
*   **SCOTUS and Taxes:** Users debated whether the Supreme Court would classify the tariffs as a tax (which the executive branch has more authority over) or an overreach of power. There was fear that the administration might attempt to replace SCOTUS justices if the courts ruled against the tariffs.
*   **Policy Volatility:** The "whim" of the tariff policy was cited as a primary reason why domestic manufacturers were hesitant to invest in expanding US capacity to replace imports.

**Source Credibility**
A minor thread debated the validity of the source—a German think tank. While some users dismissed the analysis based on the institute's location, others argued that economic data should be evaluated on its merits regardless of the origin, and that US-based think tanks might face political pressure to align with the administration's narrative.

---

## [A decentralized peer-to-peer messaging application that operates over Bluetooth](https://bitchat.free/)
**Score:** 558 | **Comments:** 311 | **ID:** 46675853

> **Article:** The article links to "Bitchat," a decentralized, peer-to-peer messaging application that operates over Bluetooth. The service aims to provide off-grid communication without relying on internet access or central servers, functioning as a local mesh network where devices relay messages to one another.
>
> **Discussion:** The Hacker News discussion surrounding Bitchat centers on its practical utility, technical limitations, and the broader context of decentralized communication tools. A significant portion of the conversation is skeptical of the app's real-world viability due to Bluetooth's short range (typically under 400 meters). Users debated specific use cases, with some suggesting scenarios like protests or crowded festivals where cellular networks are congested or blocked, while others argued that the range is too limited for effective communication in such environments.

Technical limitations were a major point of contention. Commenters noted that the app currently lacks "store-and-forward" capabilities, meaning messages are only broadcast and not cached for users who come online later—a feature considered essential for practical mesh networking. There were also concerns about operating system restrictions, particularly on iOS, which aggressively manages background processes and could hinder the app's ability to relay messages continuously.

The discussion frequently compared Bitchat to Briar, a more established, security-audited alternative that offers similar offline capabilities. Several users expressed a preference for Briar, citing its maturity and open-source credibility, while others criticized Bitchat due to the involvement of Jack Dorsey, whose presence in the project drew mixed reactions ranging from skepticism to outright dismissal.

Finally, the conversation touched on the broader landscape of P2P file sharing and messaging. Users shared frustrations with existing solutions like Android's QuickShare and LocalSend, noting they often rely on Wi-Fi infrastructure or suffer from bugs. The thread concluded with a mention of integrating Bitchat with Meshtastic (a long-range mesh networking project) to extend its range, highlighting a community desire for a robust, off-grid communication network that is harder for governments to monitor or censor.

---

## [Radboud University selects Fairphone as standard smartphone for employees](https://www.ru.nl/en/staff/news/radboud-university-selects-fairphone-as-standard-smartphone-for-employees)
**Score:** 484 | **Comments:** 231 | **ID:** 46676276

> **Article:** Radboud University in the Netherlands has selected Fairphone as the standard smartphone for its employees. The university's announcement highlights the device's modularity and repairability, which aligns with the institution's sustainability goals and allows for in-house maintenance and repair, reducing electronic waste and long-term costs.
>
> **Discussion:** The Hacker News community discussion centered on the practical and philosophical implications of the university's choice. Key themes included the logistical benefits of in-house repair, skepticism about Fairphone's reliability and long-term viability, and broader desires for more ethical and user-controllable technology.

A significant portion of the conversation focused on the practicality of the decision. Several users noted that the modular design is ideal for an institutional setting like a university, where an IT department can stock spare parts and perform repairs on-site, which is often more cost-effective and efficient than sending devices to external vendors. This was contrasted with the experience of individual consumers, where one commenter shared a negative experience of a friend's Fairphone model being discontinued after four years, rendering its modular parts useless and forcing a new purchase. This led to a debate on whether buying second-hand phones is a more sustainable alternative.

The discussion also touched upon the technical and market limitations of Fairphone and similar devices. While some praised the company for offering long-term software support and a de-googled OS option, others were critical of specific hardware compromises, such as the lack of USB 3.0 and DisplayPort support in the Fairphone 6, which they felt made it uncompetitive. A recurring sentiment was the desire for legislation mandating repairability (e.g., easily replaceable batteries and screens) across all smartphone manufacturers, not just niche players. This reflects a broader frustration with the current market, where many users feel forced to choose between repairability and features, or to compromise on privacy by using devices from major tech giants. The conversation concluded with a user expressing a long-standing desire for a simple, privacy-respecting device with a headphone jack and a web browser, highlighting a perceived gap in the market for a truly open and functional alternative to mainstream smartphones.

---

## [Amazon is ending all inventory commingling as of March 31, 2026](https://twitter.com/ghhughes/status/2012824754319753456)
**Score:** 455 | **Comments:** 244 | **ID:** 46678205

> **Article:** Amazon is ending its inventory commingling program for third-party sellers, effective March 31, 2026. Commingling allowed different sellers to pool identical items in Amazon's fulfillment centers, meaning a customer buying from a specific seller might receive a product from a different seller's inventory. The change ensures that when a customer buys from a specific vendor, they will receive the item actually sourced from that vendor. This move is widely seen as a response to persistent issues with counterfeit and substandard goods that plagued the commingled system.
>
> **Discussion:** The discussion is overwhelmingly positive, with users viewing the end of commingling as a long-overdue correction to a system that prioritized logistics efficiency over product authenticity and consumer trust. Many commenters shared personal experiences of receiving counterfeit or inferior goods (such as HDMI adapters and toner cartridges) despite ordering from reputable sellers, confirming that the policy change addresses a significant pain point.

While some celebrated the decision as Amazon finally acknowledging that counterfeiting issues outweighed logistical benefits, others offered a more cynical perspective. They speculated that the policy shift might be driven by financial penalties from manufacturers or large buyers who were burned by the system, rather than a purely goodwill gesture.

The discussion also touched on broader themes of Amazon's market power and declining reputation. Several users noted that this change comes too late to regain their trust, citing a long history of frustration with the platform. The conversation expanded to include the negative impact of Prime on service quality and the general problem of counterfeit goods making Amazon an unreliable marketplace for certain categories of items. A tangential but notable thread compared Amazon's commingling to the Kenyan coffee market, using it as an analogy for how pooling fungible goods can disincentivize quality and harm producers.

---

## [Letter from a Birmingham Jail (1963)](https://www.africa.upenn.edu/Articles_Gen/Letter_Birmingham.html)
**Score:** 397 | **Comments:** 128 | **ID:** 46683205

> **Article:** The article is the full text of Martin Luther King Jr.'s "Letter from a Birmingham Jail," written in 1963 while he was imprisoned for participating in nonviolent demonstrations against segregation. Addressed to fellow clergymen who criticized his actions as "unwise and untimely," the letter defends the strategy of nonviolent direct action. King explains that freedom is never voluntarily given by the oppressor and must be demanded by the oppressed. He outlines the four basic steps of any nonviolent campaign: collection of facts, negotiation, self-purification, and direct action. He argues that an individual has a moral responsibility to disobey unjust laws, provided they do so openly, lovingly, and with a willingness to accept the penalty. King expresses profound disappointment with the "white moderate," whom he identifies as a greater obstacle to justice than the Ku Klux Klanner, for preferring order over justice and for constantly advising Black Americans to wait for a "more convenient season."
>
> **Discussion:** The Hacker News discussion is largely reverential, with many users stating they reread the letter annually and find new meaning in it. The conversation centers on three main themes: the nature of civil disobedience, King's critique of moderation, and the application of his philosophy to contemporary politics.

Users highlight King's nuanced argument that breaking an unjust law openly and accepting the penalty is a form of respect for the law itself, a concept they find rare in modern discourse. This is contrasted with references to Thoreau's "Civil Disobedience," framing the idea as a foundational American principle.

A significant portion of the discussion focuses on King's condemnation of the "white moderate." Commenters draw direct parallels to current political dynamics, particularly within the Democratic Party, where they see a schism between "liberals" who prioritize order and incrementalism and "leftists" who demand more radical, systemic change. This is applied to issues like immigration (calls to abolish ICE versus reform) and foreign policy (Palestine), with some arguing that moderates' defense of the status quo is a greater hindrance to progress than overt opposition.

The discussion also includes a counterpoint on the effectiveness of nonviolence, with one user noting that King's influence is often presented in a "white-washed" manner that omits the concurrent threat of violence from groups like the Black Panthers, which may have made King's "carrot" more appealing to the establishment. Finally, there is a brief debate about applying King's principles to the January 6th Capitol riot, with most commenters distinguishing between peaceful civil disobedience and a violent mob.

---

## [GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash)
**Score:** 316 | **Comments:** 104 | **ID:** 46679872

> **Article:** The article links to the Hugging Face page for GLM-4.7-Flash, a new open-weight large language model released by z.ai (GLM). It is a 31B parameter model (a distilled version of the larger GLM-4.7) designed to offer high performance at a size that is feasible for local deployment. The model aims to compete with proprietary small models like GPT-4o-mini and Claude 3.5 Haiku.
>
> **Discussion:** The Hacker News discussion focuses on the model's accessibility, performance relative to size, and the practicalities of running it locally.

**Performance and Benchmarks**
Commentators are analyzing the model's capabilities, noting that while it is a 31B parameter model, it reportedly achieves coding performance on SWE-Bench Verified comparable to much larger models like o3 or GPT-5 mini. However, some users caution that benchmarks don't always translate to real-world usage, with one user noting that similar small models (like GPT-OSS 20B) tend to hallucinate when solutions aren't immediately clear. There is skepticism about whether it truly outperforms established models like GPT-4o-mini or Qwen3-coder in practical tasks.

**Local Deployment and Hardware Requirements**
A major theme is the feasibility of self-hosting. Users highlight that the 31B size makes it a "perfect size for running fine-tuning experiments" and allows it to run on consumer hardware, such as a MacBook with 32GB of RAM or a PC with a 32GB GPU. There is significant interest in quantized versions (specifically 4-bit GGUF) to reduce memory footprint and increase speed.

**Practical Setup and Tooling**
Users are actively sharing setup instructions. The consensus is to use `llama.cpp` (specifically `llama-server`) with a 4-bit quantization (e.g., Q4_K_M). One user provided a specific command line example for running the model with CUDA support and accessing it via a local OpenAI-compatible API. There is also a mention of available quantizations on Hugging Face by users like "unsloth" and "ngxson."

**Cost and API Availability**
For those not running locally, users discussed API availability. While z.ai and Novita are listed as providers, the cost-effectiveness is debated. One user shared a negative experience with the Cerebras API (hosting the non-Flash version), citing strict rate limits and high costs due to lack of token caching, making it less practical than competitors like OpenRouter.

**Context and Comparisons**
Users compared the model to the broader landscape, noting that open models continue to lag behind proprietary ones by roughly a year in benchmarks, though the gap is closing. The discussion also touched on the lack of a strong SOTA 8B model, with users pointing to alternatives like Mistral's Ministral 3B or EssentialAI's rnj-1.

---

## [Show HN: I quit coding years ago. AI brought me back](https://calquio.com/finance/compound-interest)
**Score:** 294 | **Comments:** 413 | **ID:** 46673809

> **Project:** The project is a web-based compound interest calculator (and likely a suite of other calculators) built by a developer who had quit coding years ago but was inspired to return by AI tools. The site is built with a modern stack (Next.js, React, TailwindCSS, shadcn/ui) and supports multiple languages. The developer credits AI for enabling rapid prototyping and overcoming the friction of setting up development environments.
>
> **Discussion:** The discussion centers on the transformative impact of AI on productivity and accessibility in software development, particularly for those who had stepped away from coding. Many users shared personal anecdotes of returning to coding or building tools for their work (e.g., a professor, a farmer) because AI drastically reduced setup time and implementation barriers.

However, the conversation also highlighted a significant critique regarding quality. While the project was praised for its concept and the developer's return to coding, the "knowledge base" section was widely criticized as AI-generated "slop" that added little value. This sparked a debate about the responsibility of developers to curate and edit AI output rather than publishing it raw. Additionally, some commenters questioned the project's originality, noting that the core functionality (compound interest calculation) is easily replicable with spreadsheets, and expressed concern over the trend of AI-generated websites on Hacker News.

---

## [Apple testing new App Store design that blurs the line between ads and results](https://9to5mac.com/2026/01/16/iphone-apple-app-store-search-results-ads-new-design/)
**Score:** 265 | **Comments:** 188 | **ID:** 46680974

> **Article:** The article reports that Apple is testing a new App Store design that blurs the line between organic search results and advertisements. The change involves making paid app placements appear more integrated with standard search results, potentially increasing click-through rates and boosting Apple's revenue from its advertising business. This move signals a deeper push by Apple to monetize its App Store traffic through ads, a strategy already heavily utilized by competitors like Amazon and Google.
>
> **Discussion:** The Hacker News community reacted with significant negativity, framing the change as a classic example of "enshittification"—the process by which a platform degrades its user experience to maximize profit. Commenters expressed disappointment that Apple, long perceived as a guardian of a premium, curated ecosystem, is succumbing to the same ad-driven business models as its competitors.

A central theme was the erosion of Apple's reputation for safety and trust. Users argued that the App Store is already plagued by scams, particularly fake or copycat apps that impersonate popular services (like Microsoft Authenticator or ChatGPT). Many believe that further blurring the lines between ads and organic results will only exacerbate this problem, making it easier for users to install malicious or deceptive software, directly contradicting Apple's long-standing marketing of the App Store as a "safe" environment.

Broader economic arguments were also made, with some commenters asserting that this is an inevitable outcome of capitalism, where companies must continually find new ways to grow revenue once market saturation is reached. Others compared the situation to Amazon and Google, where advertising has become a massive profit center, suggesting Apple is simply following a proven, albeit user-hostile, playbook.

Finally, the discussion touched on Apple's innovation and leadership. Some commenters likened the move to a "Ballmer moment" for Apple, suggesting the company has become creatively stagnant under its current leadership and is prioritizing spreadsheets over user experience. The consensus was that while this change might not be enough to make dedicated Apple users switch to Android (which faces similar issues), it represents a significant decline in the company's principles and a further degradation of the digital user experience.

---

## [Article by article, how Big Tech shaped the EU's roll-back of digital rights](https://corporateeurope.org/en/2026/01/article-article-how-big-tech-shaped-eus-roll-back-digital-rights)
**Score:** 261 | **Comments:** 152 | **ID:** 46678430

> **Article:** The article from Corporate Europe Observatory argues that Big Tech lobbying has significantly influenced the European Union to weaken or "roll back" proposed digital rights regulations. It details how major US tech companies have systematically lobbied against strict rules on issues like data privacy, competition, and content moderation. The author contends that this corporate pressure has led the EU to water down legislation, prioritizing the interests of powerful American corporations over the digital rights and protections of European citizens.
>
> **Discussion:** The Hacker News discussion is highly critical of the EU's perceived capitulation to US corporate and political interests, though it also explores the complexities and potential consequences of this dynamic.

A dominant theme is the sentiment that the EU is sacrificing its citizens' rights and sovereignty for the benefit of American tech giants. Commenters express cynicism about the political process, viewing the EU Commission as "pro-corpo" and largely unaccountable to public opinion or street protests. There is a strong call for a consumer-led boycott of American products and services, with links to "Go European" alternatives shared. However, this is tempered by a realistic discussion on the immense difficulty of sustaining such a movement, citing the impracticality of a total boycott and the challenge of maintaining momentum.

Another major thread discusses the broader geopolitical context, particularly the role of the current US administration. Some commenters see a potential "silver lining" in the aggressive US foreign policy, hoping it will force Europe to recognize its over-reliance on US technology and infrastructure, thereby spurring the development of independent European alternatives. The idea of the EU being forced into a "China approach" of creating its own tech ecosystem is raised. However, this is countered by skepticism that any single event, like a conflict over Greenland, would be enough to cause a fundamental shift in policy or public consciousness.

Finally, the conversation touches on the nature of political power and regulation. One commenter offers a counter-narrative, suggesting the EU's regulatory pushback is not just about US pressure but also a response to internal concerns that overly complex laws are stifling European companies' competitiveness, as highlighted in the Draghi report. The discussion also includes a minor debate about the role of far-right MEPs in the lobbying process, with some dismissing it as standard political maneuvering while others see it as a specific cause for concern.

---

## [What came first: the CNAME or the A record?](https://blog.cloudflare.com/cname-a-record-order-dns-standards/)
**Score:** 246 | **Comments:** 91 | **ID:** 46681611

> **Article:** The Cloudflare blog post details an incident where a seemingly minor change to their authoritative DNS servers—reordering CNAME records to appear after A records in responses—caused widespread outages. The author explains that the DNS standard (RFC 1034) is ambiguous regarding the order of records in the answer section. While Cloudflare interpreted the standard to mean that record order within a Resource Record Set (RRset) is insignificant, many DNS clients, including the widely used glibc `getaddrinfo` function and Cisco switch firmware, implicitly relied on CNAMEs appearing first. This strict dependency caused these clients to fail or crash when receiving the reordered records. To resolve the issue, Cloudflare reverted the change and proposed a new standard clarifying that CNAME records must precede other records in the answer section to ensure compatibility with existing infrastructure.
>
> **Discussion:** The Hacker News discussion largely focuses on the surprising nature of the failure and the lessons learned regarding software standards and testing. A central theme is the debate over the clarity of the RFC. Several commenters argue that the "possibly preface" language in RFC 1034 clearly implies that CNAMEs, if present, must come before the final answer, suggesting Cloudflare's interpretation was a misreading rather than genuine ambiguity. Others, however, acknowledge that the standard was ambiguous enough to cause confusion and that the pragmatic solution is to standardize the existing behavior of major clients.

There is significant criticism of Cloudflare's testing procedures. Commenters express shock that a company of Cloudflare's scale would deploy a change without testing against common implementations like glibc, which is ubiquitous in Linux environments. This is viewed as a fundamental oversight rather than an unavoidable edge case. The incident is frequently cited as a real-world example of Hyrum's Law—where users depend on observable behaviors not explicitly defined in the contract—and a failure to adhere to Postel's Law (be liberal in what you accept).

Finally, the discussion touches on the broader implications for the DNS ecosystem. Users share anecdotes about the fragility of DNS clients and the difficulty of maintaining backward compatibility in internet-scale infrastructure. There is a consensus that while randomizing record order might theoretically weed out broken implementations, the practical cost is too high, and the industry must instead codify existing behaviors to ensure stability.

---

## [Wikipedia: WikiProject AI Cleanup](https://en.wikipedia.org/wiki/Wikipedia:WikiProject_AI_Cleanup)
**Score:** 217 | **Comments:** 84 | **ID:** 46677106

> **Article:** The article links to the Wikipedia page for "WikiProject AI Cleanup," a collaborative effort to identify and remove AI-generated content from the encyclopedia. The project aims to maintain Wikipedia's quality and reliability by flagging text that exhibits common traits of LLM output, such as generic phrasing, incorrect tone, or fabricated details, especially when unsourced. It serves as a centralized hub for editors to coordinate this cleanup work.
>
> **Discussion:** The Hacker News discussion reveals a community grappling with the implications of AI on Wikipedia's integrity and future. The central theme is the necessity of the AI Cleanup project, with many commenters expressing concern over the proliferation of low-quality, unsourced AI-generated "slop" that dilutes the value of the encyclopedia. There is a strong consensus that the primary issue isn't the use of AI itself, but the lack of human oversight and verification, which is a core tenet of Wikipedia's model.

Several interesting sub-themes emerged. One is the desire for historical versions of Wikipedia, with users suggesting tools like Kiwix or the "View history" feature to access pre-LLM snapshots, effectively seeking an escape from AI-tainted content. Another theme explores the potential for AI as a tool for good, not just a source of problems. Commenters pointed to research using LLMs to find contradictions within articles or proposed systems for real-time fact-checking against a knowledge graph, suggesting AI could be a powerful assistant for editors rather than just a content generator.

The discussion also touched on the broader context of Wikipedia's relationship with tech companies. While some initially viewed data licensing deals with AI firms negatively, others clarified that these are primarily infrastructure-saving measures, not a betrayal of Wikipedia's principles. A notable technical contribution came from an editor who shared a custom extension they built to flag AI contributions for human verification, demonstrating a proactive, tool-based approach to the problem. Finally, a minor counterpoint was raised about the value of AI-generated text if properly sourced, but this was largely rebutted by the argument that AI output often lacks the quality control and nuance inherent in human-written articles.

---

## [Nvidia contacted Anna's Archive to access books](https://torrentfreak.com/nvidia-contacted-annas-archive-to-secure-access-to-millions-of-pirated-books/)
**Score:** 184 | **Comments:** 114 | **ID:** 46677628

> **Article:** A class-action lawsuit by book authors alleges that NVIDIA, a trillion-dollar company, directly contacted Anna's Archive—a shadow library of pirated books—to secure high-speed access to its dataset for AI training. According to internal documents cited in the lawsuit, NVIDIA executives authorized the use of millions of pirated books to fuel its AI models, specifically for developing large language models (LLMs) showcased at a developer event. NVIDIA is defending its actions as "fair use," arguing that books are merely "statistical correlations" for its AI models.
>
> **Discussion:** The Hacker News discussion largely condemns NVIDIA's actions, viewing the company's "fair use" defense as a disingenuous attempt to justify mass copyright infringement. Commenters argue that while using a single book for research might be fair use, downloading the entire corpus of published literature to train commercial models is not. There is significant skepticism regarding NVIDIA's legal justification, with some noting that copyright laws, particularly in the EU regarding databases, should not favor the company.

The conversation highlights the irony of a hardware manufacturer like NVIDIA resorting to pirated data rather than licensing it, suggesting that even the wealthiest AI players are desperate for high-quality training data. This desperation is contrasted with the industry narrative that synthetic data will eventually solve data shortages. Several users pointed out the hypocrisy of NVIDIA seeking "permission" from a piracy site merely for a faster download pipe, rather than for legal clearance. The discussion also touched on the broader implications for the AI industry, with speculation that other tech giants, like Amazon, have likely already utilized similar datasets, and that the pursuit of profit is driving companies to ignore copyright laws until they are forced to litigate.

---

## [Show HN: Pdfwithlove – PDF tools that run 100% locally (no uploads, no back end)](https://pdfwithlove.netlify.app)
**Score:** 171 | **Comments:** 118 | **ID:** 46675231

> **Project:** The project is "Pdfwithlove," a web-based suite of tools for manipulating PDFs and images that runs entirely in the user's browser. The developer emphasizes that no files are uploaded to a backend, ensuring privacy. The application is currently a web app, but the developer plans to release a Chrome extension and a desktop version, potentially as a paid product (around $2 for lifetime access). The roadmap includes expanding beyond PDFs to include image processing features like cropping, compression, and meme generation.
>
> **Discussion:** Discussion unavailable.

---

## [Ask HN: COBOL devs, how are AI coding affecting your work?](https://news.ycombinator.com/item?id=46678550)
**Score:** 148 | **Comments:** 163 | **ID:** 46678550

> **Question:** The user asks how AI coding tools are affecting developers who work with COBOL, a legacy language heavily used in banking and finance.
>
> **Discussion:** The discussion reveals a mixed but pragmatic view on AI's role in COBOL development. A banking developer notes that COBOL's verbose, English-like syntax aligns well with LLMs, and their team uses fine-tuned models for their codebase. However, others caution that proprietary COBOL variants and closed-source legacy systems limit public training data, potentially hindering general-purpose AI effectiveness.

Practical applications mentioned include code generation, migrations, and using AI as an advanced search tool for documentation. A key theme is that AI is most effective when developers act as architects, carefully prompting and reviewing code rather than expecting "one-shot" production-ready solutions. Skepticism exists regarding AI's ability to handle complex, context-heavy legacy systems without human oversight, with some developers preferring to write code themselves to avoid the time cost of vetting AI output.

The conversation broadens to the longevity of COBOL and Fortran in critical systems, with some asserting these languages will outlast newer ones. A debate also emerged comparing LLMs to compilers, emphasizing that AI's probabilistic nature differs fundamentally from deterministic compilation. Ultimately, the consensus leans toward AI as a productivity aid rather than a replacement, especially in specialized, high-stakes environments like finance.

---

## [The Code-Only Agent](https://rijnard.com/blog/the-code-only-agent)
**Score:** 147 | **Comments:** 64 | **ID:** 46674416

> **Article:** The article "The Code-Only Agent" proposes a minimalist approach to AI agent design. Instead of providing an agent with a large set of specific tools (like "search web," "query database," "read file"), the author suggests giving it a single, powerful tool: the ability to execute arbitrary code (e.g., Python). The agent's primary task then becomes writing and refining scripts to accomplish user goals. This approach is argued to be more flexible, robust, and composable, as the agent can create its own specialized tools on the fly rather than being limited to a pre-defined toolkit.
>
> **Discussion:** The Hacker News discussion reveals a nuanced and multi-faceted debate on the "code-only agent" concept, with commenters exploring its practical applications, limitations, and philosophical implications.

Several users shared their positive experiences with similar approaches. One commenter detailed a system where an agent uses a single `run_bash` tool to create and improve composable CLIs, effectively building its own skillset over time. Another user described a "bottoms-up" approach where an evolving DAG of skills emerges from these created tools. However, a practical counterpoint was raised about the "context loading bottleneck." It was argued that successful agentic tools like Cursor and Claude succeed not because of their toolsets, but because they proactively manage context (e.g., using RAG or loading specific files), saving tokens and time that a code-only agent would waste on discovery tasks like listing directories.

The discussion also branched into architectural considerations. One commenter critiqued the idea of a single code-execution tool as a "myth," suggesting that a multi-agent system with specialized agents for requirements gathering and configuration is more reliable for complex tasks. Another user noted that their experience showed that limiting an agent's action space and permissions at each step, rather than adding more tools or planning layers, significantly improved predictability and reliability. A more abstract perspective suggested that the logical conclusion of this idea isn't just a code tool, but a live programming environment like a Jupyter notebook, where the entire interaction is a timeline of executable code and text.

Finally, the discussion included a humorous, dystopian tangent about an AI generating binary directly on hardware, and a philosophical debate on whether shell commands like `grep` are fundamentally different from other forms of code, with some arguing they represent a "rock solid" and well-understood API that agents should leverage.

---

## [Nearly a third of social media research has undisclosed ties to industry](https://www.science.org/content/article/nearly-third-social-media-research-has-undisclosed-ties-industry-preprint-claims)
**Score:** 141 | **Comments:** 71 | **ID:** 46682534

> **Article:** A preprint analysis of social media research published in top journals found that nearly a third of studies have undisclosed ties to industry, such as prior funding, collaboration, or employment. The study claims that industry-tied research receives more attention from academics, policymakers, and the media, but often focuses less on the impacts of platform-scale features. The authors argue that these opaque relationships compromise the integrity of the research and call for stronger disclosure norms and policies to ensure the visibility of independent work.
>
> **Discussion:** The Hacker News discussion largely expresses cynicism and lack of surprise at the findings, with many commenters viewing undisclosed industry influence as a systemic issue rather than an isolated problem. Several users draw parallels to other industries like pharmaceuticals, fossil fuels, and tobacco, suggesting that corporate influence on research is a widespread and perhaps inevitable consequence of funding structures. There is a notable sense of distrust in institutions, with some commenters expressing a desire to disconnect from modern society due to perceived corruption.

The conversation also touches on the ethics of corporate research, particularly regarding social media platforms' ability to experiment on users without the oversight required in academia. While some defend this as necessary for product development, others express alarm at the lack of independent ethical review. A recurring point is the practical reality that industry often holds the funding and expertise necessary for research in specialized fields, making complete independence difficult to achieve. The discussion concludes with a meta-commentary questioning the potential biases of the study itself, highlighting the pervasive nature of the trust issue.

---

## [Nonviolence](https://kinginstitute.stanford.edu/nonviolence)
**Score:** 139 | **Comments:** 101 | **ID:** 46683410

> **Article:** The linked article from the Martin Luther King, Jr. Research and Education Institute provides a theological and philosophical definition of nonviolence. It explains that for Dr. King, nonviolence was not merely a tactic but a way of life, rooted in the concept of *agape*—disinterested, redemptive love for all humanity. The article distinguishes nonviolence from passive resistance, emphasizing that it is an active force for justice that seeks to convert the opponent rather than defeat them. It outlines six key principles: courage, friendship with the opponent, defeating evil rather than people, accepting suffering without retaliation, avoiding both violence and internal hatred, and believing that the universe is on the side of justice.
>
> **Discussion:** The Hacker News discussion centers on the strategic efficacy and historical context of nonviolence, particularly in social and political movements. The conversation reveals a spectrum of views, from moral idealism to pragmatic game theory.

A dominant theme is the "dual-force" theory, where commenters argue that nonviolence is most effective when it operates alongside a credible threat of violence. Several users posit that nonviolence works because it presents a "good cop" alternative to a "bad cop" (violent faction), making the nonviolent group seem more reasonable and negotiable to authorities. This dynamic is cited in the context of the American Civil Rights Movement, where Martin Luther King Jr.'s nonviolence is seen as having been empowered by the presence of more militant groups like the Black Panthers, shifting the baseline for what constituted a compromise. However, one commenter corrects this timeline, noting that the Black Panthers rose to prominence after many of King's key achievements.

Another major point of discussion is the role of public sympathy and international pressure. Users debate whether nonviolence works by appealing to the morality of the oppressor or by generating sympathy from impartial third parties. The latter is seen as a key strategic advantage, as violence against non-combatants is often counterproductive to building a broad coalition. This is contrasted with the idea that nonviolence fails when appealing to an opponent's "goodness" is ineffective, particularly in conflicts driven by material or economic interests rather than moral ones.

The discussion also touches on the philosophical underpinnings of nonviolence. One commenter introduces the concept of *agape* as a philosophy that transcends a zero-sum, utilitarian view of activism, though others note this is a less common understanding today. There is also a debate over historical causality, with some suggesting that leaders like Gandhi and King benefited from favorable historical circumstances (e.g., post-WWII British sentiment) and that their nonviolent success was not guaranteed.

Finally, the conversation is grounded in empirical data. A user cites research showing that nonviolent movements have a significantly higher success rate (over 40%) compared to violent ones (around 25%) and are more likely to lead to democratic outcomes. This is attributed to the ability of nonviolent movements to grow larger coalitions, as violence tends to alienate potential supporters. However, a counterpoint is raised about correlation versus causation: it's possible that movements with better odds of success from the outset are less likely to resort to violence. The discussion concludes with a pragmatic warning about the harsh realities of modern state responses to nonviolent protest.

---

## [Threads edges out X in daily mobile users, new data shows](https://techcrunch.com/2026/01/18/threads-edges-out-x-in-daily-mobile-users-new-data-shows/)
**Score:** 133 | **Comments:** 126 | **ID:** 46683947

> **Article:** A TechCrunch article reports that Meta's Threads has surpassed X (formerly Twitter) in daily mobile users, citing new data. The article highlights that a year ago, X had twice as many daily active users in the U.S. as it does now, indicating a significant shift in the social media landscape. The growth is partly attributed to Meta's massive user funnel, as Threads requires an Instagram account for signup, providing constant cross-promotion to its 2 billion+ user base.
>
> **Discussion:** The Hacker News discussion offers a skeptical and nuanced take on the reported user metrics, focusing on data interpretation, platform quality, and the broader social media ecosystem.

Several users questioned the data's validity and context. One commenter pointed out a discrepancy between the article's claim of a U.S. user decline and the provided graph, which showed a worldwide drop from 150m to 125m. Another user noted that Threads' growth is heavily influenced by its mandatory Instagram integration, suggesting its "daily active user" achievement is different when built on such a massive distribution network. Skepticism was also voiced based on anecdotal evidence from HN itself, where posts linking to X significantly outnumber those to Threads.

The quality and user experience of both platforms were a major point of contention. Many commenters described X as a "shadow of its former self," citing a perceived decline in engagement, an increase in bots and abandoned accounts, and a shift in its algorithm that shows less content from followed accounts. However, others defended X, particularly within the tech ecosystem, praising its discovery algorithm for surfacing relevant content and noting that many users are passive "lurkers." Threads was described by some as moderately engaging but lacking the unique, high-value information found on X.

A recurring theme was the frustration with the current state of social media, characterized as a battle between "walled gardens" run by billionaires. While one commenter dismissed the entire landscape as "tiresome," others pointed to alternatives like Bluesky, Mastodon, and Lemmy as viable options. A key point of debate was whether Threads is truly a walled garden, with one user highlighting its ActivityPub integration as a step toward the fediverse, though others noted this functionality is still a work in progress.

Finally, broader trends were discussed, including the rise of short-form video platforms like TikTok as a primary competitor to microblogging and the political polarization of X, which some believe has alienated a significant portion of the potential user base.

---

## ["Anyone else out there vibe circuit-building?"](https://twitter.com/beneater/status/2012988790709928305)
**Score:** 131 | **Comments:** 93 | **ID:** 46679896

> **Article:** The post links to a tweet by Ben Eater, a well-known electronics educator, showing a circuit on a breadboard with a component that is visibly smoking or on fire. The image serves as a metaphor for the concept of "vibe circuit-building"—using AI tools (like LLMs) to generate circuit designs without deep understanding, similar to "vibe coding" in software. The tweet implicitly questions the safety and reliability of using generative AI for hardware design, where mistakes can have physical consequences like fire, unlike software crashes.
>
> **Discussion:** The discussion centers on the viability and risks of using Large Language Models (LLMs) for electronics design, using the burning circuit image as a catalyst. Commenters are divided on the utility and safety of "vibe circuit-building."

A significant portion of the conversation highlights the dangers and limitations of current AI tools in this domain. Several users with electronics expertise argue that LLMs often provide outdated or incorrect information, such as recommending obsolete components or flawed circuit topologies. They caution that while software errors can be easily debugged, hardware mistakes can lead to physical damage, fire, or safety hazards, making blind reliance on AI particularly risky for high-voltage or critical applications.

However, other users share positive experiences, noting that AI can be a powerful assistant for learning and exploration. One commenter explains how using LLMs helped them learn about specific components (like Schottky diodes for ADC protection) and navigate complex topics, effectively providing the vocabulary and context needed for further research. The consensus here is that AI works best as a tool to augment human knowledge, not replace it, especially when the user can verify the AI's suggestions.

The discussion also explores potential solutions to mitigate these risks. A key proposal is to use simulation software (like LTspice) as a validation layer in a feedback loop, allowing AI-generated designs to be tested virtually before physical implementation. Another approach, highlighted by a developer of a tool called Phaestus, is to constrain the AI to a library of pre-validated, human-designed circuit blocks. This "selection and integration" model prevents the AI from "hallucinating" novel designs, mirroring how software development relies on trusted libraries rather than writing everything from scratch. The debate then shifts to whether this constraint-based approach is a workaround for AI's current limitations or a fundamental shift in how AI should be applied to engineering.

---

## [The Microstructure of Wealth Transfer in Prediction Markets](https://www.jbecker.dev/research/prediction-market-microstructure)
**Score:** 127 | **Comments:** 115 | **ID:** 46680515

> **Article:** The article analyzes a dataset of 72.1 million trades and $18.26 billion in volume on the prediction market Kalshi from 2021 to 2025. It identifies several key microstructural patterns: a "longshot bias" where low-probability contracts are overpriced; a "wealth transfer" where liquidity makers profit at the expense of liquidity takers; and a significant "optimism tax," where buying "Yes" contracts has a negative expected value while buying "No" contracts is profitable. The study finds that market efficiency varies by category, with finance markets being highly efficient while high-engagement categories like media and world events are inefficient. The core mechanism identified is that makers profit not by having superior forecasts, but by passively selling "Yes" contracts to optimistic takers.
>
> **Discussion:** The discussion on Hacker News centered on the implications of the article's findings, the mechanics of prediction markets, and their broader societal impact.

Several users focused on the core findings, with one providing a relatable analogy of betting against the overpriced positions of a rabid sports fanbase (Notre Dame football). The "optimism tax" and "Yes/No" asymmetry sparked a technical debate about market structure. Users explained that Kalshi's dual order book for "Yes" and "No" shares is a mechanism for margin and that apparent arbitrage opportunities are often nullified by transaction fees.

Broader economic and philosophical points were raised. One user argued that the existence of predictable inefficiencies and negative expected value bets contradicts the theory of perfectly efficient markets. Another user countered that the article's premise is flawed, comparing prediction markets to slot machines, while another pointed out that interest rates are a factor, with Kalshi paying interest on open positions to account for the time value of money.

The conversation also touched on the social and ethical dimensions of prediction markets. Users expressed concern over predatory advertising targeting financially vulnerable individuals and the platforms' resemblance to sports gambling. A significant theme was the potential for manipulation and corruption, with commenters noting that powerful insiders could make lopsided bets on outcomes they control. Finally, some users explored potential positive applications for prediction markets beyond gambling, such as using them to incentivize code review or penalize deception, though others were skeptical of these models.

---

