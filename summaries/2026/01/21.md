# Hacker News Summary - 2026-01-21

## [EU–INC – A new pan-European legal entity](https://www.eu-inc.org/)
**Score:** 583 | **Comments:** 551 | **ID:** 46703763

> **Article:** The article links to a proposal for "EU-Inc," a new pan-European legal entity championed by EU leadership. The initiative aims to create a single, standardized corporate structure to simplify business operations across the European Union. Key goals include allowing entrepreneurs to register a company fully online within 48 hours in any member state, eliminating minimum capital requirements, and establishing a harmonized regulatory framework. The proposal is presented as part of a broader strategy to boost EU competitiveness, alongside creating a deeper Capital Markets Union and an integrated, affordable energy market. The target for implementation is projected to be 2027.
>
> **Discussion:** The Hacker News discussion reveals a mix of optimism and skepticism regarding the EU-Inc proposal, with debate centering on the specific nature of the bureaucratic challenges it aims to solve.

A key theme is the disparity in business formation ease across the EU. Some users, particularly from Germany, strongly support the initiative, describing their national systems as bureaucratic "nightmares" and hoping EU-Inc will bring much-needed agility. However, others counter that company registration is already simple and fast in countries like Sweden, the UK, and Estonia, suggesting the problem is more acute in certain nations than others.

There is a consensus that the true difficulty lies not in incorporation but in ongoing operations and cross-border compliance. Proponents argue that a unified entity would be revolutionary for SMEs by simplifying VAT, employment regulations, and legal compliance when operating across multiple member states. This would prevent the need to shut down and re-establish a company when a director moves residence, for example.

Technical and political concerns are also raised. One commenter notes that the proposal is likely to be implemented as a "directive" rather than a "regulation," which would require individual member states to create their own interpretations, potentially leading to 27 different versions and undermining the goal of uniformity. The proposal's comparison to the existing Societas Europaea (SE) is addressed, with EU-Inc touted as superior due to its lack of a €120,000 minimum capital requirement and a more streamlined, digital-first governance structure.

Finally, the discussion broadens to the wider context of EU economic policy. While the initial post focuses on corporate law, comments branch into the EU's energy market, with some users expressing concern over unification efforts leading to higher prices and taxes. The mention of the EU-Mercosur trade deal also sparks a brief, separate debate on the complexities of international trade agreements.

---

## [Anthropic's original take home assignment open sourced](https://github.com/anthropics/original_performance_takehome)
**Score:** 558 | **Comments:** 276 | **ID:** 46700594

> **Article:** The article links to a GitHub repository containing Anthropic's original performance-oriented take-home assignment for engineering candidates. The assignment tasks the candidate with optimizing a kernel within a custom Python-based simulator of a GPU-like machine. The goal is to minimize the number of CPU cycles required to execute the kernel, with the benchmark being a specific function (`test_kernel_cycles`). The repository includes the simulator code, the unoptimized kernel, and instructions to beat the performance of a solution generated by their AI model, Claude Opus 4.5.
>
> **Discussion:** The Hacker News discussion revolves around the nature of the assignment, the hiring philosophy it represents, and the technical specifics of the challenge.

A central theme is the debate over what this assignment selects for. Some commenters view it as a test of low-level optimization skills, such as GPU architecture knowledge, compiler optimizations, or polyhedral layout algebra, which are niche but critical for AI infrastructure. Others criticize it as a narrow test that favors "nerds" with specific hardware knowledge over general creativity, though one user countered that this is preferable to typical web development take-home projects.

Technically, users dissected the problem. It was clarified that the assignment involves optimizing for a simulated, abstract machine rather than a real-world GPU, though the principles are transferable. Several commenters compared the challenge to "code golf" or the "demoscene," where the goal is extreme performance in a constrained environment. A user currently attempting the assignment noted the difficulty in "packing the vectors" correctly.

There was significant commentary on the tone of the assignment's instructions, which challenge candidates to beat Claude's performance to secure an interview. Some found this "snarky" or arrogant, while others saw it as a confident benchmark. Finally, a few users touched on the broader context of Anthropic, with one suggesting the release was a "DDOS attack" on other AI models (like Gemini) that might struggle to solve it, and another referencing past controversies regarding intellectual property.

---

## [A 26,000-year astronomical monument hidden in plain sight (2019)](https://longnow.org/ideas/the-26000-year-astronomical-monument-hidden-in-plain-sight/)
**Score:** 544 | **Comments:** 106 | **ID:** 46695628

> **Article:** The article from the Long Now Foundation describes a "celestial clock" built into the Hoover Dam, designed by artist Oskar Hansen. The monument uses the dam's architecture to track the 26,000-year precession of the equinoxes. A central ray of light, projected from a tower, marks the North Star (specifically Polaris) on the dam's facade. Because the Earth's axis wobbles over millennia, the North Star will eventually shift, and the monument is designed to point to the next North Star, Thuban, in roughly 14,000 years, and then back to Polaris in 26,000 years. The piece serves as a rare example of modern architecture intended to communicate a message to a distant future civilization.
>
> **Discussion:** The discussion centers on the fascination with ancient and modern attempts to encode astronomical knowledge for future generations. Many commenters expressed awe at the monument's design and the dedication required to observe the night sky before modern technology. There was a notable debate regarding the credibility of Graham Hancock, who mentioned the monument in his books; while one user credited him for introducing the concept, another immediately questioned his reputation as a "fraud."

Technical discussions focused on the mechanics of the 26,000-year precession cycle, with users linking to Milankovitch cycles and the history of the North Star (Thuban, Kochab, and Polaris). Several users debated the necessity of a fixed point for celestial navigation, clarifying that while Polaris is helpful in the Northern Hemisphere, it is not strictly required for the practice. Finally, the conversation touched on the contrast between the artistic ambition of 1930s public works (like the Hoover Dam) and modern architecture, which is often viewed as purely functional and efficient.

---

## [California is free of drought for the first time in 25 years](https://www.latimes.com/california/story/2026-01-09/california-has-no-areas-of-dryness-first-time-in-25-years)
**Score:** 433 | **Comments:** 221 | **ID:** 46698660

> **Article:** A Los Angeles Times article reports that for the first time in 25 years, California is officially free of drought conditions. The U.S. Drought Monitor has removed all drought designations from the state following a period of heavy rainfall and snowfall. The article notes that while reservoirs are replenished, the state's long-term water infrastructure challenges and climate variability remain concerns.
>
> **Discussion:** The Hacker News discussion contextualizes the news with historical patterns, infrastructure debates, and regional comparisons. While some commenters express relief and share personal anecdotes about the lush greenery and recent heavy rains, the prevailing sentiment is one of caution rather than celebration.

Key themes in the discussion include:

*   **Historical Cycles and Memory:** Several users invoked John Steinbeck’s *East of Eden* to illustrate the historical "30-year cycle" of wet and dry years in California, noting that society tends to forget droughts during wet years and vice versa. There was also mention of extreme climate risks like the ARkStorm scenario, which predicts catastrophic flooding that could dwarf current rainfall levels.
*   **Infrastructure and Policy:** A debate emerged regarding California's water management. One user argued that the state's failure to build more dams is the primary cause of water shortages. However, others countered that the best sites for dams are already utilized, and that dams come with significant environmental and financial costs that don't necessarily solve multi-year droughts.
*   **Snowpack and Climate Nuance:** Skepticism was raised about the longevity of the drought-free status due to the warm winter, which has resulted in below-average snowpack levels despite high rainfall. Users noted that snowmelt is crucial for summer water supply, suggesting that full reservoirs now might not guarantee water security later in the year.
*   **Regional Disparity:** Commenters pointed out that while California is drought-free, other parts of the U.S. (specifically mentioned were Utah, Colorado, and general "flyover country") are experiencing severe dry conditions. This highlighted a perception of media focus being disproportionately centered on California while other agricultural regions face water crises.
*   **Economic Reality:** Despite the end of the drought, users noted that water rates remain high. The consensus was that the costs of water infrastructure and delivery are long-term investments unrelated to short-term rainfall fluctuations.

---

## [cURL removes bug bounties](https://etn.se/index.php/nyheter/72808-curl-removes-bug-bounties.html)
**Score:** 382 | **Comments:** 211 | **ID:** 46701733

> **Article:** The article reports that the cURL project has discontinued its bug bounty program on HackerOne. The primary reason cited is the overwhelming volume of low-quality, automated, and AI-generated submissions, referred to as "slop." These reports are often nonsensical, false positives, or demonstrate a fundamental misunderstanding of the software, wasting the project maintainers' time and resources. The decision highlights a growing problem where the incentive of bounties is exploited by bad actors using AI tools to spam vulnerability reporting systems.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, with commenters sharing similar experiences and analyzing the root causes. The consensus is that AI tools, particularly large language models (LLMs), are the primary driver of this "slop" problem. Several users shared anecdotes of receiving AI-generated, nonsensical security reports for their own projects, filled with false positives and technically impossible claims.

The conversation explores potential solutions and their drawbacks. A popular suggestion was to implement an entry fee for submissions, reimbursed only for valid reports, to create a barrier against spam. However, others noted the difficulty in defining what constitutes a "reasonable" vulnerability, which could unfairly penalize legitimate but inexperienced reporters. The idea of using AI to filter out AI-generated slop was also mentioned, but met with skepticism and irony about fighting AI with AI.

Broader implications were also discussed. Some commenters argued that open-source projects are disproportionately affected, as they are both trained on open-source code and are frequent targets of such spam due to their public nature. The problem was also linked to a wider trend of "security theater," where compliance and metrics (like the number of reports submitted) are prioritized over genuine security. Ultimately, the discussion concluded that while bounties are a motivator, the low cost and ease of generating AI submissions have created a system that is easily abused, forcing projects like cURL to abandon the model.

---

## [IPv6 is not insecure because it lacks a NAT](https://www.johnmaguire.me/blog/ipv6-is-not-insecure-because-it-lacks-nat/)
**Score:** 282 | **Comments:** 397 | **ID:** 46696303

> **Article:** The article argues that the common belief that IPv6 is insecure due to the lack of Network Address Translation (NAT) is a misconception. The author clarifies that NAT was primarily introduced to solve IPv4 address exhaustion, not to provide security. While NAT can incidentally provide a layer of obscurity by hiding internal network topology, its security benefits are often overstated and can be bypassed. The article emphasizes that proper security in IPv6 should be managed by stateful firewalls, which can be configured to deny unsolicited inbound traffic just as effectively as a NAT device, ensuring that only requested connections are allowed. The author concludes that IPv6's lack of default NAT does not inherently make it less secure than IPv4.
>
> **Discussion:** The discussion on Hacker News reveals a nuanced and often contentious debate about the role of NAT in network security. A central theme is the clarification that NAT's primary function is address conservation, not security, a point supported by several commenters who reference its origins in RFCs. However, a significant counterpoint emerges, with some users, including a self-identified network engineer, arguing that while NAT isn't a designed security feature, it provides a crucial, material layer of security as a side effect. This "security through obscurity" is valued because it makes internal devices unreachable from the internet by default, forcing users to explicitly configure port forwarding.

The conversation then delves into the practical differences between IPv4 and IPv6 environments. Commenters debate the default behavior of routers, with some noting that routers don't inherently drop packets that don't match a NAT rule, but rather route them to their destination IP. This leads to a discussion on the importance of default-deny firewalls in an IPv6 world where every device can have a public IP address. The security implications of this shift are a major concern; while IPv6's vast address space makes random scanning infeasible, misconfigured firewalls or user-enabled services (like UPnP) can expose individual devices directly to the internet, a risk that is less pronounced in a typical IPv4/NAT setup where only one device is typically exposed by default.

Finally, the discussion touches on practical realities and alternative solutions. Some commenters mention that IPv6 NAT does exist (e.g., for Unique Local Addresses), but it's not common. A more nuanced solution of "Prefix-Translation" is also brought up, which can simplify network renumbering while providing some of the topology-hiding benefits of traditional NAT. The debate concludes with a pragmatic acknowledgment that while technical experts may agree on the principles, the perception of NAT as a security feature persists and can influence real-world decisions, such as corporate audits that may still favor IPv4 environments.

---

## [How AI destroys institutions](https://cyberlaw.stanford.edu/publications/how-ai-destroys-institutions/)
**Score:** 275 | **Comments:** 213 | **ID:** 46705606

> **Article:** The article, "How AI Destroys Institutions," argues that AI, particularly generative AI, poses a fundamental threat to the integrity and function of purpose-driven institutions like universities, courts, and hospitals. The author posits that these institutions rely on established processes and shared epistemic standards to manage complexity and reduce chaos. AI tools can bypass these processes, offering seemingly authoritative answers without the underlying expertise or accountability. This "frictionless" access erodes the institutional authority and the professional judgment that underpins them. The paper uses examples like the FDA's use of AI for recalls and the potential for AI to generate legal or medical arguments to illustrate how AI can degrade the quality of work and undermine the trust necessary for these institutions to function. The core thesis is that AI doesn't just automate tasks; it fundamentally changes the relationship between individuals and the complex systems that structure society, potentially leading to institutional collapse.
>
> **Discussion:** The Hacker News discussion is highly critical and skeptical of the paper's thesis and academic rigor. A dominant theme is the questioning of the paper's credibility. Several commenters point out that the paper is a draft, not peer-reviewed, and relies on weak evidence, such as citing a news article (Engadget via CNN) based on anonymous sources to critique the FDA's use of AI. This leads to accusations that the paper is more of an "opinion piece" than a substantive academic work, with one user noting the irony of a Stanford paper being poorly cited.

Another major thread of discussion pushes back on the premise that AI is the primary destructive force. Commenters argue that institutions were already in a state of decay due to factors like high costs (in healthcare and education), low public trust, and the pre-existing impact of social media. From this perspective, AI is not the cause of destruction but rather an accelerant or a symptom of deeper institutional failures. One user states, "Institutions bear some responsibility for what makes AI so attractive," highlighting that AI's appeal often stems from the shortcomings of existing systems.

Finally, there is a recurring sentiment of anti-elitism and dismissal of the paper's academic framing. The title is seen as hyperbolic and off-putting, and the author's background at Stanford is used to critique the institution itself. Some commenters view the paper as a defensive reaction from knowledge professionals (like lawyers) whose fields are being disrupted. While a few users defend the paper's core ideas or the importance of opinion in academic discourse, the overwhelming consensus is one of skepticism toward the paper's evidence, argument, and perceived alarmism.

---

## [Instabridge has acquired Nova Launcher](https://novalauncher.com/nova-is-here-to-stay)
**Score:** 239 | **Comments:** 161 | **ID:** 46696357

> **Article:** Instabridge has acquired Nova Launcher, as announced on the official Nova website. The post, titled "Nova is here to stay," attempts to reassure users that the launcher will continue to be maintained. It states that data collection will be minimal and purpose-driven, that personal data will not be sold, and that Nova Prime will remain ad-free. The company also mentions exploring ad-based options for the free version to support development.
>
> **Discussion:** The Hacker News community reacted with deep skepticism and concern regarding the acquisition. The primary point of contention is the timing of the announcement, which follows recent news that Nova Launcher's original founder and sole remaining developer, Kevin Barry, left the company after being instructed to stop working on an open-source release. Users view the acquisition as a classic "bait and switch," fearing the "here to stay" promise is a prelude to the app's decline or "enshittification."

Discussion heavily focused on privacy and data tracking. Commenters pointed to a recent update that added Facebook and Google Ads tracking, viewing this as a direct contradiction of the new owners' promises. A common cynical interpretation was that the company would technically avoid selling "personal" data by selling "anonymized" data, a distinction users find meaningless.

Many users are actively seeking alternatives. Popular suggestions included Lawnchair (an open-source launcher) and AIO Launcher. The conversation also highlighted a broader trend of acquisitions leading to the degradation of beloved software, with users sharing examples of other apps ruined by buyouts. The overall sentiment is one of betrayal and a strong recommendation for users to switch launchers.

---

## [The challenges of soft delete](https://atlas9.dev/blog/soft-delete.html)
**Score:** 236 | **Comments:** 136 | **ID:** 46698061

> **Article:** The article "The challenges of soft delete" explores the common practice of marking records as deleted (e.g., via a `deleted_at` timestamp) rather than physically removing them from the database. It outlines the primary motivation for this approach: the ability to recover data easily and maintain historical context. However, the article details significant downsides, including performance degradation as tables accumulate "dead" rows, the complexity of ensuring every query correctly filters out deleted records (risking data leaks), and complications with data integrity and foreign key constraints. It also touches on compliance issues, noting that while soft delete aids data retention policies, it conflicts with "right to erasure" regulations like GDPR or CCPA unless paired with a separate, managed archival and purging process. The piece concludes that while soft delete solves the problem of accidental data loss, it introduces substantial technical and operational debt.
>
> **Discussion:** The Hacker News discussion reveals a nuanced debate on data retention strategies, with commenters weighing the technical, operational, and legal trade-offs of soft deletion versus hard deletion.

A central theme is the technical implementation and performance impact. Several users proposed alternatives to a simple `deleted_at` flag to mitigate performance issues and complexity. A popular NoSQL approach is moving deleted records to a separate collection, which keeps primary queries fast and eliminates the need for filters. In relational databases, users suggested using views or PostgreSQL's Row-Level Security (RLS) to abstract the filtering logic from the application code, ensuring deleted rows are automatically hidden. Others mentioned using table partitioning to physically separate active and deleted data while keeping it in the same logical table, or employing database triggers for automated archiving.

The discussion also highlighted the tension between data retention for business value and the risks of holding data. Many commenters championed the "never delete" philosophy, arguing that storage is cheap and historical data is invaluable for analytics, debugging, and future business intelligence. This was contrasted with the "data as a toxic asset" perspective, where the security, privacy, and maintenance costs of retaining data outweigh its potential value. The legal landscape was identified as a critical factor, with soft delete being mandatory in some jurisdictions (for data retention) and a direct violation in others (for right-to-erasure requests like GDPR), necessitating robust, time-based purging processes.

Finally, the conversation touched on practical challenges like schema drift. While some argued that archived data is rarely accessed and schema changes are a low risk, others shared experiences where migrating or making sense of old, archived data became a difficult and error-prone project. The consensus was not absolute; the choice depends heavily on the specific domain (e.g., banking vs. social media), data volume, and regulatory environment.

---

## [Show HN: ChartGPU – WebGPU-powered charting library (1M points at 60fps)](https://github.com/ChartGPU/ChartGPU)
**Score:** 236 | **Comments:** 85 | **ID:** 46706528

> **Project:** ChartGPU is a new open-source charting library that leverages WebGPU to render high-performance data visualizations directly in the browser. Its primary selling point is the ability to handle 1 million data points at 60 frames per second, targeting use cases like time-series, financial charts, and dashboards. The project is built with modern web technologies and is currently in early development.
>
> **Discussion:** The response to ChartGPU was overwhelmingly positive, with users impressed by the performance and smoothness of the demos, particularly on high-end hardware. Several commenters compared its potential to established libraries like three.js, noting its specialized focus on 2D data visualization.

The discussion highlighted a few key areas:
*   **Browser Compatibility:** A significant point of conversation was WebGPU support. While the library works well in Chromium-based browsers (Chrome, Edge), users reported issues with Firefox (which requires manual flag changes and is platform-dependent) and noted that Safari lacks support. One user found that enabling the Vulkan renderer in Chrome flags was necessary for it to work on Linux.
*   **User Experience Bugs:** A couple of bugs were identified and acknowledged by the developer. A Windows and Mac user reported that a data zoom slider's scroll behavior was too fast, and another user found that interactive buttons on a candlestick streaming demo were non-functional.
*   **Developer Engagement & Roadmap:** The creator ("huntergemmer") was highly responsive, actively engaging with feedback, logging bugs, and discussing future plans. They mentioned considering a "pro" tier for enterprise features while keeping the core library open-source (MIT license). A notable feature suggestion from a user—adding a built-in performance benchmarking tool—was well-received and added to the project's roadmap.
*   **Minor Criticism:** One user made a critical comment about the presence of `.cursor` and `.claude` folders, implying the project was AI-generated "slop." This was quickly countered by another community member who argued that the quality of the results should be judged over the tools used to create them.

---

## [SETI@home is in hiberation](https://setiathome.berkeley.edu/)
**Score:** 215 | **Comments:** 111 | **ID:** 46703301

> **Article:** The article links to the SETI@home website, which announces that the project is now in hibernation. The project, which utilized distributed computing to analyze radio telescope data for signs of extraterrestrial intelligence, has stopped distributing new work units. This decision follows the completion of the data processing for the Arecibo Observatory data collected through October 30, 2020. The team is now focused on analyzing the accumulated results and preparing scientific publications, marking the end of the active data collection and processing phase for the project.
>
> **Discussion:** The discussion is a mix of nostalgia, clarification of the project's status, and comparisons to similar distributed computing projects. Many commenters expressed strong sentimental attachment to SETI@home, recalling their childhood experiences running the distinctive screensaver on old hardware like Pentium II and Pentium 4 processors. This sparked a secondary conversation about the cultural context of the late 90s and early 2000s, with users reminiscing about *The X-Files* and the era's "fun" conspiracies.

Several users clarified the reason for the recent news, pointing out that while the project has been in hibernation for a few years, a final scientific paper was recently published, prompting the official announcement. The conversation also pivoted to the current state of other distributed computing projects. Folding@home was mentioned as still being active, though one commenter questioned its relevance in the age of AI-driven protein folding models like AlphaFold. Other active BOINC projects, such as climateprediction.net, were also highlighted. A few users expressed a desire to run the old screensaver again for nostalgia's sake, while others searched for modern alternatives that offer a similar visual experience.

---

## [Stories removed from the Hacker News Front Page, updated in real time (2024)](https://github.com/vitoplantamura/HackerNewsRemovals)
**Score:** 214 | **Comments:** 139 | **ID:** 46704555

> **Article:** The linked GitHub repository, "HackerNewsRemovals," provides a real-time log of stories that have been removed from the Hacker News front page. It tracks these removals, offering a transparent look at what content is being moderated out of public view on the popular tech forum.
>
> **Discussion:** The discussion reveals a community grappling with the nature and effectiveness of HN's moderation. A central theme is the tension between maintaining a focused, high-quality tech forum and addressing the reality that technology now intersects heavily with politics and society.

Several users express concern that important tech-related political debates are being "silenced by mass flagging," making it difficult to discuss topics like Elon Musk's actions or government technology policy. This is seen by some as a dangerous avoidance of reality, while others argue that the "silent majority" works to keep the site free of political echo chambers and ads, a effort they value. One user suggests that HN's apolitical stance can sometimes feel like a nihilistic refusal to engage with important issues.

Another prominent topic is moderation itself. While many users, including one who would "pay for HN because of Dan," express deep appreciation for the moderation team's efforts, there are calls for more transparency in the flagging system. One user alleges a pattern where posts critical of right-wing causes are disproportionately flagged, raising questions about whether this is spontaneous or the work of a coordinated group.

Finally, there is a noticeable user fatigue with the prevalence of LLM-related news, with some commenters drawing parallels to past tech-naming trends (e-something, i-something) and expressing a desire for more diverse and "interesting" content on the front page.

---

## [The Agentic AI Handbook: Production-Ready Patterns](https://www.nibzard.com/agentic-handbook)
**Score:** 193 | **Comments:** 126 | **ID:** 46701969

> **Article:** The article "The Agentic AI Handbook: Production-Ready Patterns" is a comprehensive guide that consolidates various techniques and patterns for developing AI agents, particularly in the context of coding and software development. It aims to standardize the vocabulary and provide a structured approach to building reliable, production-ready agentic systems. The handbook covers strategies for agent coordination, handling failures, and managing complex workflows, positioning itself as a practical resource for developers navigating this emerging field.
>
> **Discussion:** The Hacker News discussion reveals a deep divide in the community's perception of agentic AI, ranging from enthusiastic adoption to profound skepticism. A significant portion of the conversation centers on the practical difficulties and high cognitive overhead of using agents. Several developers shared experiences where managing agent behavior—preventing them from "going off the rails" and fixing downstream regressions—felt more burdensome than performing the tasks manually. This sentiment is captured in the comment that the "pipe dream" of automated issue-to-PR workflows often becomes a "nightmare" of cleanup.

Conversely, other users see value in the handbook as a necessary step toward taming this new paradigm. They argue that standardizing patterns and vocabulary is crucial for learning and effective collaboration, much like established software engineering practices such as TDD. However, a strong undercurrent of skepticism questions the author's credibility and the article's origin, with some dismissing it as "slop" or AI-generated content. This cynicism extends to the broader "AI strategy" trend, which some view as snake oil for managers.

Beyond the specific article, the discussion touched on broader themes of tooling and learning. Some commenters felt that the complexity of "agentic" systems is overblown and that simply using a tool like Claude Code is sufficient, while others expressed concern that the ecosystem is moving toward specialized IDEs, leaving traditional tools behind. Ultimately, the debate reflects a field in its early, chaotic stages, with developers grappling with the gap between the hype of automation and the reality of its current implementation challenges.

---

## [RSS.Social – the latest and best from small sites across the web](https://rss.social/)
**Score:** 192 | **Comments:** 44 | **ID:** 46700503

> **Article:** RSS.Social is a new website that aggregates and displays the latest posts from a curated list of small, independent websites. It aims to help users discover content from the "small web" by presenting a simple, feed-like interface of recent articles and blog posts from various personal sites.
>
> **Discussion:** The HN community's response to RSS.Social is largely positive, with users appreciating the curation of small web content and the general revival of RSS-based discovery. However, the discussion quickly expands to cover practical improvements and alternative tools.

Key points of feedback for RSS.Social include:
*   **Feature Requests:** Multiple users suggest adding one-sentence summaries for each post to improve browsing efficiency and reduce the friction of clicking on uninteresting titles.
*   **Clarification of Curation:** Commenters ask for more transparency on how "best" is determined, suggesting the creator define the content criteria or curation philosophy.
*   **Technical Issues:** One user reported being blocked by Cloudflare, raising questions about potential geoblocking or access restrictions.

A significant portion of the discussion revolves around alternative services for discovering small web content:
*   **Kagi's Small Web:** Several users highlight Kagi's Small Web feature as a powerful, existing solution. They share links to its main feed, an "appreciated" feed (curated by likes), and specialized feeds for topics like code and videos.
*   **Other Aggregators:** Commenters also mentioned other similar projects, such as `hcker.news` (which filters Kagi's feed), `indieblog.page` for random blog discovery, and `minifeed.net`, another directory and reader for personal blogs.

Finally, there were minor notes on the service's branding (the `.social` TLD) and a user mentioning their own "RSS Tinder" project for personal feed filtering.

---

## [Which AI Lies Best? A game theory classic designed by John Nash](https://so-long-sucker.vercel.app/)
**Score:** 181 | **Comments:** 74 | **ID:** 46698370

> **Article:** The article presents a study that uses "So Long Sucker," a 1950 negotiation and betrayal game designed by John Nash, as a benchmark to test deception capabilities in modern LLMs. The study involved 162 AI vs. AI games across models like Gemini 3 Flash, GPT-OSS 120B, Kimi K2, and Qwen3 32B. Key findings suggest that simple benchmarks underestimate deceptive capabilities, as GPT-OSS dominated in simple games but collapsed in complex ones, while Gemini showed the opposite trend. The study highlights Gemini's ability to use "alliance banks" to manipulate other players and its tendency to be more exploitative against weaker models. An interactive demo and full methodology are provided for further exploration.
>
> **Discussion:** The Hacker News discussion reveals a mix of fascination with the study's premise and significant skepticism regarding its execution and presentation. Many commenters were intrigued by the application of game theory to AI deception, with several suggesting similar games like Diplomacy as future benchmarks and sharing links to related research and YouTube videos of AIs playing social deduction games like Mafia.

However, the primary focus of the discussion was criticism of the study's presentation and the interactive demo's functionality. Multiple users reported bugs in the demo, such as the game getting stuck or bots repeating themselves, which undermined their trust in the results. The writing style of the article was also heavily criticized as "brainless AI writing," making it difficult for readers to draw credible conclusions. While the author was responsive, fixing a reported issue and adding a game rules section, the initial user experience left a lasting negative impression. Other users shared links to alternative LLM game benchmarks and a "Turing test battle royale" game.

---

## [The percentage of Show HN posts is increasing, but their scores are decreasing](https://snubi.net/posts/Show-HN/)
**Score:** 180 | **Comments:** 135 | **ID:** 46702099

> **Article:** The article analyzes data from Hacker News to show that while the percentage of "Show HN" posts has been increasing over time, their average score has been decreasing. The author suggests this indicates a decline in the perceived quality or engagement with these posts, despite their growing volume.
>
> **Discussion:** The discussion centers on the perceived decline in quality of "Show HN" posts, with a strong consensus that the rise of AI-generated projects is the primary cause. Commenters describe the influx as "slop," "AI DOS," and "vibe-coded," leading to community fatigue and making it harder for non-AI or genuinely innovative projects to get attention. Several users shared personal anecdotes of their projects receiving zero interaction, blaming the noise and the fact that "Show HN" posts get less visibility than regular submissions.

A key theme is the changing purpose of the section. Moderators and long-time users argue that "Show HN" has shifted from a place for sharing deep, educational work to a marketing funnel for startups and landing pages. There's a call to return to the original goal of showcasing substance and innovation rather than product launches.

Broader implications were also debated. Some see this as a microcosm of a larger trend where low barriers to creation flood the market with mediocre content, making marketing and trust more critical than ever. While some are optimistic that the technology enables more people to build interesting things, the prevailing sentiment is one of concern about the erosion of quality and the difficulty of surfacing valuable work in an era of AI-driven content.

---

## [Claude Chill: Fix Claude Code's flickering in terminal](https://github.com/davidbeesley/claude-chill)
**Score:** 153 | **Comments:** 114 | **ID:** 46699072

> **Article:** The article links to a GitHub repository for "claude-chill," a tool created by a developer to fix a persistent flickering and screen-scraping issue in the Anthropic CLI tool, Claude Code. The tool works by intercepting and modifying the terminal output to stabilize the display, addressing a bug that has been a source of frustration for many users.
>
> **Discussion:** The discussion is overwhelmingly positive and grateful toward the developer who created the fix, with many users expressing relief from headaches and visual discomfort caused by the flickering. However, the gratitude quickly pivots into sharp criticism of Anthropic. Commenters express disbelief that a company building advanced AI coding assistants cannot fix a basic terminal UI bug, viewing it as a sign of a messy internal codebase. The irony is a central theme: the company championing "vibe coding" and AI-generated software is being outperformed by a human developer fixing a fundamental user experience issue. Several users contrast the experience with other CLI tools like Codex and Gemini, which they claim do not have similar problems. There is also skepticism about whether the fix itself was AI-generated, with one user joking that Anthropic might have acquired the Bun runtime to increase flickering speed.

---

## [Maintenance: Of Everything, Part One](https://press.stripe.com/maintenance-part-one)
**Score:** 147 | **Comments:** 29 | **ID:** 46696276

> **Article:** The article links to the introduction of Stewart Brand's new book, "Maintenance: Of Everything, Part One," published by Stripe Press. Brand, known for the "Whole Earth Catalog" and the "Clock of the Long Now," argues that maintenance is the hidden, essential process that keeps civilization running. He posits that while we celebrate innovation and creation, we undervalue the constant effort required to sustain systems, objects, and societies. The introduction frames maintenance as a fundamental principle of order against entropy, using examples from the maintenance of sailing vessels to complex infrastructure. The book aims to explore the culture, skills, and mindset required for long-term stewardship.
>
> **Discussion:** Discussion unavailable.

---

## [Tell HN: Bending Spoons laid off almost everybody at Vimeo yesterday](https://news.ycombinator.com/item?id=46707699)
**Score:** 145 | **Comments:** 75 | **ID:** 46707699

> **Post:** A user on Hacker News reported that Bending Spoons, an Italian tech company, has laid off almost all of Vimeo staff following its acquisition of the video platform for $1.38 billion. The post serves as an alert to the community about the significant workforce reduction.
>
> **Discussion:** The discussion centered on Bending Spoons' aggressive business model, which users described as a "next generation private equity" strategy. Commenters noted a clear pattern from the company's previous acquisitions of Evernote and WeTransfer: acquiring a product, drastically cutting staff and operational costs, rewriting the code with a minimal team, and restricting free tiers to increase revenue.

Users expressed concern that this approach prioritizes short-term profit over long-term growth and community health. While some acknowledged that Bending Spoons might be efficient at maintaining products, many criticized the resulting price hikes and feature limitations. Specific worries were raised regarding the future of Vimeo's business customers (such as Dropout and MST3K) and the potential loss of the platform's value proposition compared to competitors like YouTube. Several users shared personal anecdotes about canceling their Vimeo subscriptions due to rising costs, and one developer offered a free alternative to Evernote for those looking to migrate.

---

## [Disaster planning for regular folks (2015)](https://lcamtuf.coredump.cx/prep/index-old.shtml)
**Score:** 144 | **Comments:** 99 | **ID:** 46700809

> **Article:** The article "Disaster Planning for Regular Folks" is a practical guide to emergency preparedness written by security researcher Michał Zalewski (lcamtuf). It argues that individuals should focus on realistic, high-probability disasters (like power outages, job loss, or natural disasters) rather than low-probability "end of the world" scenarios. The guide provides a structured framework for assessing personal risk, building a 72-hour kit, securing finances, and planning for communication and shelter. It emphasizes common sense, proportionality, and psychological readiness over extreme "prepper" tactics.
>
> **Discussion:** The Hacker News discussion largely validates the article's pragmatic approach, with users sharing personal experiences and debating the nuances of preparedness. A recurring theme is the importance of prioritizing mundane, high-probability risks over cinematic catastrophes. Several commenters shared stories of how basic preparations (like stored food or savings) proved invaluable during routine hardships like unemployment or power outages.

The conversation evolved into a broader debate on the philosophy of preparedness. Many argued that true resilience lies in community and social networks rather than solitary survivalism. The "hermit prepper" fantasy of a self-sufficient bunker was widely criticized as unrealistic and vulnerable to organized threats; instead, building local relationships and mutual aid was seen as a more robust strategy. This was contrasted with a more pessimistic view, particularly from a commenter in California, who detailed a cascade of systemic failures (insurance crises, infrastructure decay, economic pressure) suggesting that societal stability itself is a primary risk factor.

The discussion also touched on the role of firearms, with some viewing them as historical artifacts or tools for last-resort defense, while others questioned their effectiveness against larger threats in a true collapse scenario. Practical advice was shared, such as using meal prepping as a natural form of food storage and leveraging resources like Finland's 72hours preparedness website. Overall, the community consensus leaned toward a balanced, community-oriented approach to preparedness, grounded in the realities of modern life rather than apocalyptic fantasies.

---

