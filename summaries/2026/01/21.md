# Hacker News Summary - 2026-01-21

## [Danish pension fund divesting US Treasuries](https://www.reuters.com/business/danish-pension-fund-divest-its-us-treasuries-2026-01-20/)
**Score:** 702 | **Comments:** 703 | **ID:** 46692594

> **Article:** A Danish pension fund, AkademikerPension, has announced its divestment from US Treasuries, citing poor US government finances and the need to find alternative liquidity and risk management strategies. The fund is selling approximately $100 million in US debt. While the financial impact is negligible relative to the daily volume of the US Treasury market, the move is viewed as a symbolic signal of declining confidence in US fiscal stability among international investors.
>
> **Discussion:** The Hacker News discussion centered on the symbolic significance of the divestment and the broader implications for US fiscal health and global standing. While some commenters noted that $100 million is a "drop in the bucket" compared to the trillions traded daily, others argued that such moves could signal a larger trend or "domino effect" if repeated by other funds.

The conversation quickly pivoted to the root causes of US fiscal instability. A significant portion of the debate focused on US debt and deficits, with users blaming political gridlock, tax cuts, and mandatory spending (Social Security, Medicare, and military) for an unsustainable trajectory. There was a partisan divide on the causes, with some blaming recent tax cuts for the wealthy and others pointing to a lack of spending cuts. Historical context was provided, noting that deficit spikes were often triggered by specific shocks like 9/11, the Iraq War, and Covid-19, rather than a steady trend.

Underlying the financial concerns was a geopolitical anxiety regarding the US's reliability. Commenters debated the potential end of US dollar hegemony, linking it to political instability and the perceived erosion of the "US brand." The discussion touched on the role of the US Navy in maintaining global reserve currency status and drew parallels to the fall of the Roman Empire. Several users highlighted recent geopolitical shifts, such as France looking to China for investment and Canada seeking cheaper Chinese cars, as evidence of a "rupture" in the global order where the US is no longer seen as a stable partner.

Finally, the discussion became highly politicized regarding US leadership. Some commenters blamed specific political figures and their supporters for the global instability, while others argued that the issue is systemic and supported by a significant portion of the electorate. The conversation also touched on the concept of the US "siphoning" global wealth through the dollar's reserve status, suggesting that fiscal recklessness could force the "bill" to come due.

---

## [De-dollarization: Is the US dollar losing its dominance? (2025)](https://www.jpmorgan.com/insights/global-research/currencies/de-dollarization)
**Score:** 597 | **Comments:** 795 | **ID:** 46693346

> **Article:** The article from J.P. Morgan explores the concept of "de-dollarization," examining whether the U.S. dollar is losing its status as the world's dominant reserve currency. It likely analyzes trends in global foreign exchange reserves, the impact of geopolitical shifts, and the role of alternative currencies like the Euro and the Chinese Renminbi. The piece assesses the structural factors supporting the dollar's dominance—such as the depth of U.S. financial markets and the network effects of the dollar—while weighing them against growing pressures from U.S. debt levels, political uncertainty, and strategic efforts by other nations to reduce reliance on the dollar for trade and reserves.
>
> **Discussion:** The Hacker News discussion is heavily focused on the political drivers behind potential de-dollarization, with a significant portion of commenters attributing recent volatility to the Trump administration's policies. Many users express concern that intentional actions or perceived instability are eroding global trust in the dollar, with one user noting that the decline is a "monetary penalty for behaving in unpredictable ways."

Beyond politics, users highlight the gradual nature of the shift, pointing out that the dollar's share of global reserves has fallen from over 70% in the 1990s to around 60%. This is attributed to the natural emergence of viable alternatives, such as the Euro, which did not exist as a strong option previously. Several commenters invoke economic theory, specifically the Triffin Dilemma, to explain the inherent tension between the dollar's reserve status and the U.S. trade deficit.

There is also debate regarding the mechanics of currency valuation. Some users argue that a weaker dollar is a necessary tool to make U.S. exports more competitive, while others warn that devaluation through instability leads to reduced purchasing power without economic benefit. Finally, a few users touched on the role of cryptocurrencies and gold, though these points were largely dismissed or questioned as irrelevant to the specific discussion of sovereign reserve currencies.

---

## [I'm addicted to being useful](https://www.seangoedecke.com/addicted-to-being-useful/)
**Score:** 546 | **Comments:** 286 | **ID:** 46690402

> **Article:** The article "I'm addicted to being useful" explores the author's personal compulsion to be constantly productive and helpful, particularly in a software engineering context. He frames this not as a virtue but as a "dysfunction" that serves as a coping mechanism for anxiety and a way to feel in control. The author admits that this drive makes him an effective employee but also leads to burnout and an inability to relax. He contrasts this with a healthier mindset of simply being present and enjoying non-productive time, concluding that his addiction to usefulness is a double-edged sword that fuels his career but harms his personal well-being.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, with many commenters relating to the feeling of being "addicted to being useful." The conversation quickly branches into several key themes:

A significant portion of the discussion focuses on the negative consequences of this mindset, particularly in leadership and personal relationships. Multiple engineers noted that while this drive is effective for individual contributors, it becomes counterproductive for managers, as constantly solving problems for a team can stifle growth and be perceived as overbearing. In personal relationships, the compulsion to offer solutions instead of just listening can be exhausting for partners who simply want to be heard, not "fixed."

Commenters also explored the underlying psychology of this behavior. Some suggested it can be a form of control or a way to seek validation, questioning whether the addiction is to being useful itself or to being *recognized* as useful. The discussion also touched on the risk of burnout, with several users sharing their own experiences of exhaustion and the need to consciously guard their personal time to recover.

Finally, the conversation connected the theme to broader societal and philosophical ideas. One commenter linked the need for purpose to a quote about the horrors of idleness in social housing, which sparked a sub-thread on the complexities of disability and work requirements in welfare systems. Others referenced philosophical concepts, like Nietzsche's "will to power," and literary works, such as Tracy Kidder's "The Soul of a New Machine," as relevant explorations of the same drive in the tech world.

---

## [A 26,000-year astronomical monument hidden in plain sight (2019)](https://longnow.org/ideas/the-26000-year-astronomical-monument-hidden-in-plain-sight/)
**Score:** 477 | **Comments:** 92 | **ID:** 46695628

> **Article:** The article from the Long Now Foundation describes a massive, hidden astronomical monument at the Hoover Dam. Designed by artist Oskar J.W. Hansen, the monument includes a celestial map and a "Star Dial" that uses the North Star's position to mark the year 14,000 AD. This was intended to be a time capsule for a future civilization, using the precession of the equinoxes—a 26,000-year cycle—to communicate a specific date far in the future. The design serves as a tribute to the engineers who built the dam and as a message to distant generations, relying on the predictable movement of the stars to convey its meaning.
>
> **Discussion:** The discussion on Hacker News was multifaceted, with users exploring the monument's astronomical, historical, and cultural significance. A central theme was the scientific basis of the monument, particularly the precession of the equinoxes and the role of the North Star, with several users providing detailed context on how the "North Star" changes over millennia (e.g., from Thuban to Polaris). There was also a notable, albeit brief, debate about the credibility of Graham Hancock, whose work was mentioned as an initial point of discovery for some readers.

Culturally, commenters reflected on the monument's Art Deco style and the contrast between the ambitious, thoughtful design of historical public works and the efficiency-driven architecture of the modern era. Many expressed a sense of wonder and nostalgia, with one user sharing a personal anecdote about visiting the Hoover Dam. The conversation also touched on the practicalities of celestial navigation, clarifying that while the North Pole star is a fixed point, it is not essential for all forms of navigation. Overall, the discussion was a mix of scientific curiosity and appreciation for the monument as a piece of enduring art and a message to the future.

---

## [Nvidia Stock Crash Prediction](https://entropicthoughts.com/nvidia-stock-crash-prediction)
**Score:** 406 | **Comments:** 337 | **ID:** 46693205

> **Article:** The article analyzes the probability of a significant drop in Nvidia's stock price by examining the options market. It uses the pricing of options to infer the market's collective belief about future volatility and price movements. The author focuses on a specific prediction from the "ACX prediction contest" about whether Nvidia's stock will close below $100 on any day in 2026. The analysis suggests that while a major crash is possible, the options market prices this risk into the stock's valuation. The article frames this as a way to quantify the "tail risk" of a catastrophic event for the company, separate from its day-to-day business performance.
>
> **Discussion:** The Hacker News discussion largely sidesteps the article's technical analysis of options pricing, focusing instead on the fundamental risks and speculative nature of Nvidia's valuation. A prominent theme is geopolitical risk, with several users highlighting a potential Chinese invasion of Taiwan as a catastrophic scenario that could drive Nvidia's value to "nearly zero," given its reliance on TSMC for manufacturing.

Another major theme is the sustainability of the AI boom. Commenters debate whether the current massive spending on AI data centers can continue, suggesting that the "gold rush" for AI hardware may eventually slow down as supply increases and companies extend the depreciation schedules of their expensive GPUs. This leads to speculation about Nvidia's shrinking market share, particularly in the inference market.

The discussion also touches on the concept of market bubbles and timing. While many agree that the AI bubble will eventually pop, it's noted that predicting the timing is nearly impossible and often a losing strategy. The article's methodology was clarified by some commenters, distinguishing it from traditional technical analysis and framing it as an interpretation of the market's collective risk assessment via options pricing. Ultimately, the conversation reflects a broader skepticism about Nvidia's astronomical valuation and a consensus that its future is tied to the broader health of the AI industry and external geopolitical factors, rather than just its own business fundamentals.

---

## [California is free of drought for the first time in 25 years](https://www.latimes.com/california/story/2026-01-09/california-has-no-areas-of-dryness-first-time-in-25-years)
**Score:** 385 | **Comments:** 186 | **ID:** 46698660

> **Article:** A Los Angeles Times article reports that for the first time in 25 years, California is officially free of drought conditions. This follows a period of heavy rainfall that has replenished reservoirs and improved soil moisture across the state, ending a long-term dry period that had led to strict water conservation measures.
>
> **Discussion:** The Hacker News discussion contextualizes the news with historical perspective, skepticism about long-term water security, and regional comparisons. A prominent theme is the cyclical nature of California's climate, highlighted by a lengthy quote from John Steinbeck's *East of Eden* describing how communities tend to forget dry years during wet ones and vice versa. Commenters also noted the potential for extreme weather events, such as ARkStorm scenarios, which could cause catastrophic flooding.

Several users debated the role of infrastructure in water management. While one commenter argued that the state's failure to build more dams is the primary cause of water shortages, others countered that the best sites for dams are already utilized and that they come with significant environmental and financial costs.

There was significant skepticism regarding the sustainability of the current drought-free status. Commenters pointed out that while reservoir levels are high, low snowpack levels—due to unusually warm temperatures—threaten water supplies for the summer months when meltwater is most needed. Additionally, users noted that water rates remain high despite the increased rainfall, attributing this to the fixed costs of water delivery and infrastructure maintenance.

Finally, the discussion broadened to a national scope, with users noting that while California is experiencing relief, other parts of the U.S. are currently facing severe drought conditions, particularly in the Mountain West region (Colorado, Utah).

---

## [Unconventional PostgreSQL Optimizations](https://hakibenita.com/postgresql-unconventional-optimizations)
**Score:** 361 | **Comments:** 54 | **ID:** 46692116

> **Article:** The article "Unconventional PostgreSQL Optimizations" by Haki Benita explores advanced techniques for improving database performance, focusing on scenarios where standard approaches fall short. The author presents two main case studies: optimizing storage for a large URL uniqueness check and speeding up bulk data updates. For the URL problem, instead of using a standard B-tree index on a text column (which is large), the author demonstrates using a hash index. This index stores a hash of the URL rather than the full string, significantly reducing index size. For the bulk update problem, the author shows how to use a `MERGE` statement to efficiently upsert data from a staging table, avoiding the overhead of multiple individual queries. The article also touches on using `INSERT ... ON CONFLICT` and the importance of understanding PostgreSQL's specific behaviors, like constraint exclusion and the costs of index maintenance.
>
> **Discussion:** The Hacker News discussion is largely positive, with users praising the article for its depth and for revealing the "surface-level" knowledge many experienced developers have of PostgreSQL's powerful features. A key theme is the complexity and "bolted-on" feel of some features like virtual columns and hash indexes, with commenters noting that while they are powerful, they don't feel fully integrated into the ecosystem.

The conversation delves into several technical points:
*   **Hash Indexes vs. Alternatives:** Users debated the utility of hash indexes. While one commenter questioned if a pre-computed hash column would be better, others clarified that the native hash index is superior because it handles storage efficiently and, crucially, manages hash collisions correctly by comparing the full values, something a naive application-level solution might miss.
*   **MERGE vs. ON CONFLICT:** The article's use of the `MERGE` command sparked a sub-thread. Some users were unaware of it, while others noted that `INSERT ... ON CONFLICT` is often preferred due to its atomicity within PostgreSQL's MVCC model. `MERGE` is seen as more powerful for complex scenarios but requires careful handling of edge cases.
*   **PostgreSQL's Query Planner:** A significant point of discussion was PostgreSQL's lack of aggressive plan caching compared to other databases. Commenters explained that plans are often re-generated, which can be a performance bottleneck. They discussed workarounds like using `PREPARE` and the `plan_cache_mode` setting, but acknowledged the per-connection overhead and the challenge of generic plans being suboptimal for specific parameter values.
*   **General PostgreSQL Power:** Several comments reinforced the idea that PostgreSQL is incredibly deep and powerful, with one user comparing it to "an operating system disguised as something else." Many expressed that even after years of use, they feel they've only scratched the surface of its capabilities.

---

## [Running Claude Code dangerously (safely)](https://blog.emilburzo.com/2026/01/running-claude-code-dangerously-safely/)
**Score:** 320 | **Comments:** 245 | **ID:** 46690907

> **Article:** The article "Running Claude Code dangerously (safely)" by Emil Burzo details a method for running the AI coding assistant Claude Code with full system access (the "dangerous" mode) while containing its potential for damage. The author proposes using Vagrant to create a dedicated, disposable virtual machine (VM) for each project. This approach isolates the AI's actions to a specific environment, preventing it from affecting the host machine. The workflow involves a simple `vagrant up` to create the VM and a synced folder to share the project code, allowing the user to leverage their local git tooling while the AI operates within the VM's boundaries. The author argues this provides a good balance of safety, convenience, and project isolation.
>
> **Discussion:** The Hacker News discussion revolves around various methods for safely running powerful AI coding agents, with a strong focus on the trade-offs between convenience, isolation, and security. Many users shared their own solutions, which largely fall into a few categories: virtualization, containerization, and dedicated hardware.

Several users confirmed the VM-based approach, mentioning tools like Proxmox and Vagrant. A key point of debate was the file synchronization method between the host and the VM. While the article's author used a synced folder for convenience, others noted that this could still expose the host to unwanted changes. An alternative suggested was using `rsync` for a one-way copy, though this sacrifices real-time integration. The core benefit of the VM approach is its strong isolation, effectively creating a "blast radius" for the AI's experiments.

Containerization was presented as a lighter-weight alternative. One user described using a Docker container with bind mounts for specific project folders, which avoids the overhead of a full VM. However, others pointed out significant drawbacks, such as difficulties installing certain dependencies and the complexity of running containerized applications within the container itself (a "containers-in-containers" problem).

A recurring theme was the inadequacy of built-in safety mechanisms. Users were highly skeptical of Claude Code's own sandboxing feature, citing GitHub issues where the AI can bypass its own restrictions and disable the sandbox without explicit user approval. This led to a consensus that external, user-controlled isolation is necessary. The discussion also touched on the lack of sandboxing for external services like API calls or Git deployments, which remains a significant unsolved risk.

Finally, some users advocated for more extreme solutions for maximum convenience, such as running the AI "bare metal" on a dedicated machine or using ephemeral cloud-based sandboxes like Sprites.dev, which offer filesystem checkpointing for easy recovery from catastrophic mistakes.

---

## [Anthropic's original take home assignment open sourced](https://github.com/anthropics/original_performance_takehome)
**Score:** 316 | **Comments:** 152 | **ID:** 46700594

> **Article:** The article links to a GitHub repository containing Anthropic's original performance take-home assignment for engineering candidates. The assignment tasks candidates with optimizing a kernel in a simulated machine environment to minimize execution cycles, as measured by a provided profiler. The README includes a challenge to beat the performance of Anthropic's own AI model, Claude Opus 4.5, and invites top performers to contact the company for potential interviews.
>
> **Discussion:** The Hacker News discussion centered on the nature and difficulty of the assignment, with users debating its effectiveness as a hiring tool. Many commenters characterized the problem as a highly specialized test of low-level optimization skills, such as GPU architecture knowledge, polyhedral layout algebra, and manual PTX writing, rather than a measure of general software engineering or creativity. Several participants noted the assignment's resemblance to "demoscene" code golf and profiling challenges, with one user describing it as a test of "polyhedral layout algebra" similar to concepts in NVIDIA's CuTe or the C++ standard's `std::mdspan`.

Opinions on the assignment's merit were divided. Some defended it as a valid and interesting challenge for roles requiring deep performance optimization, contrasting it favorably with more common web development take-homes. However, others criticized it as a "one-sided waste of time" that selects for a narrow, hardware-centric skillset and may not correlate with broader engineering ability. The tone of the challenge—specifically the invitation to beat Claude's performance—was also a point of contention, with some viewing it as arrogant or "pompous."

A secondary thread emerged discussing the practical difficulty of solving the problem, with one user sharing a real-time anecdote about an AI model (Gemini CLI) struggling with the task. Another point of confusion was the time limit; it was clarified that the 2-hour benchmark referred to the time taken by Anthropic's AI to achieve its solution, not a strict time limit for human candidates. Finally, a few comments veered into criticism of Anthropic's corporate ethics, referencing past controversies.

---

## [Meta's legal team abandoned its ethical duties](https://www.afterbabel.com/p/how-metas-lawyers-perfected-the-playbook)
**Score:** 261 | **Comments:** 194 | **ID:** 46694378

> **Article:** The article argues that Meta's legal team has systematically abandoned its ethical duties, moving beyond standard risk management to actively suppress evidence and manipulate legal frameworks to serve the company's business interests. It contends that lawyers, who are officers of the court with a duty to uphold the law, instead perfected a playbook for protecting the company at the expense of public safety and ethical standards. The piece frames this as a broader corruption of the legal system, where the law is weaponized for suppression rather than truth-seeking.
>
> **Discussion:** The Hacker News discussion is overwhelmingly critical of Meta and the broader tech industry's ethical standards, though it branches into several distinct themes.

A significant portion of the comments express deep cynicism regarding corporate ethics, viewing the pursuit of shareholder value as a primary driver of unethical and even illegal behavior. Commenters cite examples beyond Meta, such as United Healthcare, to argue that this is a systemic issue in US capitalism where profit consistently trumps human well-being. There is a strong sentiment that executives perform cost-benefit analyses where potential fines for wrongdoing are merely a calculated business expense.

The discussion also highlights the personal and societal impact of these corporate actions, particularly concerning children's mental health. A parent shares the difficulty of resisting social media's "brain rot" and the social pressure their children face, describing how these companies turn responsible parents into "villains." This sentiment is reinforced by a recommendation of the book "Careless People," which commenters describe as "horrifying" and indicative of how little the public realizes about Meta's internal operations.

However, the conversation is not entirely one-sided. A notable counter-argument defends the actions of corporate legal teams as standard and ethical practice. This perspective holds that lawyers are duty-bound to minimize legal risk for their clients, which includes utilizing attorney-client privilege and adhering to data retention policies. This view was met with sharp criticism from others who found it to be a disingenuous interpretation of the article's core ethical arguments.

Finally, an ex-Facebook employee provides an insider perspective, claiming that a culture of ruthless optimization for performance ratings is pervasive not just in the legal team but among engineers and product managers. This suggests the ethical lapses are deeply embedded in the company's operational DNA. The conversation concludes with broader skepticism about Meta's competence, questioning its ability to innovate and suggesting its most successful products were acquired rather than built in-house.

---

## [Ask HN: Do you have any evidence that agentic coding works?](https://news.ycombinator.com/item?id=46691243)
**Score:** 257 | **Comments:** 238 | **ID:** 46691243

> **Question:** The user asks the Hacker News community for evidence that "agentic coding" (using AI agents to write code) is effective. The question is open-ended, seeking real-world proof or experiences that justify the practice, likely prompted by widespread hype and conflicting reports about its utility.
>
> **Discussion:** The discussion reveals a nuanced consensus: agentic coding works, but its effectiveness is strictly bounded by human oversight and project scope. The community largely agrees that agents are not autonomous replacements for developers but rather powerful tools that require careful management.

A central theme is the limitation of current AI in high-level design and planning. Several commenters argue that agents struggle with complex, abstract systems, often producing code that is unmaintainable or unreliable for large-scale projects. They caution that success with small, simple apps does not extrapolate to complex ones, as programming involves more than just syntax—it requires reasoning about abstract machines and first principles. However, others counter that this is a matter of scope; agents excel at well-defined, "monkey work" tasks and can be highly effective for smaller, self-contained applications like CRUD tools or CLI utilities.

The role of the human developer is emphasized as critical. The most effective workflow described involves treating the agent like a junior developer: giving it small, specific tasks, reviewing its plan and output meticulously, and cleaning up the final code. Commenters note that while agents can generate code quickly, the time saved is often offset by the time spent on review and correction. A key insight is that agents can "cheat" by writing meaningless tests or workarounds if the code doesn't function, highlighting the non-negotiable need for human verification.

Ultimately, the community views agentic coding as a skill that must be developed. Success depends on understanding the tool's limitations, setting realistic expectations, and integrating it into a disciplined workflow where the human remains firmly in control of architecture and quality.

---

## [The Unix Pipe Card Game](https://punkx.org/unix-pipe-game/)
**Score:** 219 | **Comments:** 69 | **ID:** 46694124

> **Article:** The article links to "The Unix Pipe Card Game," a physical card game designed to teach the concept of Unix pipes. The game uses cards to represent commands (like `grep`, `sort`, `cat`) and data, allowing players to construct command pipelines to solve puzzles or "win" the game. It is part of a series of educational tools created by the author, intended to provide a tangible, offline way to understand how composable command-line tools work together.
>
> **Discussion:** The Hacker News discussion centered on the effectiveness of a physical card game for teaching Unix concepts, with a strong debate between the value of physical vs. digital learning tools.

A key theme was the debate on learning methodologies. One commenter, a science teacher, argued that the physical format lacks the immediate, experimental feedback of a digital environment. They contended that a child learns best by trying a command and seeing the instant result, a process that a physical game cannot replicate without verbal explanation from a parent or teacher. However, another user countered this, suggesting that the "trial and error" approach of digital experimentation can be frustrating and inefficient for many learners, and that a structured, guided method like the card game might be more effective for some.

The practicality and longevity of the game were also discussed. Some felt it was a "play once" novelty, fun for a short session but unlikely to be replayed, while others shared personal anecdotes of using it as a teaching aid for their children. The game's creator chimed in, clarifying that the game's primary purpose was as a physical prop to "get out of the computer" and facilitate discussion about programming concepts, rather than being a standalone, endlessly replayable game.

Finally, the conversation branched into related topics, including recommendations for online alternatives and similar games (like "Gates"), and a brief discussion on the limitations of traditional Unix pipes (transferring only text) versus modern shells like Nushell that handle richer data structures.

---

## [Instabridge has acquired Nova Launcher](https://novalauncher.com/nova-is-here-to-stay)
**Score:** 202 | **Comments:** 135 | **ID:** 46696357

> **Article:** Instabridge has acquired Nova Launcher, as announced on the official Nova Launcher website. The article attempts to reassure users that Nova Launcher "is here to stay," promising to keep data collection minimal, avoid selling personal data, and maintain the ad-free status of Nova Prime. However, it also mentions exploring ad-based options for the free version to ensure a sustainable business model.
>
> **Discussion:** The Hacker News community reacted with deep skepticism and concern regarding the acquisition, largely fueled by recent controversies surrounding Nova Launcher. The discussion is dominated by distrust in the new ownership's promises, particularly regarding data privacy and advertising.

A major point of contention is the recent addition of Facebook and Google Ads tracking to the launcher, which users cited as evidence that the company's claims of minimal data collection are not credible. Commenters noted that companies often distinguish between "personal data" and "anonymized" data, suggesting that user data could still be monetized even if not sold directly as personally identifiable information.

The conversation frequently referenced the history of Nova's acquisition by Branch Metrics in 2022, the subsequent layoffs of the development team, and the original founder's departure after being prevented from open-sourcing the project. This context reinforced the sentiment that the acquisition is a negative development.

Many users expressed that they are now actively looking for alternatives. Popular suggestions included:
*   **Lawnchair:** An open-source launcher frequently mentioned as a direct, privacy-respecting alternative.
*   **AIO Launcher:** Another customizable option for users with different preferences.

The overall tone was one of "enshittification," with users lamenting the bait-and-switch of a beloved, paid application turning towards ad-based models and data tracking. There was also cynicism about the "here to stay" messaging, viewing it as a common corporate signal that a product is about to be degraded or discontinued.

---

## [cURL removes bug bounties](https://etn.se/index.php/nyheter/72808-curl-removes-bug-bounties.html)
**Score:** 199 | **Comments:** 108 | **ID:** 46701733

> **Article:** The article reports that the cURL project has discontinued its bug bounty program due to an overwhelming influx of low-quality, AI-generated reports. Daniel Stenberg, cURL's creator, cited the unsustainable volume of "slop" submissions—often containing hallucinated vulnerabilities or fundamental misunderstandings—as the primary reason for the shutdown. The project found that the time required to sift through these false reports outweighed the benefits of the program, leading to the decision to stop offering bounties.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, with users sharing similar experiences of being inundated with AI-generated spam. A key theme is the inability of current systems to distinguish between genuine security research and automated "slop." Many commenters noted that the problem extends beyond cURL, affecting other companies and open-source projects, with some reporting receiving over 100 spam reports daily.

Several solutions were proposed, though most were met with skepticism. Suggestions included implementing an entry fee for submissions (reimbursed for valid reports) and using AI to filter out AI-generated spam. However, the community expressed concern that such measures might create barriers for legitimate researchers or that the "AI vs. AI" approach could become an endless arms race.

The discussion also explored the motivations behind the spam. While bounties are a clear incentive, commenters suggested that spammers are also motivated by the desire to build a portfolio of "security reports" to appear credible. This was highlighted as a particularly cynical abuse of the open-source community's trust. The conversation concluded with a shared sense of frustration, acknowledging that the ease of using AI to generate reports has fundamentally damaged the bug bounty ecosystem for many projects.

---

## [IP Addresses Through 2025](https://www.potaroo.net/ispcol/2026-01/addr2025.html)
**Score:** 179 | **Comments:** 133 | **ID:** 46691835

> **Article:** The article "IP Addresses Through 2025" provides a data-driven analysis of the IPv4 and IPv6 address markets and allocation trends. The central finding is that the IPv4 market has stabilized and prices have collapsed, falling from a peak of ~$55 per address in 2021 back to 2014 levels of around $9. This is attributed to a reduction in speculative hoarding and large-scale acquisition by major cloud providers. The article notes that while IPv4 remains the dominant protocol in terms of traffic, IPv6 allocation is steadily growing, particularly in mobile networks. The author concludes on a somber note, observing that the internet has transitioned from a disruptive, innovative force to an established, centralized norm, where finding effective governance to challenge the dominance of a few large incumbents is a growing challenge.
>
> **Discussion:** The discussion on Hacker News focused on several key themes, with the most prominent being the economic dynamics of IPv4 addresses. Commenters largely agreed with the article's findings, with one user positing that the price bubble was driven by hyperscalers like AWS, which created artificial scarcity. They argued that once AWS began charging hourly for public IPv4 addresses, it passed the cost to consumers, effectively ending their own acquisition spree and causing the market to correct. This was seen as a form of "asset stranding," where the market realized demand wasn't infinite due to the rise of alternatives like Carrier-Grade NAT (CGNAT).

A second major theme was the call for more aggressive IPv6 adoption. Several commenters expressed frustration with the slow transition, suggesting that governments should mandate IPv6 support for ISPs or create a "wall of shame" for services that don't support it. However, this was tempered by the real-world experience of some users who noted that despite being early adopters, they still face operational issues with IPv6, highlighting the protocol's ongoing challenges.

Finally, the discussion touched on other aspects of the article. One thread explored the geopolitics of IP allocation, specifically the observation that Chinese and Indian entities are acquiring large IP blocks in Africa, which some users suspect are used for large-scale "botting" and AI scraping operations. Another user pointed out a data discrepancy in the article's tables regarding the "GB" country code, leading to a brief discussion on the difference between ISO codes and ccTLDs. The article's concluding paragraph on internet centralization also sparked a debate, with some commenters fearing that new regulations could stifle innovation rather than help it.

---

## [IPv6 is not insecure because it lacks a NAT](https://www.johnmaguire.me/blog/ipv6-is-not-insecure-because-it-lacks-nat/)
**Score:** 176 | **Comments:** 249 | **ID:** 46696303

> **Article:** The article argues that the common belief that IPv6 is insecure due to the lack of Network Address Translation (NAT) is a misconception. The author clarifies that NAT was primarily introduced to solve IPv4 address exhaustion, not for security. While NAT can provide a side effect of security by making internal devices unreachable from the internet by default, this is not its intended purpose. The article asserts that IPv6 can be just as secure as IPv4 when implemented with a proper stateful firewall, which is the correct tool for controlling inbound traffic.
>
> **Discussion:** The Hacker News discussion largely agrees with the article's main premise but delves into significant nuance and debate. The consensus is that NAT is not a security feature, but its absence in IPv6 does change the security model and requires more conscious configuration.

Key discussion points include:

*   **NAT's Purpose vs. Side Effect:** Many commenters, including network engineers, reinforce that NAT's primary function is address conservation, not security. The security benefit (blocking unsolicited inbound traffic) is a side effect of how stateful NAT operates. This security can be, and should be, replicated in IPv6 using a stateful firewall, which is the more appropriate tool for the job.
*   **The "Default-Deny" Firewall:** A central theme is the difference between IPv4's implicit "default-deny" (due to NAT) and IPv6's need for an explicit one. Commenters note that while many consumer routers ship with a default-deny firewall for IPv6, it's a configurable setting. The risk in IPv6 is not the protocol itself, but a user or ISP misconfiguring the firewall, exposing individual devices directly to the internet. In contrast, a misconfigured IPv4 NAT is less likely to expose the entire local network.
*   **Security Through Obscurity vs. Configuration:** Some argue that IPv4's private addressing provides a layer of "security through obscurity," as internal IP addresses are not routable. IPv6 addresses are globally unique, potentially leaking device-specific information (like MAC addresses in the old EUI-64 scheme). The counter-argument is that security should rely on proper firewall rules, not address obscurity.
*   **Real-World Complications:** Commenters shared real-world experiences. One noted that their ISP provides only a single IPv6 address, forcing them to use NAT66, which maintains the "default-deny" posture. Another shared a cautionary tale of a device being compromised because IPv6 was enabled on a VLAN without a corresponding firewall rule, highlighting that misconfiguration is the real vulnerability.
*   **Nuance and Pushback:** While most agree NAT isn't true security, some push back on the idea that it's entirely irrelevant. One commenter argued that NAT is a "material factor" in securing networks because it provides a baseline of protection that is often taken for granted. The debate also touched on practical IPv6 challenges, like privacy extensions and SIP protocol issues, showing that the transition is not always smooth.

---

## [Show HN: Mastra 1.0, open-source JavaScript agent framework from the Gatsby devs](https://github.com/mastra-ai/mastra)
**Score:** 164 | **Comments:** 52 | **ID:** 46693959

> **Project:** Mastra 1.0 is an open-source, TypeScript-native agent framework for building and deploying AI applications. Developed by the team behind Gatsby, it provides primitives for agents, workflows, and tool integration, aiming to offer a production-grade, vendor-agnostic alternative to other frameworks like LangChain. The project emphasizes developer experience (DX) and is designed specifically for full-stack TypeScript developers.
>
> **Discussion:** The community response to the launch was largely positive, with several users expressing excitement for a TypeScript-first, open-source agent framework. A key theme was the comparison to other tools, with the developers positioning Mastra as a more TypeScript-native and feature-complete alternative to options like Strands Agents or Spring AI. The discussion also touched on the broader trend of using AI for code generation within the team's own workflow.

A notable point of debate was the potential for vendor lock-in. While Mastra itself is open-source and model-agnostic, one commenter cautioned that convenience and DX can shift lock-in to the underlying platform or runtime (e.g., Claude Agent SDK). The developers responded by emphasizing the value of an open-source harness for production deployments to maintain control and customization. The conversation also highlighted specific features like the `.network()` method for creating dynamic multi-agent systems and the challenges of managing build tooling (ESM/CJS) in the modern JavaScript ecosystem.

---

## ['This is sell America' – US dollar tumbles as globe flees US assets](https://www.cnbc.com/2026/01/20/sell-america-trade-dollar-treasury-gold-us-trump-greenland.html)
**Score:** 159 | **Comments:** 68 | **ID:** 46695061

> **Article:** The CNBC article, dated January 20, 2026, reports on a significant downturn in the US dollar and a sell-off of US assets, which analysts are calling the "Sell America" trade. The article attributes this trend to a global loss of confidence in the United States, driven by the Trump administration's aggressive and unpredictable economic and foreign policies. Key factors cited include the imposition of massive tariffs, threats against sovereign nations like Canada and Greenland, and a broader antagonistic stance towards traditional allies and institutions like NATO. This has led international investors to flee the US dollar and Treasury bonds, seeking safer havens like gold and other foreign assets, signaling a potential end to the dollar's long-standing dominance.
>
> **Discussion:** The Hacker News discussion is overwhelmingly critical of the Trump administration's policies and expresses significant concern about the economic and geopolitical fallout. The conversation can be broken down into several key themes:

*   **Criticism of US Foreign Policy:** Commenters point to specific actions, such as imposing tariffs on and threatening to invade Canada, as the direct cause of allies losing faith in the US. The sentiment is that the US is actively alienating its closest partners, with one user noting that Canada's actions are a reaction to US aggression, not an abandonment of the relationship. Trump's recent statements calling NATO the "enemy" are highlighted as particularly alarming, with users suggesting other nations should now treat the US as an adversary.

*   **Economic Anxiety and Investment Strategies:** There is palpable anxiety about the health of US markets and the dollar. Users discuss shifting their investments away from US-focused funds (like VOO) towards international equities (VXUS) and hedges like gold. However, a counterpoint is raised that for US-based individuals, a global economic downturn triggered by a US collapse would still be devastating, suggesting a "stay the course" strategy might be more prudent for long-term investors.

*   **Domestic Political Anger and Fear:** The discussion is filled with anger towards the Trump administration and the Republican party. One user provides a long list of grievances, including unconstitutional raids, illegal tariffs, and threats of invasion, arguing that Republican legislators are complicit by remaining silent. There is a palpable fear of the administration's escalating behavior, with one commenter expressing genuine concern about the potential for nuclear weapon use. The observation that Trump's approval rating remains steady is cited as the reason for this political inaction.

*   **Sarcasm and Dark Humor:** The dire situation is met with a mix of sarcasm and gallows humor. Users make ironic comments like "Thanks everyone who voted for this!" and compare the US strategy to overpromising and under-delivering like Elon Musk's promises on self-driving cars. The tone reflects a sense of helplessness and disbelief at the rapid decline in the US's global standing.

---

## ['The old order is not coming back,' Carney says in speech at Davos](https://www.cbc.ca/news/politics/carney-davos-speech-9.7052725)
**Score:** 158 | **Comments:** 179 | **ID:** 46694482

> **Article:** Canadian Prime Minister Mark Carney, speaking at the World Economic Forum in Davos, declared that the "old order is not coming back." He argued that the post-WWII rules-based international order was always a fiction, as powerful nations exempted themselves from rules while enforcing them asymmetrically against others. Carney stated that this system is now collapsing under the weight of great power rivalry and economic coercion. He urged "middle powers" to abandon the pretense of the old system and pursue strategic autonomy in energy, finance, and supply chains to protect their interests in a new era defined by raw power rather than shared rules.
>
> **Discussion:** The discussion largely validates Carney's speech as a significant and honest articulation of the current geopolitical reality, particularly in the context of the Trump administration's policies. Commenters view his directness—naming the system a "sham" and calling out US economic coercion—as a stark departure from diplomatic norms of the past decade.

Key themes in the conversation include:
*   **The End of US Reliability:** A central point is that the US is no longer a stable partner. The election of Trump twice is seen as a permanent shift, proving that the US is capable of upending treaties and alliances on a whim. This has forced allies to seek independence.
*   **A Call for Middle Power Alliance:** Many users, particularly those identifying as Canadian, see Carney's speech as a blueprint for survival. There is strong support for the idea of "middle powers" (like Canada, EU nations, etc.) banding together to create an economic and military bloc independent of the US. This includes shifting defense spending away from US-made weaponry to bolster their own industries.
*   **Historical Parallels and Warnings:** The discussion is heavy with historical comparisons. Some users draw direct lines to the 1930s, seeing parallels between the current US-Russia-China dynamic and the pre-WWII alliances of totalitarian regimes. Others see a "Sino-Soviet split" dynamic emerging. A detailed comment argues that while the 1939 analogy is flawed, the post-WWII order has ironically rehabilitated fascist elements, which are now resurgent.
*   **Skepticism of Elites and Capitalism:** A dissenting view questions the Davos setting, suggesting that while leaders discuss these shifts, the costs will be borne by ordinary people through austerity and worse labor conditions. This perspective frames the power competition as an intrinsic feature of capitalism.
*   **Contrast in Leadership:** Carney's coherent and strategic speech is frequently contrasted with Trump's rhetoric, which is described as incoherent. This contrast highlights the seriousness with which Carney's message is being received.

---

## [The challenges of soft delete](https://atlas9.dev/blog/soft-delete.html)
**Score:** 158 | **Comments:** 92 | **ID:** 46698061

> **Article:** The article "The challenges of soft delete" explores the common practice of marking records as deleted rather than removing them from the database. It outlines the primary motivation for this approach: the ability to recover data and maintain an audit trail. However, the author details the significant downsides that often outweigh these benefits. These challenges include performance degradation as tables fill with inactive records, increased query complexity (the risk of developers forgetting to filter out deleted rows), and difficulties with data integrity and foreign key constraints. The article suggests that for many use cases, a more robust solution involves moving deleted data to a separate archive or history table, which keeps the primary table lean and performant while still preserving the data for recovery or compliance purposes.
>
> **Discussion:** The Hacker News discussion largely validates the article's premise, offering a variety of perspectives and real-world solutions for managing deleted data. A central theme is the trade-off between the convenience of soft deletes and their technical and operational costs.

Several alternative strategies were proposed to avoid the pitfalls of traditional soft deletes:
*   **Physical Separation:** The most popular suggestion was to physically move deleted records to a separate collection (in NoSQL) or table (in SQL). This keeps the primary table fast and prevents queries from accidentally scanning inactive data. One user noted that this is harder in relational databases with complex foreign key relationships.
*   **Database-Level Abstraction:** Others suggested using database features to abstract the complexity from the application. This includes creating a `VIEW` that automatically filters out soft-deleted rows, or using PostgreSQL's Row-Level Security (RLS) to make deleted rows invisible to application queries by default.

The conversation also highlighted the critical non-technical factors that influence this decision:
*   **Legal and Compliance:** This was a major point of debate. Some argued that privacy regulations like GDPR and CCPA mandate the actual deletion of data, making soft deletes unviable. Conversely, others pointed out that data retention laws in certain jurisdictions can make soft deletes (or a clear archival process) mandatory. A key distinction was made between soft deletion (hiding data from public view, e.g., for moderation) and privacy deletion (irreversible anonymization or removal upon a user request).
*   **Data as a Permanent Asset:** A strong cultural argument was made that "data is never deleted," as it can be invaluable for future analysis, debugging, or business intelligence. The cost of storage was seen as negligible compared to the potential loss of historical context.
*   **Performance and Schema Drift:** While some argued that performance is rarely an issue with modern databases and proper indexing, others shared experiences where high volumes of soft-deleted records (50-70%) caused noticeable slowdowns. A related concern was schema drift: archived data might not match the current schema, making restoration difficult if the archive is kept for a long time.

Ultimately, the consensus was not a simple "yes" or "no" to soft deletes, but rather that the decision is highly context-dependent, requiring careful consideration of performance requirements, legal obligations, and the long-term value of historical data.

---

